{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\u001b[0m\r\n",
      "cycler (0.10.0)\r\n",
      "kiwisolver (1.1.0)\r\n",
      "matplotlib (3.1.3)\r\n",
      "numpy (1.18.1)\r\n",
      "pandas (1.0.1)\r\n",
      "pip (9.0.1)\r\n",
      "pkg-resources (0.0.0)\r\n",
      "pyparsing (2.4.6)\r\n",
      "python-dateutil (2.8.1)\r\n",
      "pytz (2019.3)\r\n",
      "scipy (1.4.1)\r\n",
      "setuptools (39.0.1)\r\n",
      "six (1.14.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package scipy.optimize in scipy:\n",
      "\n",
      "NAME\n",
      "    scipy.optimize\n",
      "\n",
      "DESCRIPTION\n",
      "    =====================================================\n",
      "    Optimization and Root Finding (:mod:`scipy.optimize`)\n",
      "    =====================================================\n",
      "    \n",
      "    .. currentmodule:: scipy.optimize\n",
      "    \n",
      "    SciPy ``optimize`` provides functions for minimizing (or maximizing)\n",
      "    objective functions, possibly subject to constraints. It includes\n",
      "    solvers for nonlinear problems (with support for both local and global\n",
      "    optimization algorithms), linear programing,  constrained\n",
      "    and nonlinear least-squares, root finding and curve fitting.\n",
      "    \n",
      "    Common functions and objects, shared across different solvers, are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       show_options - Show specific options optimization solvers.\n",
      "       OptimizeResult - The optimization result returned by some optimizers.\n",
      "       OptimizeWarning - The optimization encountered problems.\n",
      "    \n",
      "    \n",
      "    Optimization\n",
      "    ============\n",
      "    \n",
      "    Scalar Functions Optimization\n",
      "    -----------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize_scalar - Interface for minimizers of univariate functions\n",
      "    \n",
      "    The `minimize_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize_scalar-brent\n",
      "       optimize.minimize_scalar-bounded\n",
      "       optimize.minimize_scalar-golden\n",
      "    \n",
      "    Local (Multivariate) Optimization\n",
      "    ---------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       minimize - Interface for minimizers of multivariate functions.\n",
      "    \n",
      "    The `minimize` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.minimize-neldermead\n",
      "       optimize.minimize-powell\n",
      "       optimize.minimize-cg\n",
      "       optimize.minimize-bfgs\n",
      "       optimize.minimize-newtoncg\n",
      "       optimize.minimize-lbfgsb\n",
      "       optimize.minimize-tnc\n",
      "       optimize.minimize-cobyla\n",
      "       optimize.minimize-slsqp\n",
      "       optimize.minimize-trustconstr\n",
      "       optimize.minimize-dogleg\n",
      "       optimize.minimize-trustncg\n",
      "       optimize.minimize-trustkrylov\n",
      "       optimize.minimize-trustexact\n",
      "    \n",
      "    Constraints are passed to `minimize` function as a single object or\n",
      "    as a list of objects from the following classes:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       NonlinearConstraint - Class defining general nonlinear constraints.\n",
      "       LinearConstraint - Class defining general linear constraints.\n",
      "    \n",
      "    Simple bound constraints are handled separately and there is a special class\n",
      "    for them:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       Bounds - Bound constraints.\n",
      "    \n",
      "    Quasi-Newton strategies implementing `HessianUpdateStrategy`\n",
      "    interface can be used to approximate the Hessian in `minimize`\n",
      "    function (available only for the 'trust-constr' method). Available\n",
      "    quasi-Newton methods implementing this interface are:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       BFGS - Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "       SR1 - Symmetric-rank-1 Hessian update strategy.\n",
      "    \n",
      "    Global Optimization\n",
      "    -------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       basinhopping - Basinhopping stochastic optimizer.\n",
      "       brute - Brute force searching optimizer.\n",
      "       differential_evolution - stochastic minimization using differential evolution.\n",
      "    \n",
      "       shgo - simplicial homology global optimisation\n",
      "       dual_annealing - Dual annealing stochastic optimizer.\n",
      "    \n",
      "    \n",
      "    Least-squares and Curve Fitting\n",
      "    ===============================\n",
      "    \n",
      "    Nonlinear Least-Squares\n",
      "    -----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       least_squares - Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "    \n",
      "    Linear Least-Squares\n",
      "    --------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       nnls - Linear least-squares problem with non-negativity constraint.\n",
      "       lsq_linear - Linear least-squares problem with bound constraints.\n",
      "    \n",
      "    Curve Fitting\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       curve_fit -- Fit curve to a set of points.\n",
      "    \n",
      "    Root finding\n",
      "    ============\n",
      "    \n",
      "    Scalar functions\n",
      "    ----------------\n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root_scalar - Unified interface for nonlinear solvers of scalar functions.\n",
      "       brentq - quadratic interpolation Brent method.\n",
      "       brenth - Brent method, modified by Harris with hyperbolic extrapolation.\n",
      "       ridder - Ridder's method.\n",
      "       bisect - Bisection method.\n",
      "       newton - Newton's method (also Secant and Halley's methods).\n",
      "       toms748 - Alefeld, Potra & Shi Algorithm 748\n",
      "       RootResults - The root finding result returned by some root finders.\n",
      "    \n",
      "    The `root_scalar` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root_scalar-brentq\n",
      "       optimize.root_scalar-brenth\n",
      "       optimize.root_scalar-bisect\n",
      "       optimize.root_scalar-ridder\n",
      "       optimize.root_scalar-newton\n",
      "       optimize.root_scalar-toms748\n",
      "       optimize.root_scalar-secant\n",
      "       optimize.root_scalar-halley\n",
      "    \n",
      "    \n",
      "    \n",
      "    The table below lists situations and appropriate methods, along with\n",
      "    *asymptotic* convergence rates per iteration (and per function evaluation)\n",
      "    for successful convergence to a simple root(*).\n",
      "    Bisection is the slowest of them all, adding one bit of accuracy for each\n",
      "    function evaluation, but is guaranteed to converge.\n",
      "    The other bracketing methods all (eventually) increase the number of accurate\n",
      "    bits by about 50% for every function evaluation.\n",
      "    The derivative-based methods, all built on `newton`, can converge quite quickly\n",
      "    if the initial value is close to the root.  They can also be applied to\n",
      "    functions defined on (a subset of) the complex plane.\n",
      "    \n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | Domain of f | Bracket? |    Derivatives?      | Solvers     |        Convergence           |\n",
      "    +             +          +----------+-----------+             +-------------+----------------+\n",
      "    |             |          | `fprime` | `fprime2` |             | Guaranteed? |  Rate(s)(*)    |\n",
      "    +=============+==========+==========+===========+=============+=============+================+\n",
      "    | `R`         | Yes      | N/A      | N/A       | - bisection | - Yes       | - 1 \"Linear\"   |\n",
      "    |             |          |          |           | - brentq    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - brenth    | - Yes       | - >=1, <= 1.62 |\n",
      "    |             |          |          |           | - ridder    | - Yes       | - 2.0 (1.41)   |\n",
      "    |             |          |          |           | - toms748   | - Yes       | - 2.7 (1.65)   |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | No       | No        | secant      | No          | 1.62 (1.62)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | No        | newton      | No          | 2.00 (1.41)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    | `R` or `C`  | No       | Yes      | Yes       | halley      | No          | 3.00 (1.44)    |\n",
      "    +-------------+----------+----------+-----------+-------------+-------------+----------------+\n",
      "    \n",
      "    .. seealso::\n",
      "    \n",
      "       `scipy.optimize.cython_optimize` -- Typed Cython versions of zeros functions\n",
      "    \n",
      "    Fixed point finding:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fixed_point - Single-variable fixed-point solver.\n",
      "    \n",
      "    Multidimensional\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       root - Unified interface for nonlinear solvers of multivariate functions.\n",
      "    \n",
      "    The `root` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.root-hybr\n",
      "       optimize.root-lm\n",
      "       optimize.root-broyden1\n",
      "       optimize.root-broyden2\n",
      "       optimize.root-anderson\n",
      "       optimize.root-linearmixing\n",
      "       optimize.root-diagbroyden\n",
      "       optimize.root-excitingmixing\n",
      "       optimize.root-krylov\n",
      "       optimize.root-dfsane\n",
      "    \n",
      "    Linear Programming\n",
      "    ==================\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog -- Unified interface for minimizers of linear programming problems.\n",
      "    \n",
      "    The `linprog` function supports the following methods:\n",
      "    \n",
      "    .. toctree::\n",
      "    \n",
      "       optimize.linprog-simplex\n",
      "       optimize.linprog-interior-point\n",
      "       optimize.linprog-revised_simplex\n",
      "    \n",
      "    The simplex method supports callback functions, such as:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linprog_verbose_callback -- Sample callback function for linprog (simplex).\n",
      "    \n",
      "    Assignment problems:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       linear_sum_assignment -- Solves the linear-sum assignment problem.\n",
      "    \n",
      "    Utilities\n",
      "    =========\n",
      "    \n",
      "    Finite-Difference Approximation\n",
      "    -------------------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       approx_fprime - Approximate the gradient of a scalar function.\n",
      "       check_grad - Check the supplied derivative using finite differences.\n",
      "    \n",
      "    \n",
      "    Line Search\n",
      "    -----------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       bracket - Bracket a minimum, given two starting points.\n",
      "       line_search - Return a step that satisfies the strong Wolfe conditions.\n",
      "    \n",
      "    Hessian Approximation\n",
      "    ---------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       LbfgsInvHessProduct - Linear operator for L-BFGS approximate inverse Hessian.\n",
      "       HessianUpdateStrategy - Interface for implementing Hessian update strategies\n",
      "    \n",
      "    Benchmark Problems\n",
      "    ------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       rosen - The Rosenbrock function.\n",
      "       rosen_der - The derivative of the Rosenbrock function.\n",
      "       rosen_hess - The Hessian matrix of the Rosenbrock function.\n",
      "       rosen_hess_prod - Product of the Rosenbrock Hessian with a vector.\n",
      "    \n",
      "    Legacy Functions\n",
      "    ================\n",
      "    \n",
      "    The functions below are not recommended for use in new scripts;\n",
      "    all of these methods are accessible via a newer, more consistent\n",
      "    interfaces, provided by the interfaces above.\n",
      "    \n",
      "    Optimization\n",
      "    ------------\n",
      "    \n",
      "    General-purpose multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin - Nelder-Mead Simplex algorithm.\n",
      "       fmin_powell - Powell's (modified) level set method.\n",
      "       fmin_cg - Non-linear (Polak-Ribiere) conjugate gradient algorithm.\n",
      "       fmin_bfgs - Quasi-Newton method (Broydon-Fletcher-Goldfarb-Shanno).\n",
      "       fmin_ncg - Line-search Newton Conjugate Gradient.\n",
      "    \n",
      "    Constrained multivariate methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fmin_l_bfgs_b - Zhu, Byrd, and Nocedal's constrained optimizer.\n",
      "       fmin_tnc - Truncated Newton code.\n",
      "       fmin_cobyla - Constrained optimization by linear approximation.\n",
      "       fmin_slsqp - Minimization using sequential least-squares programming.\n",
      "    \n",
      "    Univariate (scalar) minimization methods:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fminbound - Bounded minimization of a scalar function.\n",
      "       brent - 1-D function minimization using Brent method.\n",
      "       golden - 1-D function minimization using Golden Section method.\n",
      "    \n",
      "    Least-Squares\n",
      "    -------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       leastsq - Minimize the sum of squares of M equations in N unknowns.\n",
      "    \n",
      "    Root Finding\n",
      "    ------------\n",
      "    \n",
      "    General nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       fsolve - Non-linear multi-variable equation solver.\n",
      "       broyden1 - Broyden's first method.\n",
      "       broyden2 - Broyden's second method.\n",
      "    \n",
      "    Large-scale nonlinear solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       newton_krylov\n",
      "       anderson\n",
      "    \n",
      "    Simple iteration solvers:\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: generated/\n",
      "    \n",
      "       excitingmixing\n",
      "       linearmixing\n",
      "       diagbroyden\n",
      "    \n",
      "    :mod:`Additional information on the nonlinear solvers <scipy.optimize.nonlin>`\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _basinhopping\n",
      "    _bglu_dense\n",
      "    _cobyla\n",
      "    _constraints\n",
      "    _differentiable_functions\n",
      "    _differentialevolution\n",
      "    _dual_annealing\n",
      "    _group_columns\n",
      "    _hessian_update_strategy\n",
      "    _lbfgsb\n",
      "    _linprog\n",
      "    _linprog_ip\n",
      "    _linprog_rs\n",
      "    _linprog_simplex\n",
      "    _linprog_util\n",
      "    _lsap\n",
      "    _lsap_module\n",
      "    _lsq (package)\n",
      "    _minimize\n",
      "    _minpack\n",
      "    _nnls\n",
      "    _numdiff\n",
      "    _remove_redundancy\n",
      "    _root\n",
      "    _root_scalar\n",
      "    _shgo\n",
      "    _shgo_lib (package)\n",
      "    _slsqp\n",
      "    _spectral\n",
      "    _trlib (package)\n",
      "    _trustregion\n",
      "    _trustregion_constr (package)\n",
      "    _trustregion_dogleg\n",
      "    _trustregion_exact\n",
      "    _trustregion_krylov\n",
      "    _trustregion_ncg\n",
      "    _tstutils\n",
      "    _zeros\n",
      "    cobyla\n",
      "    cython_optimize (package)\n",
      "    lbfgsb\n",
      "    linesearch\n",
      "    minpack\n",
      "    minpack2\n",
      "    moduleTNC\n",
      "    nnls\n",
      "    nonlin\n",
      "    optimize\n",
      "    setup\n",
      "    slsqp\n",
      "    tests (package)\n",
      "    tnc\n",
      "    zeros\n",
      "\n",
      "CLASSES\n",
      "    builtins.UserWarning(builtins.Warning)\n",
      "        scipy.optimize.optimize.OptimizeWarning\n",
      "    builtins.dict(builtins.object)\n",
      "        scipy.optimize.optimize.OptimizeResult\n",
      "    builtins.object\n",
      "        scipy.optimize._constraints.Bounds\n",
      "        scipy.optimize._constraints.LinearConstraint\n",
      "        scipy.optimize._constraints.NonlinearConstraint\n",
      "        scipy.optimize._hessian_update_strategy.HessianUpdateStrategy\n",
      "        scipy.optimize.zeros.RootResults\n",
      "    scipy.optimize._hessian_update_strategy.FullHessianUpdateStrategy(scipy.optimize._hessian_update_strategy.HessianUpdateStrategy)\n",
      "        scipy.optimize._hessian_update_strategy.BFGS\n",
      "        scipy.optimize._hessian_update_strategy.SR1\n",
      "    scipy.sparse.linalg.interface.LinearOperator(builtins.object)\n",
      "        scipy.optimize.lbfgsb.LbfgsInvHessProduct\n",
      "    \n",
      "    class BFGS(FullHessianUpdateStrategy)\n",
      "     |  Broyden-Fletcher-Goldfarb-Shanno (BFGS) Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  exception_strategy : {'skip_update', 'damp_update'}, optional\n",
      "     |      Define how to proceed when the curvature condition is violated.\n",
      "     |      Set it to 'skip_update' to just skip the update. Or, alternatively,\n",
      "     |      set it to 'damp_update' to interpolate between the actual BFGS\n",
      "     |      result and the unmodified matrix. Both exceptions strategies\n",
      "     |      are explained  in [1]_, p.536-537.\n",
      "     |  min_curvature : float\n",
      "     |      This number, scaled by a normalization factor, defines the\n",
      "     |      minimum curvature ``dot(delta_grad, delta_x)`` allowed to go\n",
      "     |      unaffected by the exception strategy. By default is equal to\n",
      "     |      1e-8 when ``exception_strategy = 'skip_update'`` and equal\n",
      "     |      to 0.2 when ``exception_strategy = 'damp_update'``.\n",
      "     |  init_scale : {float, 'auto'}\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.140.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BFGS\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, exception_strategy='skip_update', min_curvature=None, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-d array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-d  represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Bounds(builtins.object)\n",
      "     |  Bounds constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= x <= ub\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  lb, ub : array_like, optional\n",
      "     |      Lower and upper bounds on independent variables. Each array must\n",
      "     |      have the same size as x or be a scalar, in which case a bound will be\n",
      "     |      the same for all the variables. Set components of `lb` and `ub` equal\n",
      "     |      to fix a variable. Use ``np.inf`` with an appropriate sign to disable\n",
      "     |      bounds on all or some variables. Note that you can mix constraints of\n",
      "     |      different types: interval, one-sided or equality, by setting different\n",
      "     |      components of `lb` and `ub` as necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class HessianUpdateStrategy(builtins.object)\n",
      "     |  Interface for implementing Hessian update strategies.\n",
      "     |  \n",
      "     |  Many optimization methods make use of Hessian (or inverse Hessian)\n",
      "     |  approximations, such as the quasi-Newton methods BFGS, SR1, L-BFGS.\n",
      "     |  Some of these  approximations, however, do not actually need to store\n",
      "     |  the entire matrix or can compute the internal matrix product with a\n",
      "     |  given vector in a very efficiently manner. This class serves as an\n",
      "     |  abstract interface between the optimization algorithm and the\n",
      "     |  quasi-Newton update strategies, giving freedom of implementation\n",
      "     |  to store and update the internal matrix as efficiently as possible.\n",
      "     |  Different choices of initialization and update procedure will result\n",
      "     |  in different quasi-Newton strategies.\n",
      "     |  \n",
      "     |  Four methods should be implemented in derived classes: ``initialize``,\n",
      "     |  ``update``, ``dot`` and ``get_matrix``.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Any instance of a class that implements this interface,\n",
      "     |  can be accepted by the method ``minimize`` and used by\n",
      "     |  the compatible solvers to approximate the Hessian (or\n",
      "     |  inverse Hessian) used by the optimization algorithms.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-d array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-d  represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      H : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian\n",
      "     |          or its inverse (depending on how 'approx_type'\n",
      "     |          is defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LbfgsInvHessProduct(scipy.sparse.linalg.interface.LinearOperator)\n",
      "     |  Linear operator for the L-BFGS approximate inverse Hessian.\n",
      "     |  \n",
      "     |  This operator computes the product of a vector with the approximate inverse\n",
      "     |  of the Hessian of the objective function, using the L-BFGS limited\n",
      "     |  memory approximation to the inverse Hessian, accumulated during the\n",
      "     |  optimization.\n",
      "     |  \n",
      "     |  Objects of this class implement the ``scipy.sparse.linalg.LinearOperator``\n",
      "     |  interface.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  sk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the solution vector.\n",
      "     |      (See [1]).\n",
      "     |  yk : array_like, shape=(n_corr, n)\n",
      "     |      Array of `n_corr` most recent updates to the gradient. (See [1]).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge. \"Updating quasi-Newton matrices with limited\n",
      "     |     storage.\" Mathematics of computation 35.151 (1980): 773-782.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LbfgsInvHessProduct\n",
      "     |      scipy.sparse.linalg.interface.LinearOperator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sk, yk)\n",
      "     |      Construct the operator.\n",
      "     |  \n",
      "     |  todense(self)\n",
      "     |      Return a dense array representation of this operator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      arr : ndarray, shape=(n, n)\n",
      "     |          An array with the same shape and containing\n",
      "     |          the same data represented by this `LinearOperator`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __add__(self, x)\n",
      "     |  \n",
      "     |  __call__(self, x)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __matmul__(self, other)\n",
      "     |  \n",
      "     |  __mul__(self, x)\n",
      "     |  \n",
      "     |  __neg__(self)\n",
      "     |  \n",
      "     |  __pow__(self, p)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __rmatmul__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, x)\n",
      "     |  \n",
      "     |  __sub__(self, x)\n",
      "     |  \n",
      "     |  adjoint(self)\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  dot(self, x)\n",
      "     |      Matrix-matrix or matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : array_like\n",
      "     |          1-d or 2-d array, representing a vector or matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Ax : array\n",
      "     |          1-d or 2-d array (depending on the shape of x) that represents\n",
      "     |          the result of applying this linear operator on x.\n",
      "     |  \n",
      "     |  matmat(self, X)\n",
      "     |      Matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*X where A is an MxN linear\n",
      "     |      operator and X dense N*K matrix or ndarray.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          An array with shape (N,K).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,K) depending on\n",
      "     |          the type of the X argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matmat wraps any user-specified matmat routine or overridden\n",
      "     |      _matmat method to ensure that y has the correct type.\n",
      "     |  \n",
      "     |  matvec(self, x)\n",
      "     |      Matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y=A*x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (N,) or (N,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (M,) or (M,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This matvec wraps the user-specified matvec routine or overridden\n",
      "     |      _matvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  rmatmat(self, X)\n",
      "     |      Adjoint matrix-matrix multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array, or 2-d array.\n",
      "     |      The default implementation defers to the adjoint.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {matrix, ndarray}\n",
      "     |          A matrix or 2D array.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Y : {matrix, ndarray}\n",
      "     |          A matrix or 2D array depending on the type of the input.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatmat wraps the user-specified rmatmat routine.\n",
      "     |  \n",
      "     |  rmatvec(self, x)\n",
      "     |      Adjoint matrix-vector multiplication.\n",
      "     |      \n",
      "     |      Performs the operation y = A^H * x where A is an MxN linear\n",
      "     |      operator and x is a column vector or 1-d array.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      x : {matrix, ndarray}\n",
      "     |          An array with shape (M,) or (M,1).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : {matrix, ndarray}\n",
      "     |          A matrix or ndarray with shape (N,) or (N,1) depending\n",
      "     |          on the type and shape of the x argument.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      This rmatvec wraps the user-specified rmatvec routine or overridden\n",
      "     |      _rmatvec method to ensure that y has the correct shape and type.\n",
      "     |  \n",
      "     |  transpose(self)\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwargs)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from scipy.sparse.linalg.interface.LinearOperator:\n",
      "     |  \n",
      "     |  H\n",
      "     |      Hermitian adjoint.\n",
      "     |      \n",
      "     |      Returns the Hermitian adjoint of self, aka the Hermitian\n",
      "     |      conjugate or Hermitian transpose. For a complex matrix, the\n",
      "     |      Hermitian adjoint is equal to the conjugate transpose.\n",
      "     |      \n",
      "     |      Can be abbreviated self.H instead of self.adjoint().\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      A_H : LinearOperator\n",
      "     |          Hermitian adjoint of self.\n",
      "     |  \n",
      "     |  T\n",
      "     |      Transpose this linear operator.\n",
      "     |      \n",
      "     |      Returns a LinearOperator that represents the transpose of this one.\n",
      "     |      Can be abbreviated self.T instead of self.transpose().\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LinearConstraint(builtins.object)\n",
      "     |  Linear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= A.dot(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and the matrix A has shape (m, n).\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  A : {array_like, sparse matrix}, shape (m, n)\n",
      "     |      Matrix defining the constraint.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, A, lb, ub, keep_feasible=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NonlinearConstraint(builtins.object)\n",
      "     |  Nonlinear constraint on the variables.\n",
      "     |  \n",
      "     |  The constraint has the general inequality form::\n",
      "     |  \n",
      "     |      lb <= fun(x) <= ub\n",
      "     |  \n",
      "     |  Here the vector of independent variables x is passed as ndarray of shape\n",
      "     |  (n,) and ``fun`` returns a vector with m components.\n",
      "     |  \n",
      "     |  It is possible to use equal bounds to represent an equality constraint or\n",
      "     |  infinite bounds to represent a one-sided constraint.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fun : callable\n",
      "     |      The function defining the constraint.\n",
      "     |      The signature is ``fun(x) -> array_like, shape (m,)``.\n",
      "     |  lb, ub : array_like\n",
      "     |      Lower and upper bounds on the constraint. Each array must have the\n",
      "     |      shape (m,) or be a scalar, in the latter case a bound will be the same\n",
      "     |      for all components of the constraint. Use ``np.inf`` with an\n",
      "     |      appropriate sign to specify a one-sided constraint.\n",
      "     |      Set components of `lb` and `ub` equal to represent an equality\n",
      "     |      constraint. Note that you can mix constraints of different types:\n",
      "     |      interval, one-sided or equality, by setting different components of\n",
      "     |      `lb` and `ub` as  necessary.\n",
      "     |  jac : {callable,  '2-point', '3-point', 'cs'}, optional\n",
      "     |      Method of computing the Jacobian matrix (an m-by-n matrix,\n",
      "     |      where element (i, j) is the partial derivative of f[i] with\n",
      "     |      respect to x[j]).  The keywords {'2-point', '3-point',\n",
      "     |      'cs'} select a finite difference scheme for the numerical estimation.\n",
      "     |      A callable must have the following signature:\n",
      "     |      ``jac(x) -> {ndarray, sparse matrix}, shape (m, n)``.\n",
      "     |      Default is '2-point'.\n",
      "     |  hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy, None}, optional\n",
      "     |      Method for computing the Hessian matrix. The keywords\n",
      "     |      {'2-point', '3-point', 'cs'} select a finite difference scheme for\n",
      "     |      numerical  estimation.  Alternatively, objects implementing\n",
      "     |      `HessianUpdateStrategy` interface can be used to approximate the\n",
      "     |      Hessian. Currently available implementations are:\n",
      "     |  \n",
      "     |          - `BFGS` (default option)\n",
      "     |          - `SR1`\n",
      "     |  \n",
      "     |      A callable must return the Hessian matrix of ``dot(fun, v)`` and\n",
      "     |      must have the following signature:\n",
      "     |      ``hess(x, v) -> {LinearOperator, sparse matrix, array_like}, shape (n, n)``.\n",
      "     |      Here ``v`` is ndarray with shape (m,) containing Lagrange multipliers.\n",
      "     |  keep_feasible : array_like of bool, optional\n",
      "     |      Whether to keep the constraint components feasible throughout\n",
      "     |      iterations. A single value set this property for all components.\n",
      "     |      Default is False. Has no effect for equality constraints.\n",
      "     |  finite_diff_rel_step: None or array_like, optional\n",
      "     |      Relative step size for the finite difference approximation. Default is\n",
      "     |      None, which will select a reasonable value automatically depending\n",
      "     |      on a finite difference scheme.\n",
      "     |  finite_diff_jac_sparsity: {None, array_like, sparse matrix}, optional\n",
      "     |      Defines the sparsity structure of the Jacobian matrix for finite\n",
      "     |      difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "     |      only few non-zero elements in *each* row, providing the sparsity\n",
      "     |      structure will greatly speed up the computations. A zero entry means\n",
      "     |      that a corresponding element in the Jacobian is identically zero.\n",
      "     |      If provided, forces the use of 'lsmr' trust-region solver.\n",
      "     |      If None (default) then dense differencing will be used.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Finite difference schemes {'2-point', '3-point', 'cs'} may be used for\n",
      "     |  approximating either the Jacobian or the Hessian. We, however, do not allow\n",
      "     |  its use for approximating both simultaneously. Hence whenever the Jacobian\n",
      "     |  is estimated via finite-differences, we require the Hessian to be estimated\n",
      "     |  using one of the quasi-Newton strategies.\n",
      "     |  \n",
      "     |  The scheme 'cs' is potentially the most accurate, but requires the function\n",
      "     |  to correctly handles complex inputs and be analytically continuable to the\n",
      "     |  complex plane. The scheme '3-point' is more accurate than '2-point' but\n",
      "     |  requires twice as many operations.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fun, lb, ub, jac='2-point', hess=<scipy.optimize._hessian_update_strategy.BFGS object at 0x7fe497923400>, keep_feasible=False, finite_diff_rel_step=None, finite_diff_jac_sparsity=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OptimizeResult(builtins.dict)\n",
      "     |  Represents the optimization result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  x : ndarray\n",
      "     |      The solution of the optimization.\n",
      "     |  success : bool\n",
      "     |      Whether or not the optimizer exited successfully.\n",
      "     |  status : int\n",
      "     |      Termination status of the optimizer. Its value depends on the\n",
      "     |      underlying solver. Refer to `message` for details.\n",
      "     |  message : str\n",
      "     |      Description of the cause of the termination.\n",
      "     |  fun, jac, hess: ndarray\n",
      "     |      Values of objective function, its Jacobian and its Hessian (if\n",
      "     |      available). The Hessians may be approximations, see the documentation\n",
      "     |      of the function in question.\n",
      "     |  hess_inv : object\n",
      "     |      Inverse of the objective function's Hessian; may be an approximation.\n",
      "     |      Not available for all solvers. The type of this attribute may be\n",
      "     |      either np.ndarray or scipy.sparse.linalg.LinearOperator.\n",
      "     |  nfev, njev, nhev : int\n",
      "     |      Number of evaluations of the objective functions and of its\n",
      "     |      Jacobian and Hessian.\n",
      "     |  nit : int\n",
      "     |      Number of iterations performed by the optimizer.\n",
      "     |  maxcv : float\n",
      "     |      The maximum constraint violation.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  There may be additional attributes not listed above depending of the\n",
      "     |  specific solver. Since this class is essentially a subclass of dict\n",
      "     |  with attribute accessors, one can see which attributes are available\n",
      "     |  using the `keys()` method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeResult\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delattr__ = __delitem__(self, key, /)\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__ = __setitem__(self, key, value, /)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if D has a key k, else False.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      D.copy() -> a shallow copy of D\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None, /) from builtins.type\n",
      "     |      Returns a new dict with keys from iterable and values equal to value.\n",
      "     |  \n",
      "     |  get(...)\n",
      "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  pop(...)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised\n",
      "     |  \n",
      "     |  popitem(...)\n",
      "     |      D.popitem() -> (k, v), remove and return some (key, value) pair as a\n",
      "     |      2-tuple; but raise KeyError if D is empty.\n",
      "     |  \n",
      "     |  setdefault(...)\n",
      "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      "     |  \n",
      "     |  update(...)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class OptimizeWarning(builtins.UserWarning)\n",
      "     |  Base class for warnings generated by user code.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizeWarning\n",
      "     |      builtins.UserWarning\n",
      "     |      builtins.Warning\n",
      "     |      builtins.Exception\n",
      "     |      builtins.BaseException\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.UserWarning:\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __delattr__(self, name, /)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __reduce__(...)\n",
      "     |      helper for pickle\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name, value, /)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(...)\n",
      "     |  \n",
      "     |  __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  with_traceback(...)\n",
      "     |      Exception.with_traceback(tb) --\n",
      "     |      set self.__traceback__ to tb and return self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.BaseException:\n",
      "     |  \n",
      "     |  __cause__\n",
      "     |      exception cause\n",
      "     |  \n",
      "     |  __context__\n",
      "     |      exception context\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  __suppress_context__\n",
      "     |  \n",
      "     |  __traceback__\n",
      "     |  \n",
      "     |  args\n",
      "    \n",
      "    class RootResults(builtins.object)\n",
      "     |  Represents the root finding result.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  root : float\n",
      "     |      Estimated root location.\n",
      "     |  iterations : int\n",
      "     |      Number of iterations needed to find the root.\n",
      "     |  function_calls : int\n",
      "     |      Number of times the function was called.\n",
      "     |  converged : bool\n",
      "     |      True if the routine converged.\n",
      "     |  flag : str\n",
      "     |      Description of the cause of termination.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, iterations, function_calls, flag)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SR1(FullHessianUpdateStrategy)\n",
      "     |  Symmetric-rank-1 Hessian update strategy.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  min_denominator : float\n",
      "     |      This number, scaled by a normalization factor,\n",
      "     |      defines the minimum denominator magnitude allowed\n",
      "     |      in the update. When the condition is violated we skip\n",
      "     |      the update. By default uses ``1e-8``.\n",
      "     |  init_scale : {float, 'auto'}, optional\n",
      "     |      Matrix scale at first iteration. At the first\n",
      "     |      iteration the Hessian matrix or its inverse will be initialized\n",
      "     |      with ``init_scale*np.eye(n)``, where ``n`` is the problem dimension.\n",
      "     |      Set it to 'auto' in order to use an automatic heuristic for choosing\n",
      "     |      the initial scale. The heuristic is described in [1]_, p.143.\n",
      "     |      By default uses 'auto'.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The update is based on the description in [1]_, p.144-146.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Nocedal, Jorge, and Stephen J. Wright. \"Numerical optimization\"\n",
      "     |         Second Edition (2006).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SR1\n",
      "     |      FullHessianUpdateStrategy\n",
      "     |      HessianUpdateStrategy\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, min_denominator=1e-08, init_scale='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from FullHessianUpdateStrategy:\n",
      "     |  \n",
      "     |  dot(self, p)\n",
      "     |      Compute the product of the internal matrix with the given vector.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      p : array_like\n",
      "     |          1-d array representing a vector.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      Hp : array\n",
      "     |          1-d  represents the result of multiplying the approximation matrix\n",
      "     |          by vector p.\n",
      "     |  \n",
      "     |  get_matrix(self)\n",
      "     |      Return the current internal matrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      M : ndarray, shape (n, n)\n",
      "     |          Dense matrix containing either the Hessian or its inverse\n",
      "     |          (depending on how `approx_type` was defined).\n",
      "     |  \n",
      "     |  initialize(self, n, approx_type)\n",
      "     |      Initialize internal matrix.\n",
      "     |      \n",
      "     |      Allocate internal memory for storing and updating\n",
      "     |      the Hessian or its inverse.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n : int\n",
      "     |          Problem dimension.\n",
      "     |      approx_type : {'hess', 'inv_hess'}\n",
      "     |          Selects either the Hessian or the inverse Hessian.\n",
      "     |          When set to 'hess' the Hessian will be stored and updated.\n",
      "     |          When set to 'inv_hess' its inverse will be used instead.\n",
      "     |  \n",
      "     |  update(self, delta_x, delta_grad)\n",
      "     |      Update internal matrix.\n",
      "     |      \n",
      "     |      Update Hessian matrix or its inverse (depending on how 'approx_type'\n",
      "     |      is defined) using information about the last evaluated points.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      delta_x : ndarray\n",
      "     |          The difference between two points the gradient\n",
      "     |          function have been evaluated at: ``delta_x = x2 - x1``.\n",
      "     |      delta_grad : ndarray\n",
      "     |          The difference between the gradients:\n",
      "     |          ``delta_grad = grad(x2) - grad(x1)``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from HessianUpdateStrategy:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    anderson(F, xin, iter=None, alpha=None, w0=0.01, M=5, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using (extended) Anderson mixing.\n",
      "        \n",
      "        The Jacobian is formed by for a 'best' solution in the space\n",
      "        spanned by last `M` vectors. As a result, only a MxM matrix\n",
      "        inversions and MxN multiplications are required. [Ey]_\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        M : float, optional\n",
      "            Number of previous vectors to retain. Defaults to 5.\n",
      "        w0 : float, optional\n",
      "            Regularization parameter for numerical stability.\n",
      "            Compared to unity, good values of the order of 0.01.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='anderson'`` in particular.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ey] V. Eyert, J. Comp. Phys., 124, 271 (1996).\n",
      "    \n",
      "    approx_fprime(xk, f, epsilon, *args)\n",
      "        Finite-difference approximation of the gradient of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xk : array_like\n",
      "            The coordinate vector at which to determine the gradient of `f`.\n",
      "        f : callable\n",
      "            The function of which to determine the gradient (partial derivatives).\n",
      "            Should take `xk` as first argument, other arguments to `f` can be\n",
      "            supplied in ``*args``.  Should return a scalar, the value of the\n",
      "            function at `xk`.\n",
      "        epsilon : array_like\n",
      "            Increment to `xk` to use for determining the function gradient.\n",
      "            If a scalar, uses the same finite difference delta for all partial\n",
      "            derivatives.  If an array, should contain one value per element of\n",
      "            `xk`.\n",
      "        \\*args : args, optional\n",
      "            Any other arguments that are to be passed to `f`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        grad : ndarray\n",
      "            The partial derivatives of `f` to `xk`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        check_grad : Check correctness of gradient function against approx_fprime.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function gradient is determined by the forward finite difference\n",
      "        formula::\n",
      "        \n",
      "                     f(xk[i] + epsilon[i]) - f(xk[i])\n",
      "            f'[i] = ---------------------------------\n",
      "                                epsilon[i]\n",
      "        \n",
      "        The main use of `approx_fprime` is in scalar function optimizers like\n",
      "        `fmin_bfgs`, to determine numerically the Jacobian of a function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c0, c1):\n",
      "        ...     \"Coordinate vector `x` should be an array of size two.\"\n",
      "        ...     return c0 * x[0]**2 + c1*x[1]**2\n",
      "        \n",
      "        >>> x = np.ones(2)\n",
      "        >>> c0, c1 = (1, 200)\n",
      "        >>> eps = np.sqrt(np.finfo(float).eps)\n",
      "        >>> optimize.approx_fprime(x, func, [eps, np.sqrt(200) * eps], c0, c1)\n",
      "        array([   2.        ,  400.00004198])\n",
      "    \n",
      "    basinhopping(func, x0, niter=100, T=1.0, stepsize=0.5, minimizer_kwargs=None, take_step=None, accept_test=None, callback=None, interval=50, disp=False, niter_success=None, seed=None)\n",
      "        Find the global minimum of a function using the basin-hopping algorithm\n",
      "        \n",
      "        Basin-hopping is a two-phase method that combines a global stepping\n",
      "        algorithm with local minimization at each step.  Designed to mimic\n",
      "        the natural process of energy minimization of clusters of atoms, it works\n",
      "        well for similar problems with \"funnel-like, but rugged\" energy landscapes\n",
      "        [5]_.\n",
      "        \n",
      "        As the step-taking, step acceptance, and minimization methods are all\n",
      "        customizable, this function can also be used to implement other two-phase\n",
      "        methods.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            Function to be optimized.  ``args`` can be passed as an optional item\n",
      "            in the dict ``minimizer_kwargs``\n",
      "        x0 : array_like\n",
      "            Initial guess.\n",
      "        niter : integer, optional\n",
      "            The number of basin-hopping iterations\n",
      "        T : float, optional\n",
      "            The \"temperature\" parameter for the accept or reject criterion.  Higher\n",
      "            \"temperatures\" mean that larger jumps in function value will be\n",
      "            accepted.  For best results ``T`` should be comparable to the\n",
      "            separation (in function value) between local minima.\n",
      "        stepsize : float, optional\n",
      "            Maximum step size for use in the random displacement.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            ``scipy.optimize.minimize()`` Some important options could be:\n",
      "        \n",
      "                method : str\n",
      "                    The minimization method (e.g. ``\"L-BFGS-B\"``)\n",
      "                args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "        \n",
      "        take_step : callable ``take_step(x)``, optional\n",
      "            Replace the default step-taking routine with this routine.  The default\n",
      "            step-taking routine is a random displacement of the coordinates, but\n",
      "            other step-taking algorithms may be better for some systems.\n",
      "            ``take_step`` can optionally have the attribute ``take_step.stepsize``.\n",
      "            If this attribute exists, then ``basinhopping`` will adjust\n",
      "            ``take_step.stepsize`` in order to try to optimize the global minimum\n",
      "            search.\n",
      "        accept_test : callable, ``accept_test(f_new=f_new, x_new=x_new, f_old=fold, x_old=x_old)``, optional\n",
      "            Define a test which will be used to judge whether or not to accept the\n",
      "            step.  This will be used in addition to the Metropolis test based on\n",
      "            \"temperature\" ``T``.  The acceptable return values are True,\n",
      "            False, or ``\"force accept\"``. If any of the tests return False\n",
      "            then the step is rejected. If the latter, then this will override any\n",
      "            other tests in order to accept the step. This can be used, for example,\n",
      "            to forcefully escape from a local minimum that ``basinhopping`` is\n",
      "            trapped in.\n",
      "        callback : callable, ``callback(x, f, accept)``, optional\n",
      "            A callback function which will be called for all minima found.  ``x``\n",
      "            and ``f`` are the coordinates and function value of the trial minimum,\n",
      "            and ``accept`` is whether or not that minimum was accepted.  This can\n",
      "            be used, for example, to save the lowest N minima found.  Also,\n",
      "            ``callback`` can be used to specify a user defined stop criterion by\n",
      "            optionally returning True to stop the ``basinhopping`` routine.\n",
      "        interval : integer, optional\n",
      "            interval for how often to update the ``stepsize``\n",
      "        disp : bool, optional\n",
      "            Set to True to print status messages\n",
      "        niter_success : integer, optional\n",
      "            Stop the run if the global minimum candidate remains the same for this\n",
      "            number of iterations.\n",
      "        seed : int or `np.random.RandomState`, optional\n",
      "            If `seed` is not specified the `np.RandomState` singleton is used.\n",
      "            If `seed` is an int, a new `np.random.RandomState` instance is used,\n",
      "            seeded with seed.\n",
      "            If `seed` is already a `np.random.RandomState instance`, then that\n",
      "            `np.random.RandomState` instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the default Metropolis\n",
      "            `accept_test` and the default `take_step`. If you supply your own\n",
      "            `take_step` and `accept_test`, and these functions use random\n",
      "            number generation, then those functions are responsible for the state\n",
      "            of their random number generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination. The ``OptimizeResult`` object returned by the\n",
      "            selected minimizer at the lowest minimum is also contained within this\n",
      "            object and can be accessed through the ``lowest_optimization_result``\n",
      "            attribute.  See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize :\n",
      "            The local minimization function called once for each basinhopping step.\n",
      "            ``minimizer_kwargs`` is passed to this routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Basin-hopping is a stochastic algorithm which attempts to find the global\n",
      "        minimum of a smooth scalar function of one or more variables [1]_ [2]_ [3]_\n",
      "        [4]_.  The algorithm in its current form was described by David Wales and\n",
      "        Jonathan Doye [2]_ http://www-wales.ch.cam.ac.uk/.\n",
      "        \n",
      "        The algorithm is iterative with each cycle composed of the following\n",
      "        features\n",
      "        \n",
      "        1) random perturbation of the coordinates\n",
      "        \n",
      "        2) local minimization\n",
      "        \n",
      "        3) accept or reject the new coordinates based on the minimized function\n",
      "           value\n",
      "        \n",
      "        The acceptance test used here is the Metropolis criterion of standard Monte\n",
      "        Carlo algorithms, although there are many other possibilities [3]_.\n",
      "        \n",
      "        This global minimization method has been shown to be extremely efficient\n",
      "        for a wide variety of problems in physics and chemistry.  It is\n",
      "        particularly useful when the function has many minima separated by large\n",
      "        barriers. See the Cambridge Cluster Database\n",
      "        http://www-wales.ch.cam.ac.uk/CCD.html for databases of molecular systems\n",
      "        that have been optimized primarily using basin-hopping.  This database\n",
      "        includes minimization problems exceeding 300 degrees of freedom.\n",
      "        \n",
      "        See the free software program GMIN (http://www-wales.ch.cam.ac.uk/GMIN) for\n",
      "        a Fortran implementation of basin-hopping.  This implementation has many\n",
      "        different variations of the procedure described above, including more\n",
      "        advanced step taking algorithms and alternate acceptance criterion.\n",
      "        \n",
      "        For stochastic global optimization there is no way to determine if the true\n",
      "        global minimum has actually been found. Instead, as a consistency check,\n",
      "        the algorithm can be run from a number of different random starting points\n",
      "        to ensure the lowest minimum found in each example has converged to the\n",
      "        global minimum.  For this reason ``basinhopping`` will by default simply\n",
      "        run for the number of iterations ``niter`` and return the lowest minimum\n",
      "        found.  It is left to the user to ensure that this is in fact the global\n",
      "        minimum.\n",
      "        \n",
      "        Choosing ``stepsize``:  This is a crucial parameter in ``basinhopping`` and\n",
      "        depends on the problem being solved.  The step is chosen uniformly in the\n",
      "        region from x0-stepsize to x0+stepsize, in each dimension.  Ideally it\n",
      "        should be comparable to the typical separation (in argument values) between\n",
      "        local minima of the function being optimized.  ``basinhopping`` will, by\n",
      "        default, adjust ``stepsize`` to find an optimal value, but this may take\n",
      "        many iterations.  You will get quicker results if you set a sensible\n",
      "        initial value for ``stepsize``.\n",
      "        \n",
      "        Choosing ``T``: The parameter ``T`` is the \"temperature\" used in the\n",
      "        Metropolis criterion.  Basinhopping steps are always accepted if\n",
      "        ``func(xnew) < func(xold)``.  Otherwise, they are accepted with\n",
      "        probability::\n",
      "        \n",
      "            exp( -(func(xnew) - func(xold)) / T )\n",
      "        \n",
      "        So, for best results, ``T`` should to be comparable to the typical\n",
      "        difference (in function values) between local minima.  (The height of\n",
      "        \"walls\" between local minima is irrelevant.)\n",
      "        \n",
      "        If ``T`` is 0, the algorithm becomes Monotonic Basin-Hopping, in which all\n",
      "        steps that increase energy are rejected.\n",
      "        \n",
      "        .. versionadded:: 0.12.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wales, David J. 2003, Energy Landscapes, Cambridge University Press,\n",
      "            Cambridge, UK.\n",
      "        .. [2] Wales, D J, and Doye J P K, Global Optimization by Basin-Hopping and\n",
      "            the Lowest Energy Structures of Lennard-Jones Clusters Containing up to\n",
      "            110 Atoms.  Journal of Physical Chemistry A, 1997, 101, 5111.\n",
      "        .. [3] Li, Z. and Scheraga, H. A., Monte Carlo-minimization approach to the\n",
      "            multiple-minima problem in protein folding, Proc. Natl. Acad. Sci. USA,\n",
      "            1987, 84, 6611.\n",
      "        .. [4] Wales, D. J. and Scheraga, H. A., Global optimization of clusters,\n",
      "            crystals, and biomolecules, Science, 1999, 285, 1368.\n",
      "        .. [5] Olson, B., Hashmi, I., Molloy, K., and Shehu1, A., Basin Hopping as\n",
      "            a General and Versatile Optimization Framework for the Characterization\n",
      "            of Biological Macromolecules, Advances in Artificial Intelligence,\n",
      "            Volume 2012 (2012), Article ID 674832, :doi:`10.1155/2012/674832`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a one-dimensional minimization problem,  with many\n",
      "        local minima superimposed on a parabola.\n",
      "        \n",
      "        >>> from scipy.optimize import basinhopping\n",
      "        >>> func = lambda x: np.cos(14.5 * x - 0.3) + (x + 0.2) * x\n",
      "        >>> x0=[1.]\n",
      "        \n",
      "        Basinhopping, internally, uses a local minimization algorithm.  We will use\n",
      "        the parameter ``minimizer_kwargs`` to tell basinhopping which algorithm to\n",
      "        use and how to set up that minimizer.  This parameter will be passed to\n",
      "        ``scipy.optimize.minimize()``.\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\": \"BFGS\"}\n",
      "        >>> ret = basinhopping(func, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = %.4f, f(x0) = %.4f\" % (ret.x, ret.fun))\n",
      "        global minimum: x = -0.1951, f(x0) = -1.0009\n",
      "        \n",
      "        Next consider a two-dimensional minimization problem. Also, this time we\n",
      "        will use gradient information to significantly speed up the search.\n",
      "        \n",
      "        >>> def func2d(x):\n",
      "        ...     f = np.cos(14.5 * x[0] - 0.3) + (x[1] + 0.2) * x[1] + (x[0] +\n",
      "        ...                                                            0.2) * x[0]\n",
      "        ...     df = np.zeros(2)\n",
      "        ...     df[0] = -14.5 * np.sin(14.5 * x[0] - 0.3) + 2. * x[0] + 0.2\n",
      "        ...     df[1] = 2. * x[1] + 0.2\n",
      "        ...     return f, df\n",
      "        \n",
      "        We'll also use a different local minimization algorithm.  Also we must tell\n",
      "        the minimizer that our function returns both energy and gradient (jacobian)\n",
      "        \n",
      "        >>> minimizer_kwargs = {\"method\":\"L-BFGS-B\", \"jac\":True}\n",
      "        >>> x0 = [1.0, 1.0]\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Here is an example using a custom step-taking routine.  Imagine you want\n",
      "        the first coordinate to take larger steps than the rest of the coordinates.\n",
      "        This can be implemented like so:\n",
      "        \n",
      "        >>> class MyTakeStep(object):\n",
      "        ...    def __init__(self, stepsize=0.5):\n",
      "        ...        self.stepsize = stepsize\n",
      "        ...    def __call__(self, x):\n",
      "        ...        s = self.stepsize\n",
      "        ...        x[0] += np.random.uniform(-2.*s, 2.*s)\n",
      "        ...        x[1:] += np.random.uniform(-s, s, x[1:].shape)\n",
      "        ...        return x\n",
      "        \n",
      "        Since ``MyTakeStep.stepsize`` exists basinhopping will adjust the magnitude\n",
      "        of ``stepsize`` to optimize the search.  We'll use the same 2-D function as\n",
      "        before\n",
      "        \n",
      "        >>> mytakestep = MyTakeStep()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=200, take_step=mytakestep)\n",
      "        >>> print(\"global minimum: x = [%.4f, %.4f], f(x0) = %.4f\" % (ret.x[0],\n",
      "        ...                                                           ret.x[1],\n",
      "        ...                                                           ret.fun))\n",
      "        global minimum: x = [-0.1951, -0.1000], f(x0) = -1.0109\n",
      "        \n",
      "        \n",
      "        Now let's do an example using a custom callback function which prints the\n",
      "        value of every minimum found\n",
      "        \n",
      "        >>> def print_fun(x, f, accepted):\n",
      "        ...         print(\"at minimum %.4f accepted %d\" % (f, int(accepted)))\n",
      "        \n",
      "        We'll run it for only 10 basinhopping steps this time.\n",
      "        \n",
      "        >>> np.random.seed(1)\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, callback=print_fun)\n",
      "        at minimum 0.4159 accepted 1\n",
      "        at minimum -0.9073 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 0.9102 accepted 1\n",
      "        at minimum 2.2945 accepted 0\n",
      "        at minimum -0.1021 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        at minimum -1.0109 accepted 1\n",
      "        \n",
      "        \n",
      "        The minimum at -1.0109 is actually the global minimum, found already on the\n",
      "        8th iteration.\n",
      "        \n",
      "        Now let's implement bounds on the problem using a custom ``accept_test``:\n",
      "        \n",
      "        >>> class MyBounds(object):\n",
      "        ...     def __init__(self, xmax=[1.1,1.1], xmin=[-1.1,-1.1] ):\n",
      "        ...         self.xmax = np.array(xmax)\n",
      "        ...         self.xmin = np.array(xmin)\n",
      "        ...     def __call__(self, **kwargs):\n",
      "        ...         x = kwargs[\"x_new\"]\n",
      "        ...         tmax = bool(np.all(x <= self.xmax))\n",
      "        ...         tmin = bool(np.all(x >= self.xmin))\n",
      "        ...         return tmax and tmin\n",
      "        \n",
      "        >>> mybounds = MyBounds()\n",
      "        >>> ret = basinhopping(func2d, x0, minimizer_kwargs=minimizer_kwargs,\n",
      "        ...                    niter=10, accept_test=mybounds)\n",
      "    \n",
      "    bisect(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find root of a function within an interval using bisection.\n",
      "        \n",
      "        Basic bisection routine to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. `f(a)` and `f(b)` cannot have the same signs.\n",
      "        Slow but sure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  `f` must be continuous, and\n",
      "            f(a) and f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            if convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where x is the root, and r is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise the convergence status is recorded in a `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.bisect(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.bisect(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        fsolve : n-dimensional root-finding\n",
      "    \n",
      "    bracket(func, xa=0.0, xb=1.0, args=(), grow_limit=110.0, maxiter=1000)\n",
      "        Bracket the minimum of the function.\n",
      "        \n",
      "        Given a function and distinct initial points, search in the\n",
      "        downhill direction (as defined by the initital points) and return\n",
      "        new points xa, xb, xc that bracket the minimum of the function\n",
      "        f(xa) > f(xb) < f(xc). It doesn't always mean that obtained\n",
      "        solution will satisfy xa<=x<=xb\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to minimize.\n",
      "        xa, xb : float, optional\n",
      "            Bracketing interval. Defaults `xa` to 0.0, and `xb` to 1.0.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to `func`.\n",
      "        grow_limit : float, optional\n",
      "            Maximum grow limit.  Defaults to 110.0\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Defaults to 1000.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xa, xb, xc : float\n",
      "            Bracket.\n",
      "        fa, fb, fc : float\n",
      "            Objective function values in bracket.\n",
      "        funcalls : int\n",
      "            Number of function evaluations made.\n",
      "    \n",
      "    brent(func, args=(), brack=None, tol=1.48e-08, full_output=0, maxiter=500)\n",
      "        Given a function of one-variable and a possible bracket, return\n",
      "        the local minimum of the function isolated to a fractional precision\n",
      "        of tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present).\n",
      "        brack : tuple, optional\n",
      "            Either a triple (xa,xb,xc) where xa<xb<xc and func(xb) <\n",
      "            func(xa), func(xc) or a pair (xa,xb) which are used as a\n",
      "            starting interval for a downhill bracket search (see\n",
      "            `bracket`). Providing the pair (xa,xb) does not always mean\n",
      "            the obtained solution will satisfy xa<=x<=xb.\n",
      "        tol : float, optional\n",
      "            Stop if between iteration change is less than `tol`.\n",
      "        full_output : bool, optional\n",
      "            If True, return all output args (xmin, fval, iter,\n",
      "            funcalls).\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations in solution.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin : ndarray\n",
      "            Optimum point.\n",
      "        fval : float\n",
      "            Optimum value.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of objective function evaluations made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Brent' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses inverse parabolic interpolation when possible to speed up\n",
      "        convergence of golden section method.\n",
      "        \n",
      "        Does not ensure that the minimum lies in the range specified by\n",
      "        `brack`. See `fminbound`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range (xa,xb).\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.brent(f,brack=(1,2))\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.brent(f,brack=(-1,0.5,2))\n",
      "        >>> minimum\n",
      "        -2.7755575615628914e-17\n",
      "    \n",
      "    brenth(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's\n",
      "        method with hyperbolic extrapolation.\n",
      "        \n",
      "        A variation on the classic Brent routine to find a zero of the function f\n",
      "        between the arguments a and b that uses hyperbolic extrapolation instead of\n",
      "        inverse quadratic extrapolation. There was a paper back in the 1980's ...\n",
      "        f(a) and f(b) cannot have the same signs. Generally on a par with the\n",
      "        brent routine, but not as heavily tested.  It is a safe version of the\n",
      "        secant method that uses hyperbolic extrapolation. The version here is by\n",
      "        Chuck Harris.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. As with `brentq`, for nice\n",
      "            functions the method will often satisfy the above condition\n",
      "            with ``xtol/2`` and ``rtol/2``.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. As with `brentq`, for nice functions\n",
      "            the method will often satisfy the above condition with\n",
      "            ``xtol/2`` and ``rtol/2``.\n",
      "        maxiter : int, optional\n",
      "            if convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brenth(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brenth(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fmin, fmin_powell, fmin_cg,\n",
      "               fmin_bfgs, fmin_ncg : multivariate local optimizers\n",
      "        \n",
      "        leastsq : nonlinear least squares minimizer\n",
      "        \n",
      "        fmin_l_bfgs_b, fmin_tnc, fmin_cobyla : constrained multivariate optimizers\n",
      "        \n",
      "        basinhopping, differential_evolution, brute : global optimizers\n",
      "        \n",
      "        fminbound, brent, golden, bracket : local scalar minimizers\n",
      "        \n",
      "        fsolve : n-dimensional root-finding\n",
      "        \n",
      "        brentq, brenth, ridder, bisect, newton : one-dimensional root-finding\n",
      "        \n",
      "        fixed_point : scalar fixed-point finder\n",
      "    \n",
      "    brentq(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in a bracketing interval using Brent's method.\n",
      "        \n",
      "        Uses the classic Brent's method to find a zero of the function `f` on\n",
      "        the sign changing interval [a , b].  Generally considered the best of the\n",
      "        rootfinding routines here.  It is a safe version of the secant method that\n",
      "        uses inverse quadratic extrapolation.  Brent's method combines root\n",
      "        bracketing, interval bisection, and inverse quadratic interpolation.  It is\n",
      "        sometimes known as the van Wijngaarden-Dekker-Brent method.  Brent (1973)\n",
      "        claims convergence is guaranteed for functions computable within [a,b].\n",
      "        \n",
      "        [Brent1973]_ provides the classic description of the algorithm.  Another\n",
      "        description can be found in a recent edition of Numerical Recipes, including\n",
      "        [PressEtal1992]_.  A third description is at\n",
      "        http://mathworld.wolfram.com/BrentsMethod.html.  It should be easy to\n",
      "        understand the algorithm just by reading our code.  Our code diverges a bit\n",
      "        from standard presentations: we choose a different formula for the\n",
      "        extrapolation step.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)` must\n",
      "            have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval :math:`[a, b]`.\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval :math:`[a, b]`.\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``. For nice functions, Brent's\n",
      "            method will often satisfy the above condition with ``xtol/2``\n",
      "            and ``rtol/2``. [Brent1973]_\n",
      "        maxiter : int, optional\n",
      "            if convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.  f(a) and f(b) must have opposite signs.\n",
      "        \n",
      "        Related functions fall into several classes:\n",
      "        \n",
      "        multivariate local optimizers\n",
      "          `fmin`, `fmin_powell`, `fmin_cg`, `fmin_bfgs`, `fmin_ncg`\n",
      "        nonlinear least squares minimizer\n",
      "          `leastsq`\n",
      "        constrained multivariate optimizers\n",
      "          `fmin_l_bfgs_b`, `fmin_tnc`, `fmin_cobyla`\n",
      "        global optimizers\n",
      "          `basinhopping`, `brute`, `differential_evolution`\n",
      "        local scalar minimizers\n",
      "          `fminbound`, `brent`, `golden`, `bracket`\n",
      "        n-dimensional root-finding\n",
      "          `fsolve`\n",
      "        one-dimensional root-finding\n",
      "          `brenth`, `ridder`, `bisect`, `newton`\n",
      "        scalar fixed-point finder\n",
      "          `fixed_point`\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Brent1973]\n",
      "           Brent, R. P.,\n",
      "           *Algorithms for Minimization Without Derivatives*.\n",
      "           Englewood Cliffs, NJ: Prentice-Hall, 1973. Ch. 3-4.\n",
      "        \n",
      "        .. [PressEtal1992]\n",
      "           Press, W. H.; Flannery, B. P.; Teukolsky, S. A.; and Vetterling, W. T.\n",
      "           *Numerical Recipes in FORTRAN: The Art of Scientific Computing*, 2nd ed.\n",
      "           Cambridge, England: Cambridge University Press, pp. 352-355, 1992.\n",
      "           Section 9.3:  \"Van Wijngaarden-Dekker-Brent Method.\"\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.brentq(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "        \n",
      "        >>> root = optimize.brentq(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "    \n",
      "    broyden1(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's first Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \\\"Broyden's good method\\\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (ie., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden1'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) dx^\\dagger H / ( dx^\\dagger H df)\n",
      "        \n",
      "        which corresponds to Broyden's first Jacobian update\n",
      "        \n",
      "        .. math:: J_+ = J + (df - J dx) dx^\\dagger / dx^\\dagger dx\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \\\"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\\\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "    \n",
      "    broyden2(F, xin, iter=None, alpha=None, reduction_method='restart', max_rank=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Broyden's second Jacobian approximation.\n",
      "        \n",
      "        This method is also known as \"Broyden's bad method\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is ``(-1/alpha)``.\n",
      "        reduction_method : str or tuple, optional\n",
      "            Method used in ensuring that the rank of the Broyden matrix\n",
      "            stays low. Can either be a string giving the name of the method,\n",
      "            or a tuple of the form ``(method, param1, param2, ...)``\n",
      "            that gives the name of the method and values for additional parameters.\n",
      "        \n",
      "            Methods available:\n",
      "        \n",
      "                - ``restart``: drop all matrix columns. Has no extra parameters.\n",
      "                - ``simple``: drop oldest matrix column. Has no extra parameters.\n",
      "                - ``svd``: keep only the most significant SVD components.\n",
      "                  Takes an extra parameter, ``to_retain``, which determines the\n",
      "                  number of SVD components to retain when rank reduction is done.\n",
      "                  Default is ``max_rank - 2``.\n",
      "        \n",
      "        max_rank : int, optional\n",
      "            Maximum rank for the Broyden matrix.\n",
      "            Default is infinity (ie., no rank reduction).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='broyden2'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm implements the inverse Jacobian Quasi-Newton update\n",
      "        \n",
      "        .. math:: H_+ = H + (dx - H df) df^\\dagger / ( df^\\dagger df)\n",
      "        \n",
      "        corresponding to Broyden's second method.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] B.A. van der Rotten, PhD thesis,\n",
      "           \"A limited memory Broyden method to solve high-dimensional\n",
      "           systems of nonlinear equations\". Mathematisch Instituut,\n",
      "           Universiteit Leiden, The Netherlands (2003).\n",
      "        \n",
      "           https://web.archive.org/web/20161022015821/http://www.math.leidenuniv.nl/scripties/Rotten.pdf\n",
      "    \n",
      "    brute(func, ranges, args=(), Ns=20, full_output=0, finish=<function fmin at 0x7fe49bece7b8>, disp=False, workers=1)\n",
      "        Minimize a function over a given range by brute force.\n",
      "        \n",
      "        Uses the \"brute force\" method, i.e. computes the function's value\n",
      "        at each point of a multidimensional grid of points, to find the global\n",
      "        minimum of the function.\n",
      "        \n",
      "        The function is evaluated everywhere in the range with the datatype of the\n",
      "        first call to the function, as enforced by the ``vectorize`` NumPy\n",
      "        function.  The value and type of the function evaluation returned when\n",
      "        ``full_output=True`` are affected in addition by the ``finish`` argument\n",
      "        (see Notes).\n",
      "        \n",
      "        The brute force approach is inefficient because the number of grid points\n",
      "        increases exponentially - the number of grid points to evaluate is\n",
      "        ``Ns ** len(x)``. Consequently, even with coarse grid spacing, even\n",
      "        moderately sized problems can take a long time to run, and/or run into\n",
      "        memory limitations.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized. Must be in the\n",
      "            form ``f(x, *args)``, where ``x`` is the argument in\n",
      "            the form of a 1-D array and ``args`` is a tuple of any\n",
      "            additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        ranges : tuple\n",
      "            Each component of the `ranges` tuple must be either a\n",
      "            \"slice object\" or a range tuple of the form ``(low, high)``.\n",
      "            The program uses these to create the grid of points on which\n",
      "            the objective function will be computed. See `Note 2` for\n",
      "            more detail.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify\n",
      "            the function.\n",
      "        Ns : int, optional\n",
      "            Number of grid points along the axes, if not otherwise\n",
      "            specified. See `Note2`.\n",
      "        full_output : bool, optional\n",
      "            If True, return the evaluation grid and the objective function's\n",
      "            values on it.\n",
      "        finish : callable, optional\n",
      "            An optimization function that is called with the result of brute force\n",
      "            minimization as initial guess.  `finish` should take `func` and\n",
      "            the initial guess as positional arguments, and take `args` as\n",
      "            keyword arguments.  It may additionally take `full_output`\n",
      "            and/or `disp` as keyword arguments.  Use None if no \"polishing\"\n",
      "            function is to be used. See Notes for more details.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages from the `finish` callable.\n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the grid is subdivided into `workers`\n",
      "            sections and evaluated in parallel (uses\n",
      "            `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply `-1` to use all cores available to the Process.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the grid in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.3.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : ndarray\n",
      "            A 1-D array containing the coordinates of a point at which the\n",
      "            objective function had its minimum value. (See `Note 1` for\n",
      "            which point is returned.)\n",
      "        fval : float\n",
      "            Function value at the point `x0`. (Returned when `full_output` is\n",
      "            True.)\n",
      "        grid : tuple\n",
      "            Representation of the evaluation grid.  It has the same\n",
      "            length as `x0`. (Returned when `full_output` is True.)\n",
      "        Jout : ndarray\n",
      "            Function values at each point of the evaluation\n",
      "            grid, `i.e.`, ``Jout = func(*grid)``. (Returned\n",
      "            when `full_output` is True.)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        basinhopping, differential_evolution\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        *Note 1*: The program finds the gridpoint at which the lowest value\n",
      "        of the objective function occurs.  If `finish` is None, that is the\n",
      "        point returned.  When the global minimum occurs within (or not very far\n",
      "        outside) the grid's boundaries, and the grid is fine enough, that\n",
      "        point will be in the neighborhood of the global minimum.\n",
      "        \n",
      "        However, users often employ some other optimization program to\n",
      "        \"polish\" the gridpoint values, `i.e.`, to seek a more precise\n",
      "        (local) minimum near `brute's` best gridpoint.\n",
      "        The `brute` function's `finish` option provides a convenient way to do\n",
      "        that.  Any polishing program used must take `brute's` output as its\n",
      "        initial guess as a positional argument, and take `brute's` input values\n",
      "        for `args` as keyword arguments, otherwise an error will be raised.\n",
      "        It may additionally take `full_output` and/or `disp` as keyword arguments.\n",
      "        \n",
      "        `brute` assumes that the `finish` function returns either an\n",
      "        `OptimizeResult` object or a tuple in the form:\n",
      "        ``(xmin, Jmin, ... , statuscode)``, where ``xmin`` is the minimizing\n",
      "        value of the argument, ``Jmin`` is the minimum value of the objective\n",
      "        function, \"...\" may be some other returned values (which are not used\n",
      "        by `brute`), and ``statuscode`` is the status code of the `finish` program.\n",
      "        \n",
      "        Note that when `finish` is not None, the values returned are those\n",
      "        of the `finish` program, *not* the gridpoint ones.  Consequently,\n",
      "        while `brute` confines its search to the input grid points,\n",
      "        the `finish` program's results usually will not coincide with any\n",
      "        gridpoint, and may fall outside the grid's boundary. Thus, if a\n",
      "        minimum only needs to be found over the provided grid points, make\n",
      "        sure to pass in `finish=None`.\n",
      "        \n",
      "        *Note 2*: The grid of points is a `numpy.mgrid` object.\n",
      "        For `brute` the `ranges` and `Ns` inputs have the following effect.\n",
      "        Each component of the `ranges` tuple can be either a slice object or a\n",
      "        two-tuple giving a range of values, such as (0, 5).  If the component is a\n",
      "        slice object, `brute` uses it directly.  If the component is a two-tuple\n",
      "        range, `brute` internally converts it to a slice object that interpolates\n",
      "        `Ns` points from its low-value to its high-value, inclusive.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the use of `brute` to seek the global minimum of a function\n",
      "        of two variables that is given as the sum of a positive-definite\n",
      "        quadratic and two deep \"Gaussian-shaped\" craters.  Specifically, define\n",
      "        the objective function `f` as the sum of three other functions,\n",
      "        ``f = f1 + f2 + f3``.  We suppose each of these has a signature\n",
      "        ``(z, *params)``, where ``z = (x, y)``,  and ``params`` and the functions\n",
      "        are as defined below.\n",
      "        \n",
      "        >>> params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
      "        >>> def f1(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (a * x**2 + b * x * y + c * y**2 + d*x + e*y + f)\n",
      "        \n",
      "        >>> def f2(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-g*np.exp(-((x-h)**2 + (y-i)**2) / scale))\n",
      "        \n",
      "        >>> def f3(z, *params):\n",
      "        ...     x, y = z\n",
      "        ...     a, b, c, d, e, f, g, h, i, j, k, l, scale = params\n",
      "        ...     return (-j*np.exp(-((x-k)**2 + (y-l)**2) / scale))\n",
      "        \n",
      "        >>> def f(z, *params):\n",
      "        ...     return f1(z, *params) + f2(z, *params) + f3(z, *params)\n",
      "        \n",
      "        Thus, the objective function may have local minima near the minimum\n",
      "        of each of the three functions of which it is composed.  To\n",
      "        use `fmin` to polish its gridpoint result, we may then continue as\n",
      "        follows:\n",
      "        \n",
      "        >>> rranges = (slice(-4, 4, 0.25), slice(-4, 4, 0.25))\n",
      "        >>> from scipy import optimize\n",
      "        >>> resbrute = optimize.brute(f, rranges, args=params, full_output=True,\n",
      "        ...                           finish=optimize.fmin)\n",
      "        >>> resbrute[0]  # global minimum\n",
      "        array([-1.05665192,  1.80834843])\n",
      "        >>> resbrute[1]  # function value at global minimum\n",
      "        -3.4085818767\n",
      "        \n",
      "        Note that if `finish` had been set to None, we would have gotten the\n",
      "        gridpoint [-1.0 1.75] where the rounded function value is -2.892.\n",
      "    \n",
      "    check_grad(func, grad, x0, *args, **kwargs)\n",
      "        Check the correctness of a gradient function by comparing it against a\n",
      "        (forward) finite-difference approximation of the gradient.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x0, *args)``\n",
      "            Function whose derivative is to be checked.\n",
      "        grad : callable ``grad(x0, *args)``\n",
      "            Gradient of `func`.\n",
      "        x0 : ndarray\n",
      "            Points to check `grad` against forward difference approximation of grad\n",
      "            using `func`.\n",
      "        args : \\*args, optional\n",
      "            Extra arguments passed to `func` and `grad`.\n",
      "        epsilon : float, optional\n",
      "            Step size used for the finite difference approximation. It defaults to\n",
      "            ``sqrt(numpy.finfo(float).eps)``, which is approximately 1.49e-08.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        err : float\n",
      "            The square root of the sum of squares (i.e. the 2-norm) of the\n",
      "            difference between ``grad(x0, *args)`` and the finite difference\n",
      "            approximation of `grad` using func at the points `x0`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        approx_fprime\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def func(x):\n",
      "        ...     return x[0]**2 - 0.5 * x[1]**3\n",
      "        >>> def grad(x):\n",
      "        ...     return [2 * x[0], -1.5 * x[1]**2]\n",
      "        >>> from scipy.optimize import check_grad\n",
      "        >>> check_grad(func, grad, [1.5, -1.5])\n",
      "        2.9802322387695312e-08\n",
      "    \n",
      "    curve_fit(f, xdata, ydata, p0=None, sigma=None, absolute_sigma=False, check_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)\n",
      "        Use non-linear least squares to fit a function, f, to data.\n",
      "        \n",
      "        Assumes ``ydata = f(xdata, *params) + eps``\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            The model function, f(x, ...).  It must take the independent\n",
      "            variable as the first argument and the parameters to fit as\n",
      "            separate remaining arguments.\n",
      "        xdata : array_like or object\n",
      "            The independent variable where the data is measured.\n",
      "            Should usually be an M-length sequence or an (k,M)-shaped array for\n",
      "            functions with k predictors, but can actually be any object.\n",
      "        ydata : array_like\n",
      "            The dependent data, a length M array - nominally ``f(xdata, ...)``.\n",
      "        p0 : array_like, optional\n",
      "            Initial guess for the parameters (length N).  If None, then the\n",
      "            initial values will all be 1 (if the number of parameters for the\n",
      "            function can be determined using introspection, otherwise a\n",
      "            ValueError is raised).\n",
      "        sigma : None or M-length sequence or MxM array, optional\n",
      "            Determines the uncertainty in `ydata`. If we define residuals as\n",
      "            ``r = ydata - f(xdata, *popt)``, then the interpretation of `sigma`\n",
      "            depends on its number of dimensions:\n",
      "        \n",
      "                - A 1-d `sigma` should contain values of standard deviations of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = sum((r / sigma) ** 2)``.\n",
      "        \n",
      "                - A 2-d `sigma` should contain the covariance matrix of\n",
      "                  errors in `ydata`. In this case, the optimized function is\n",
      "                  ``chisq = r.T @ inv(sigma) @ r``.\n",
      "        \n",
      "                  .. versionadded:: 0.19\n",
      "        \n",
      "            None (default) is equivalent of 1-d `sigma` filled with ones.\n",
      "        absolute_sigma : bool, optional\n",
      "            If True, `sigma` is used in an absolute sense and the estimated parameter\n",
      "            covariance `pcov` reflects these absolute values.\n",
      "        \n",
      "            If False, only the relative magnitudes of the `sigma` values matter.\n",
      "            The returned parameter covariance matrix `pcov` is based on scaling\n",
      "            `sigma` by a constant factor. This constant is set by demanding that the\n",
      "            reduced `chisq` for the optimal parameters `popt` when using the\n",
      "            *scaled* `sigma` equals unity. In other words, `sigma` is scaled to\n",
      "            match the sample variance of the residuals after the fit.\n",
      "            Mathematically,\n",
      "            ``pcov(absolute_sigma=False) = pcov(absolute_sigma=True) * chisq(popt)/(M-N)``\n",
      "        check_finite : bool, optional\n",
      "            If True, check that the input arrays do not contain nans of infs,\n",
      "            and raise a ValueError if they do. Setting this parameter to\n",
      "            False may silently produce nonsensical results if the input arrays\n",
      "            do contain nans. Default is True.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on parameters. Defaults to no bounds.\n",
      "            Each element of the tuple must be either an array with the length equal\n",
      "            to the number of parameters, or a scalar (in which case the bound is\n",
      "            taken to be the same for all parameters.) Use ``np.inf`` with an\n",
      "            appropriate sign to disable bounds on all or some parameters.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        method : {'lm', 'trf', 'dogbox'}, optional\n",
      "            Method to use for optimization.  See `least_squares` for more details.\n",
      "            Default is 'lm' for unconstrained problems and 'trf' if `bounds` are\n",
      "            provided. The method 'lm' won't work when the number of observations\n",
      "            is less than the number of variables, use 'trf' or 'dogbox' in this\n",
      "            case.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "        jac : callable, string or None, optional\n",
      "            Function with signature ``jac(x, ...)`` which computes the Jacobian\n",
      "            matrix of the model function with respect to parameters as a dense\n",
      "            array_like structure. It will be scaled according to provided `sigma`.\n",
      "            If None (default), the Jacobian will be estimated numerically.\n",
      "            String keywords for 'trf' and 'dogbox' methods can be used to select\n",
      "            a finite difference scheme, see `least_squares`.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        kwargs\n",
      "            Keyword arguments passed to `leastsq` for ``method='lm'`` or\n",
      "            `least_squares` otherwise.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        popt : array\n",
      "            Optimal values for the parameters so that the sum of the squared\n",
      "            residuals of ``f(xdata, *popt) - ydata`` is minimized\n",
      "        pcov : 2d array\n",
      "            The estimated covariance of popt. The diagonals provide the variance\n",
      "            of the parameter estimate. To compute one standard deviation errors\n",
      "            on the parameters use ``perr = np.sqrt(np.diag(pcov))``.\n",
      "        \n",
      "            How the `sigma` parameter affects the estimated covariance\n",
      "            depends on `absolute_sigma` argument, as described above.\n",
      "        \n",
      "            If the Jacobian matrix at the solution doesn't have a full rank, then\n",
      "            'lm' method returns a matrix filled with ``np.inf``, on the other hand\n",
      "            'trf'  and 'dogbox' methods use Moore-Penrose pseudoinverse to compute\n",
      "            the covariance matrix.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        ValueError\n",
      "            if either `ydata` or `xdata` contain NaNs, or if incompatible options\n",
      "            are used.\n",
      "        \n",
      "        RuntimeError\n",
      "            if the least-squares minimization fails.\n",
      "        \n",
      "        OptimizeWarning\n",
      "            if covariance of the parameters can not be estimated.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Minimize the sum of squares of nonlinear functions.\n",
      "        scipy.stats.linregress : Calculate a linear least squares regression for\n",
      "                                 two sets of measurements.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        With ``method='lm'``, the algorithm uses the Levenberg-Marquardt algorithm\n",
      "        through `leastsq`. Note that this algorithm can only deal with\n",
      "        unconstrained problems.\n",
      "        \n",
      "        Box constraints can be handled by methods 'trf' and 'dogbox'. Refer to\n",
      "        the docstring of `least_squares` for more information.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> from scipy.optimize import curve_fit\n",
      "        \n",
      "        >>> def func(x, a, b, c):\n",
      "        ...     return a * np.exp(-b * x) + c\n",
      "        \n",
      "        Define the data to be fit with some noise:\n",
      "        \n",
      "        >>> xdata = np.linspace(0, 4, 50)\n",
      "        >>> y = func(xdata, 2.5, 1.3, 0.5)\n",
      "        >>> np.random.seed(1729)\n",
      "        >>> y_noise = 0.2 * np.random.normal(size=xdata.size)\n",
      "        >>> ydata = y + y_noise\n",
      "        >>> plt.plot(xdata, ydata, 'b-', label='data')\n",
      "        \n",
      "        Fit for the parameters a, b, c of the function `func`:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata)\n",
      "        >>> popt\n",
      "        array([ 2.55423706,  1.35190947,  0.47450618])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'r-',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        Constrain the optimization to the region of ``0 <= a <= 3``,\n",
      "        ``0 <= b <= 1`` and ``0 <= c <= 0.5``:\n",
      "        \n",
      "        >>> popt, pcov = curve_fit(func, xdata, ydata, bounds=(0, [3., 1., 0.5]))\n",
      "        >>> popt\n",
      "        array([ 2.43708906,  1.        ,  0.35015434])\n",
      "        >>> plt.plot(xdata, func(xdata, *popt), 'g--',\n",
      "        ...          label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))\n",
      "        \n",
      "        >>> plt.xlabel('x')\n",
      "        >>> plt.ylabel('y')\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "    \n",
      "    diagbroyden(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using diagonal Broyden Jacobian approximation.\n",
      "        \n",
      "        The Jacobian approximation is derived from previous iterations, by\n",
      "        retaining only the diagonal of Broyden matrices.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial guess for the Jacobian is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='diagbroyden'`` in particular.\n",
      "    \n",
      "    differential_evolution(func, bounds, args=(), strategy='best1bin', maxiter=1000, popsize=15, tol=0.01, mutation=(0.5, 1), recombination=0.7, seed=None, callback=None, disp=False, polish=True, init='latinhypercube', atol=0, updating='immediate', workers=1, constraints=())\n",
      "        Finds the global minimum of a multivariate function.\n",
      "        \n",
      "        Differential Evolution is stochastic in nature (does not use gradient\n",
      "        methods) to find the minimum, and can search large areas of candidate\n",
      "        space, but often requires larger numbers of function evaluations than\n",
      "        conventional gradient based techniques.\n",
      "        \n",
      "        The algorithm is due to Storn and Price [1]_.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds for variables.  There are two ways to specify the bounds:\n",
      "            1. Instance of `Bounds` class.\n",
      "            2. ``(min, max)`` pairs for each element in ``x``, defining the finite\n",
      "            lower and upper bounds for the optimizing argument of `func`. It is\n",
      "            required to have ``len(bounds) == len(x)``. ``len(bounds)`` is used\n",
      "            to determine the number of parameters in ``x``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to\n",
      "            completely specify the objective function.\n",
      "        strategy : str, optional\n",
      "            The differential evolution strategy to use. Should be one of:\n",
      "        \n",
      "                - 'best1bin'\n",
      "                - 'best1exp'\n",
      "                - 'rand1exp'\n",
      "                - 'randtobest1exp'\n",
      "                - 'currenttobest1exp'\n",
      "                - 'best2exp'\n",
      "                - 'rand2exp'\n",
      "                - 'randtobest1bin'\n",
      "                - 'currenttobest1bin'\n",
      "                - 'best2bin'\n",
      "                - 'rand2bin'\n",
      "                - 'rand1bin'\n",
      "        \n",
      "            The default is 'best1bin'.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of generations over which the entire population is\n",
      "            evolved. The maximum number of function evaluations (with no polishing)\n",
      "            is: ``(maxiter + 1) * popsize * len(x)``\n",
      "        popsize : int, optional\n",
      "            A multiplier for setting the total population size.  The population has\n",
      "            ``popsize * len(x)`` individuals (unless the initial population is\n",
      "            supplied via the `init` keyword).\n",
      "        tol : float, optional\n",
      "            Relative tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        mutation : float or tuple(float, float), optional\n",
      "            The mutation constant. In the literature this is also known as\n",
      "            differential weight, being denoted by F.\n",
      "            If specified as a float it should be in the range [0, 2].\n",
      "            If specified as a tuple ``(min, max)`` dithering is employed. Dithering\n",
      "            randomly changes the mutation constant on a generation by generation\n",
      "            basis. The mutation constant for that generation is taken from\n",
      "            ``U[min, max)``. Dithering can help speed convergence significantly.\n",
      "            Increasing the mutation constant increases the search radius, but will\n",
      "            slow down convergence.\n",
      "        recombination : float, optional\n",
      "            The recombination constant, should be in the range [0, 1]. In the\n",
      "            literature this is also known as the crossover probability, being\n",
      "            denoted by CR. Increasing this value allows a larger number of mutants\n",
      "            to progress into the next generation, but at the risk of population\n",
      "            stability.\n",
      "        seed : int or `np.random.RandomState`, optional\n",
      "            If `seed` is not specified the `np.RandomState` singleton is used.\n",
      "            If `seed` is an int, a new `np.random.RandomState` instance is used,\n",
      "            seeded with seed.\n",
      "            If `seed` is already a `np.random.RandomState instance`, then that\n",
      "            `np.random.RandomState` instance is used.\n",
      "            Specify `seed` for repeatable minimizations.\n",
      "        disp : bool, optional\n",
      "            Prints the evaluated `func` at every iteration.\n",
      "        callback : callable, `callback(xk, convergence=val)`, optional\n",
      "            A function to follow the progress of the minimization. ``xk`` is\n",
      "            the current value of ``x0``. ``val`` represents the fractional\n",
      "            value of the population convergence.  When ``val`` is greater than one\n",
      "            the function halts. If callback returns `True`, then the minimization\n",
      "            is halted (any polishing is still carried out).\n",
      "        polish : bool, optional\n",
      "            If True (default), then `scipy.optimize.minimize` with the `L-BFGS-B`\n",
      "            method is used to polish the best population member at the end, which\n",
      "            can improve the minimization slightly. If a constrained problem is\n",
      "            being studied then the `trust-constr` method is used instead.\n",
      "        init : str or array-like, optional\n",
      "            Specify which type of population initialization is performed. Should be\n",
      "            one of:\n",
      "        \n",
      "                - 'latinhypercube'\n",
      "                - 'random'\n",
      "                - array specifying the initial population. The array should have\n",
      "                  shape ``(M, len(x))``, where len(x) is the number of parameters.\n",
      "                  `init` is clipped to `bounds` before use.\n",
      "        \n",
      "            The default is 'latinhypercube'. Latin Hypercube sampling tries to\n",
      "            maximize coverage of the available parameter space. 'random'\n",
      "            initializes the population randomly - this has the drawback that\n",
      "            clustering can occur, preventing the whole of parameter space being\n",
      "            covered. Use of an array to specify a population subset could be used,\n",
      "            for example, to create a tight bunch of initial guesses in an location\n",
      "            where the solution is known to exist, thereby reducing time for\n",
      "            convergence.\n",
      "        atol : float, optional\n",
      "            Absolute tolerance for convergence, the solving stops when\n",
      "            ``np.std(pop) <= atol + tol * np.abs(np.mean(population_energies))``,\n",
      "            where and `atol` and `tol` are the absolute and relative tolerance\n",
      "            respectively.\n",
      "        updating : {'immediate', 'deferred'}, optional\n",
      "            If ``'immediate'``, the best solution vector is continuously updated\n",
      "            within a single generation [4]_. This can lead to faster convergence as\n",
      "            trial vectors can take advantage of continuous improvements in the best\n",
      "            solution.\n",
      "            With ``'deferred'``, the best solution vector is updated once per\n",
      "            generation. Only ``'deferred'`` is compatible with parallelization, and\n",
      "            the `workers` keyword can over-ride this option.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        workers : int or map-like callable, optional\n",
      "            If `workers` is an int the population is subdivided into `workers`\n",
      "            sections and evaluated in parallel\n",
      "            (uses `multiprocessing.Pool <multiprocessing>`).\n",
      "            Supply -1 to use all available CPU cores.\n",
      "            Alternatively supply a map-like callable, such as\n",
      "            `multiprocessing.Pool.map` for evaluating the population in parallel.\n",
      "            This evaluation is carried out as ``workers(func, iterable)``.\n",
      "            This option will override the `updating` keyword to\n",
      "            ``updating='deferred'`` if ``workers != 1``.\n",
      "            Requires that `func` be pickleable.\n",
      "        \n",
      "            .. versionadded:: 1.2.0\n",
      "        \n",
      "        constraints : {NonLinearConstraint, LinearConstraint, Bounds}\n",
      "            Constraints on the solver, over and above those applied by the `bounds`\n",
      "            kwd. Uses the approach by Lampinen [5]_.\n",
      "        \n",
      "            .. versionadded:: 1.4.0\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.  If `polish`\n",
      "            was employed, and a lower minimum was obtained by the polishing, then\n",
      "            OptimizeResult also contains the ``jac`` attribute.\n",
      "            If the eventual solution does not satisfy the applied constraints\n",
      "            ``success`` will be `False`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Differential evolution is a stochastic population based method that is\n",
      "        useful for global optimization problems. At each pass through the population\n",
      "        the algorithm mutates each candidate solution by mixing with other candidate\n",
      "        solutions to create a trial candidate. There are several strategies [2]_ for\n",
      "        creating trial candidates, which suit some problems more than others. The\n",
      "        'best1bin' strategy is a good starting point for many systems. In this\n",
      "        strategy two members of the population are randomly chosen. Their difference\n",
      "        is used to mutate the best member (the `best` in `best1bin`), :math:`b_0`,\n",
      "        so far:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            b' = b_0 + mutation * (population[rand0] - population[rand1])\n",
      "        \n",
      "        A trial vector is then constructed. Starting with a randomly chosen 'i'th\n",
      "        parameter the trial is sequentially filled (in modulo) with parameters from\n",
      "        ``b'`` or the original candidate. The choice of whether to use ``b'`` or the\n",
      "        original candidate is made with a binomial distribution (the 'bin' in\n",
      "        'best1bin') - a random number in [0, 1) is generated.  If this number is\n",
      "        less than the `recombination` constant then the parameter is loaded from\n",
      "        ``b'``, otherwise it is loaded from the original candidate.  The final\n",
      "        parameter is always loaded from ``b'``.  Once the trial candidate is built\n",
      "        its fitness is assessed. If the trial is better than the original candidate\n",
      "        then it takes its place. If it is also better than the best overall\n",
      "        candidate it also replaces that.\n",
      "        To improve your chances of finding a global minimum use higher `popsize`\n",
      "        values, with higher `mutation` and (dithering), but lower `recombination`\n",
      "        values. This has the effect of widening the search radius, but slowing\n",
      "        convergence.\n",
      "        By default the best solution vector is updated continuously within a single\n",
      "        iteration (``updating='immediate'``). This is a modification [4]_ of the\n",
      "        original differential evolution algorithm which can lead to faster\n",
      "        convergence as trial vectors can immediately benefit from improved\n",
      "        solutions. To use the original Storn and Price behaviour, updating the best\n",
      "        solution once per iteration, set ``updating='deferred'``.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function is implemented in `rosen` in `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, differential_evolution\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Now repeat, but with parallelization.\n",
      "        \n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = differential_evolution(rosen, bounds, updating='deferred',\n",
      "        ...                                 workers=2)\n",
      "        >>> result.x, result.fun\n",
      "        (array([1., 1., 1., 1., 1.]), 1.9216496320061384e-19)\n",
      "        \n",
      "        Let's try and do a constrained minimization\n",
      "        >>> from scipy.optimize import NonlinearConstraint, Bounds\n",
      "        >>> def constr_f(x):\n",
      "        ...     return np.array(x[0] + x[1])\n",
      "        >>>\n",
      "        >>> # the sum of x[0] and x[1] must be less than 1.9\n",
      "        >>> nlc = NonlinearConstraint(constr_f, -np.inf, 1.9)\n",
      "        >>> # specify limits using a `Bounds` object.\n",
      "        >>> bounds = Bounds([0., 0.], [2., 2.])\n",
      "        >>> result = differential_evolution(rosen, bounds, constraints=(nlc),\n",
      "        ...                                 seed=1)\n",
      "        >>> result.x, result.fun\n",
      "        (array([0.96633867, 0.93363577]), 0.0011361355854792312)\n",
      "        \n",
      "        Next find the minimum of the Ackley function\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization).\n",
      "        \n",
      "        >>> from scipy.optimize import differential_evolution\n",
      "        >>> import numpy as np\n",
      "        >>> def ackley(x):\n",
      "        ...     arg1 = -0.2 * np.sqrt(0.5 * (x[0] ** 2 + x[1] ** 2))\n",
      "        ...     arg2 = 0.5 * (np.cos(2. * np.pi * x[0]) + np.cos(2. * np.pi * x[1]))\n",
      "        ...     return -20. * np.exp(arg1) - np.exp(arg2) + 20. + np.e\n",
      "        >>> bounds = [(-5, 5), (-5, 5)]\n",
      "        >>> result = differential_evolution(ackley, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 0.,  0.]), 4.4408920985006262e-16)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Storn, R and Price, K, Differential Evolution - a Simple and\n",
      "               Efficient Heuristic for Global Optimization over Continuous Spaces,\n",
      "               Journal of Global Optimization, 1997, 11, 341 - 359.\n",
      "        .. [2] http://www1.icsi.berkeley.edu/~storn/code.html\n",
      "        .. [3] http://en.wikipedia.org/wiki/Differential_evolution\n",
      "        .. [4] Wormington, M., Panaccione, C., Matney, K. M., Bowen, D. K., -\n",
      "               Characterization of structures from X-ray scattering data using\n",
      "               genetic algorithms, Phil. Trans. R. Soc. Lond. A, 1999, 357,\n",
      "               2827-2848\n",
      "        .. [5] Lampinen, J., A constraint handling approach for the differential\n",
      "               evolution algorithm. Proceedings of the 2002 Congress on\n",
      "               Evolutionary Computation. CEC'02 (Cat. No. 02TH8600). Vol. 2. IEEE,\n",
      "               2002.\n",
      "    \n",
      "    dual_annealing(func, bounds, args=(), maxiter=1000, local_search_options={}, initial_temp=5230.0, restart_temp_ratio=2e-05, visit=2.62, accept=-5.0, maxfun=10000000.0, seed=None, no_local_search=False, callback=None, x0=None)\n",
      "        Find the global minimum of a function using Dual Annealing.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a  tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence, shape (n, 2)\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining bounds for the objective function parameter.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        maxiter : int, optional\n",
      "            The maximum number of global search iterations. Default value is 1000.\n",
      "        local_search_options : dict, optional\n",
      "            Extra keyword arguments to be passed to the local minimizer\n",
      "            (`minimize`). Some important options could be:\n",
      "            ``method`` for the minimizer method to use and ``args`` for\n",
      "            objective function additional arguments.\n",
      "        initial_temp : float, optional\n",
      "            The initial temperature, use higher values to facilitates a wider\n",
      "            search of the energy landscape, allowing dual_annealing to escape\n",
      "            local minima that it is trapped in. Default value is 5230. Range is\n",
      "            (0.01, 5.e4].\n",
      "        restart_temp_ratio : float, optional\n",
      "            During the annealing process, temperature is decreasing, when it\n",
      "            reaches ``initial_temp * restart_temp_ratio``, the reannealing process\n",
      "            is triggered. Default value of the ratio is 2e-5. Range is (0, 1).\n",
      "        visit : float, optional\n",
      "            Parameter for visiting distribution. Default value is 2.62. Higher\n",
      "            values give the visiting distribution a heavier tail, this makes\n",
      "            the algorithm jump to a more distant region. The value range is (0, 3].\n",
      "        accept : float, optional\n",
      "            Parameter for acceptance distribution. It is used to control the\n",
      "            probability of acceptance. The lower the acceptance parameter, the\n",
      "            smaller the probability of acceptance. Default value is -5.0 with\n",
      "            a range (-1e4, -5].\n",
      "        maxfun : int, optional\n",
      "            Soft limit for the number of objective function calls. If the\n",
      "            algorithm is in the middle of a local search, this number will be\n",
      "            exceeded, the algorithm will stop just after the local search is\n",
      "            done. Default value is 1e7.\n",
      "        seed : {int or `~numpy.random.mtrand.RandomState` instance}, optional\n",
      "            If `seed` is not specified the `~numpy.random.mtrand.RandomState`\n",
      "            singleton is used.\n",
      "            If `seed` is an int, a new ``RandomState`` instance is used,\n",
      "            seeded with `seed`.\n",
      "            If `seed` is already a ``RandomState`` instance, then that\n",
      "            instance is used.\n",
      "            Specify `seed` for repeatable minimizations. The random numbers\n",
      "            generated with this seed only affect the visiting distribution\n",
      "            function and new coordinates generation.\n",
      "        no_local_search : bool, optional\n",
      "            If `no_local_search` is set to True, a traditional Generalized\n",
      "            Simulated Annealing will be performed with no local search\n",
      "            strategy applied.\n",
      "        callback : callable, optional\n",
      "            A callback function with signature ``callback(x, f, context)``,\n",
      "            which will be called for all minima found.\n",
      "            ``x`` and ``f`` are the coordinates and function value of the\n",
      "            latest minimum found, and ``context`` has value in [0, 1, 2], with the\n",
      "            following meaning:\n",
      "        \n",
      "                - 0: minimum detected in the annealing process.\n",
      "                - 1: detection occurred in the local search process.\n",
      "                - 2: detection done in the dual annealing process.\n",
      "        \n",
      "            If the callback implementation returns True, the algorithm will stop.\n",
      "        x0 : ndarray, shape(n,), optional\n",
      "            Coordinates of a single n-dimensional starting point.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are: ``x`` the solution array, ``fun`` the value\n",
      "            of the function at the solution, and ``message`` which describes the\n",
      "            cause of the termination.\n",
      "            See `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements the Dual Annealing optimization. This stochastic\n",
      "        approach derived from [3]_ combines the generalization of CSA (Classical\n",
      "        Simulated Annealing) and FSA (Fast Simulated Annealing) [1]_ [2]_ coupled\n",
      "        to a strategy for applying a local search on accepted locations [4]_.\n",
      "        An alternative implementation of this same algorithm is described in [5]_\n",
      "        and benchmarks are presented in [6]_. This approach introduces an advanced\n",
      "        method to refine the solution found by the generalized annealing\n",
      "        process. This algorithm uses a distorted Cauchy-Lorentz visiting\n",
      "        distribution, with its shape controlled by the parameter :math:`q_{v}`\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            g_{q_{v}}(\\Delta x(t)) \\propto \\frac{ \\\n",
      "            \\left[T_{q_{v}}(t) \\right]^{-\\frac{D}{3-q_{v}}}}{ \\\n",
      "            \\left[{1+(q_{v}-1)\\frac{(\\Delta x(t))^{2}} { \\\n",
      "            \\left[T_{q_{v}}(t)\\right]^{\\frac{2}{3-q_{v}}}}}\\right]^{ \\\n",
      "            \\frac{1}{q_{v}-1}+\\frac{D-1}{2}}}\n",
      "        \n",
      "        Where :math:`t` is the artificial time. This visiting distribution is used\n",
      "        to generate a trial jump distance :math:`\\Delta x(t)` of variable\n",
      "        :math:`x(t)` under artificial temperature :math:`T_{q_{v}}(t)`.\n",
      "        \n",
      "        From the starting point, after calling the visiting distribution\n",
      "        function, the acceptance probability is computed as follows:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            p_{q_{a}} = \\min{\\{1,\\left[1-(1-q_{a}) \\beta \\Delta E \\right]^{ \\\n",
      "            \\frac{1}{1-q_{a}}}\\}}\n",
      "        \n",
      "        Where :math:`q_{a}` is a acceptance parameter. For :math:`q_{a}<1`, zero\n",
      "        acceptance probability is assigned to the cases where\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            [1-(1-q_{a}) \\beta \\Delta E] < 0\n",
      "        \n",
      "        The artificial temperature :math:`T_{q_{v}}(t)` is decreased according to\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            T_{q_{v}}(t) = T_{q_{v}}(1) \\frac{2^{q_{v}-1}-1}{\\left( \\\n",
      "            1 + t\\right)^{q_{v}-1}-1}\n",
      "        \n",
      "        Where :math:`q_{v}` is the visiting parameter.\n",
      "        \n",
      "        .. versionadded:: 1.2.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsallis C. Possible generalization of Boltzmann-Gibbs\n",
      "            statistics. Journal of Statistical Physics, 52, 479-487 (1998).\n",
      "        .. [2] Tsallis C, Stariolo DA. Generalized Simulated Annealing.\n",
      "            Physica A, 233, 395-406 (1996).\n",
      "        .. [3] Xiang Y, Sun DY, Fan W, Gong XG. Generalized Simulated\n",
      "            Annealing Algorithm and Its Application to the Thomson Model.\n",
      "            Physics Letters A, 233, 216-220 (1997).\n",
      "        .. [4] Xiang Y, Gong XG. Efficiency of Generalized Simulated\n",
      "            Annealing. Physical Review E, 62, 4473 (2000).\n",
      "        .. [5] Xiang Y, Gubian S, Suomela B, Hoeng J. Generalized\n",
      "            Simulated Annealing for Efficient Global Optimization: the GenSA\n",
      "            Package for R. The R Journal, Volume 5/1 (2013).\n",
      "        .. [6] Mullen, K. Continuous Global Optimization in R. Journal of\n",
      "            Statistical Software, 60(6), 1 - 45, (2014). DOI:10.18637/jss.v060.i06\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following example is a 10-dimensional problem, with many local minima.\n",
      "        The function involved is called Rastrigin\n",
      "        (https://en.wikipedia.org/wiki/Rastrigin_function)\n",
      "        \n",
      "        >>> from scipy.optimize import dual_annealing\n",
      "        >>> func = lambda x: np.sum(x*x - 10*np.cos(2*np.pi*x)) + 10*np.size(x)\n",
      "        >>> lw = [-5.12] * 10\n",
      "        >>> up = [5.12] * 10\n",
      "        >>> ret = dual_annealing(func, bounds=list(zip(lw, up)), seed=1234)\n",
      "        >>> print(\"global minimum: xmin = {0}, f(xmin) = {1:.6f}\".format(\n",
      "        ...       ret.x, ret.fun))\n",
      "        global minimum: xmin = [-4.26437714e-09 -3.91699361e-09 -1.86149218e-09 -3.97165720e-09\n",
      "         -6.29151648e-09 -6.53145322e-09 -3.93616815e-09 -6.55623025e-09\n",
      "        -6.05775280e-09 -5.00668935e-09], f(xmin) = 0.000000\n",
      "    \n",
      "    excitingmixing(F, xin, iter=None, alpha=None, alphamax=1.0, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a tuned diagonal Jacobian approximation.\n",
      "        \n",
      "        The Jacobian matrix is diagonal and is tuned on each iteration.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='excitingmixing'`` in particular.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            Initial Jacobian approximation is (-1/alpha).\n",
      "        alphamax : float, optional\n",
      "            The entries of the diagonal Jacobian are kept in the range\n",
      "            ``[alpha, alphamax]``.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "    \n",
      "    fixed_point(func, x0, args=(), xtol=1e-08, maxiter=500, method='del2')\n",
      "        Find a fixed point of the function.\n",
      "        \n",
      "        Given a function of one or more variables and a starting point, find a\n",
      "        fixed-point of the function: i.e. where ``func(x0) == x0``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : function\n",
      "            Function to evaluate.\n",
      "        x0 : array_like\n",
      "            Fixed point of function.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to `func`.\n",
      "        xtol : float, optional\n",
      "            Convergence tolerance, defaults to 1e-08.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations, defaults to 500.\n",
      "        method : {\"del2\", \"iteration\"}, optional\n",
      "            Method of finding the fixed-point, defaults to \"del2\"\n",
      "            which uses Steffensen's Method with Aitken's ``Del^2``\n",
      "            convergence acceleration [1]_. The \"iteration\" method simply iterates\n",
      "            the function until convergence is detected, without attempting to\n",
      "            accelerate the convergence.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Burden, Faires, \"Numerical Analysis\", 5th edition, pg. 80\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> def func(x, c1, c2):\n",
      "        ...    return np.sqrt(c1/(x+c2))\n",
      "        >>> c1 = np.array([10,12.])\n",
      "        >>> c2 = np.array([3, 5.])\n",
      "        >>> optimize.fixed_point(func, [1.2, 1.3], args=(c1,c2))\n",
      "        array([ 1.4920333 ,  1.37228132])\n",
      "    \n",
      "    fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, initial_simplex=None)\n",
      "        Minimize a function using the downhill simplex algorithm.\n",
      "        \n",
      "        This algorithm only uses function values, not derivatives or second\n",
      "        derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            The objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func, i.e. ``f(x,*args)``.\n",
      "        xtol : float, optional\n",
      "            Absolute error in xopt between iterations that is acceptable for\n",
      "            convergence.\n",
      "        ftol : number, optional\n",
      "            Absolute error in func(xopt) between iterations that is acceptable for\n",
      "            convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : number, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            Set to True if fopt and warnflag outputs are desired.\n",
      "        disp : bool, optional\n",
      "            Set to True to print convergence messages.\n",
      "        retall : bool, optional\n",
      "            Set to True to return list of solutions at each iteration.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        initial_simplex : array_like of shape (N + 1, N), optional\n",
      "            Initial simplex. If given, overrides `x0`.\n",
      "            ``initial_simplex[j,:]`` should contain the coordinates of\n",
      "            the j-th vertex of the ``N+1`` vertices in the simplex, where\n",
      "            ``N`` is the dimension.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter that minimizes function.\n",
      "        fopt : float\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        iter : int\n",
      "            Number of iterations performed.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            1 : Maximum number of function evaluations made.\n",
      "            2 : Maximum number of iterations reached.\n",
      "        allvecs : list\n",
      "            Solution at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Nelder-Mead' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a Nelder-Mead simplex algorithm to find the minimum of function of\n",
      "        one or more variables.\n",
      "        \n",
      "        This algorithm has a long history of successful use in applications.\n",
      "        But it will usually be slower than an algorithm that uses first or\n",
      "        second derivative information. In practice it can have poor\n",
      "        performance in high-dimensional problems and is not robust to\n",
      "        minimizing complicated functions. Additionally, there currently is no\n",
      "        complete theory describing when the algorithm will successfully\n",
      "        converge to the minimum, or how fast it will if it does. Both the ftol and\n",
      "        xtol criteria must be met for convergence.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin(f, 1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 17\n",
      "                 Function evaluations: 34\n",
      "        >>> minimum[0]\n",
      "        -8.8817841970012523e-16\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J.A. and Mead, R. (1965), \"A simplex method for function\n",
      "               minimization\", The Computer Journal, 7, pp. 308-313\n",
      "        \n",
      "        .. [2] Wright, M.H. (1996), \"Direct Search Methods: Once Scorned, Now\n",
      "               Respectable\", in Numerical Analysis 1995, Proceedings of the\n",
      "               1995 Dundee Biennial Conference in Numerical Analysis, D.F.\n",
      "               Griffiths and G.A. Watson (Eds.), Addison Wesley Longman,\n",
      "               Harlow, UK, pp. 191-208.\n",
      "    \n",
      "    fmin_bfgs(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using the BFGS algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable f'(x,*args), optional\n",
      "            Gradient of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f and fprime.\n",
      "        gtol : float, optional\n",
      "            Gradient norm must be less than gtol before successful termination.\n",
      "        norm : float, optional\n",
      "            Order of norm (Inf is max, -Inf is min)\n",
      "        epsilon : int or ndarray, optional\n",
      "            If fprime is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function to call after each\n",
      "            iteration.  Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True,return fopt, func_calls, grad_calls, and warnflag\n",
      "            in addition to xopt.\n",
      "        disp : bool, optional\n",
      "            Print convergence message if True.\n",
      "        retall : bool, optional\n",
      "            Return a list of results at each iteration if True.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. f(xopt) == fopt.\n",
      "        fopt : float\n",
      "            Minimum value.\n",
      "        gopt : ndarray\n",
      "            Value of gradient at minimum, f'(xopt), which should be near 0.\n",
      "        Bopt : ndarray\n",
      "            Value of 1/f''(xopt), i.e. the inverse hessian matrix.\n",
      "        func_calls : int\n",
      "            Number of function_calls made.\n",
      "        grad_calls : int\n",
      "            Number of gradient calls made.\n",
      "        warnflag : integer\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Gradient and/or function calls not changing.\n",
      "            3 : NaN result encountered.\n",
      "        allvecs  :  list\n",
      "            The value of xopt at each iteration.  Only returned if retall is True.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'BFGS' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Optimize the function, f, whose gradient is given by fprime\n",
      "        using the quasi-Newton method of Broyden, Fletcher, Goldfarb,\n",
      "        and Shanno (BFGS)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.\n",
      "    \n",
      "    fmin_cg(f, x0, fprime=None, args=(), gtol=1e-05, norm=inf, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Minimize a function using a nonlinear conjugate gradient algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable, ``f(x, *args)``\n",
      "            Objective function to be minimized.  Here `x` must be a 1-D array of\n",
      "            the variables that are to be changed in the search for a minimum, and\n",
      "            `args` are the other (fixed) parameters of `f`.\n",
      "        x0 : ndarray\n",
      "            A user-supplied initial estimate of `xopt`, the optimal value of `x`.\n",
      "            It must be a 1-D array of values.\n",
      "        fprime : callable, ``fprime(x, *args)``, optional\n",
      "            A function that returns the gradient of `f` at `x`. Here `x` and `args`\n",
      "            are as described above for `f`. The returned value must be a 1-D array.\n",
      "            Defaults to None, in which case the gradient is approximated\n",
      "            numerically (see `epsilon`, below).\n",
      "        args : tuple, optional\n",
      "            Parameter values passed to `f` and `fprime`. Must be supplied whenever\n",
      "            additional fixed parameters are needed to completely specify the\n",
      "            functions `f` and `fprime`.\n",
      "        gtol : float, optional\n",
      "            Stop when the norm of the gradient is less than `gtol`.\n",
      "        norm : float, optional\n",
      "            Order to use for the norm of the gradient\n",
      "            (``-np.Inf`` is min, ``np.Inf`` is max).\n",
      "        epsilon : float or ndarray, optional\n",
      "            Step size(s) to use when `fprime` is approximated numerically. Can be a\n",
      "            scalar or a 1-D array.  Defaults to ``sqrt(eps)``, with eps the\n",
      "            floating point machine precision.  Usually ``sqrt(eps)`` is about\n",
      "            1.5e-8.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform. Default is ``200 * len(x0)``.\n",
      "        full_output : bool, optional\n",
      "            If True, return `fopt`, `func_calls`, `grad_calls`, and `warnflag` in\n",
      "            addition to `xopt`.  See the Returns section below for additional\n",
      "            information on optional return values.\n",
      "        disp : bool, optional\n",
      "            If True, return a convergence message, followed by `xopt`.\n",
      "        retall : bool, optional\n",
      "            If True, add to the returned values the results of each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each iteration.\n",
      "            Called as ``callback(xk)``, where ``xk`` is the current value of `x0`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
      "        fopt : float, optional\n",
      "            Minimum value found, f(xopt).  Only returned if `full_output` is True.\n",
      "        func_calls : int, optional\n",
      "            The number of function_calls made.  Only returned if `full_output`\n",
      "            is True.\n",
      "        grad_calls : int, optional\n",
      "            The number of gradient calls made. Only returned if `full_output` is\n",
      "            True.\n",
      "        warnflag : int, optional\n",
      "            Integer value with warning status, only returned if `full_output` is\n",
      "            True.\n",
      "        \n",
      "            0 : Success.\n",
      "        \n",
      "            1 : The maximum number of iterations was exceeded.\n",
      "        \n",
      "            2 : Gradient and/or function calls were not changing.  May indicate\n",
      "                that precision was lost, i.e., the routine did not converge.\n",
      "        \n",
      "            3 : NaN result encountered.\n",
      "        \n",
      "        allvecs : list of ndarray, optional\n",
      "            List of arrays, containing the results at each iteration.\n",
      "            Only returned if `retall` is True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        minimize : common interface to all `scipy.optimize` algorithms for\n",
      "                   unconstrained and constrained minimization of multivariate\n",
      "                   functions.  It provides an alternative way to call\n",
      "                   ``fmin_cg``, by specifying ``method='CG'``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This conjugate gradient algorithm is based on that of Polak and Ribiere\n",
      "        [1]_.\n",
      "        \n",
      "        Conjugate gradient methods tend to work better when:\n",
      "        \n",
      "        1. `f` has a unique global minimizing point, and no local minima or\n",
      "           other stationary points,\n",
      "        2. `f` is, at least locally, reasonably well approximated by a\n",
      "           quadratic function of the variables,\n",
      "        3. `f` is continuous and has a continuous gradient,\n",
      "        4. `fprime` is not too large, e.g., has a norm less than 1000,\n",
      "        5. The initial guess, `x0`, is reasonably close to `f` 's global\n",
      "           minimizing point, `xopt`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Wright & Nocedal, \"Numerical Optimization\", 1999, pp. 120-122.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Example 1: seek the minimum value of the expression\n",
      "        ``a*u**2 + b*u*v + c*v**2 + d*u + e*v + f`` for given values\n",
      "        of the parameters and an initial guess ``(u, v) = (0, 0)``.\n",
      "        \n",
      "        >>> args = (2, 3, 7, 8, 9, 10)  # parameter values\n",
      "        >>> def f(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     return a*u**2 + b*u*v + c*v**2 + d*u + e*v + f\n",
      "        >>> def gradf(x, *args):\n",
      "        ...     u, v = x\n",
      "        ...     a, b, c, d, e, f = args\n",
      "        ...     gu = 2*a*u + b*v + d     # u-component of the gradient\n",
      "        ...     gv = b*u + 2*c*v + e     # v-component of the gradient\n",
      "        ...     return np.asarray((gu, gv))\n",
      "        >>> x0 = np.asarray((0, 0))  # Initial guess.\n",
      "        >>> from scipy import optimize\n",
      "        >>> res1 = optimize.fmin_cg(f, x0, fprime=gradf, args=args)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 1.617021\n",
      "                 Iterations: 4\n",
      "                 Function evaluations: 8\n",
      "                 Gradient evaluations: 8\n",
      "        >>> res1\n",
      "        array([-1.80851064, -0.25531915])\n",
      "        \n",
      "        Example 2: solve the same problem using the `minimize` function.\n",
      "        (This `myopts` dictionary shows all of the available options,\n",
      "        although in practice only non-default values would be needed.\n",
      "        The returned value will be a dictionary.)\n",
      "        \n",
      "        >>> opts = {'maxiter' : None,    # default value.\n",
      "        ...         'disp' : True,    # non-default value.\n",
      "        ...         'gtol' : 1e-5,    # default value.\n",
      "        ...         'norm' : np.inf,  # default value.\n",
      "        ...         'eps' : 1.4901161193847656e-08}  # default value.\n",
      "        >>> res2 = optimize.minimize(f, x0, jac=gradf, args=args,\n",
      "        ...                          method='CG', options=opts)\n",
      "        Optimization terminated successfully.\n",
      "                Current function value: 1.617021\n",
      "                Iterations: 4\n",
      "                Function evaluations: 8\n",
      "                Gradient evaluations: 8\n",
      "        >>> res2.x  # minimum found\n",
      "        array([-1.80851064, -0.25531915])\n",
      "    \n",
      "    fmin_cobyla(func, x0, cons, args=(), consargs=None, rhobeg=1.0, rhoend=0.0001, maxfun=1000, disp=None, catol=0.0002)\n",
      "        Minimize a function using the Constrained Optimization BY Linear\n",
      "        Approximation (COBYLA) method. This method wraps a FORTRAN\n",
      "        implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            Function to minimize. In the form func(x, \\*args).\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        cons : sequence\n",
      "            Constraint functions; must all be ``>=0`` (a single function\n",
      "            if only 1 constraint). Each function takes the parameters `x`\n",
      "            as its first argument, and it can return either a single number or\n",
      "            an array or list of numbers.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to pass to function.\n",
      "        consargs : tuple, optional\n",
      "            Extra arguments to pass to constraint functions (default of None means\n",
      "            use same extra arguments as those passed to func).\n",
      "            Use ``()`` for no extra arguments.\n",
      "        rhobeg : float, optional\n",
      "            Reasonable initial changes to the variables.\n",
      "        rhoend : float, optional\n",
      "            Final accuracy in the optimization (not precisely guaranteed). This\n",
      "            is a lower bound on the size of the trust region.\n",
      "        disp : {0, 1, 2, 3}, optional\n",
      "            Controls the frequency of output; 0 implies no output.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        catol : float, optional\n",
      "            Absolute tolerance for constraint violations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The argument that minimises `f`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'COBYLA' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This algorithm is based on linear approximations to the objective\n",
      "        function and each constraint. We briefly describe the algorithm.\n",
      "        \n",
      "        Suppose the function is being minimized over k variables. At the\n",
      "        jth iteration the algorithm has k+1 points v_1, ..., v_(k+1),\n",
      "        an approximate solution x_j, and a radius RHO_j.\n",
      "        (i.e. linear plus a constant) approximations to the objective\n",
      "        function and constraint functions such that their function values\n",
      "        agree with the linear approximation on the k+1 points v_1,.., v_(k+1).\n",
      "        This gives a linear program to solve (where the linear approximations\n",
      "        of the constraint functions are constrained to be non-negative).\n",
      "        \n",
      "        However the linear approximations are likely only good\n",
      "        approximations near the current simplex, so the linear program is\n",
      "        given the further requirement that the solution, which\n",
      "        will become x_(j+1), must be within RHO_j from x_j. RHO_j only\n",
      "        decreases, never increases. The initial RHO_j is rhobeg and the\n",
      "        final RHO_j is rhoend. In this way COBYLA's iterations behave\n",
      "        like a trust region algorithm.\n",
      "        \n",
      "        Additionally, the linear program may be inconsistent, or the\n",
      "        approximation may give poor improvement. For details about\n",
      "        how these issues are resolved, as well as how the points v_i are\n",
      "        updated, refer to the source code or the references below.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1994), \"A direct search optimization method that models\n",
      "        the objective and constraint functions by linear interpolation.\", in\n",
      "        Advances in Optimization and Numerical Analysis, eds. S. Gomez and\n",
      "        J-P Hennart, Kluwer Academic (Dordrecht), pp. 51-67\n",
      "        \n",
      "        Powell M.J.D. (1998), \"Direct search algorithms for optimization\n",
      "        calculations\", Acta Numerica 7, 287-336\n",
      "        \n",
      "        Powell M.J.D. (2007), \"A view of algorithms for optimization without\n",
      "        derivatives\", Cambridge University Technical Report DAMTP 2007/NA03\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Minimize the objective function f(x,y) = x*y subject\n",
      "        to the constraints x**2 + y**2 < 1 and y > 0::\n",
      "        \n",
      "            >>> def objective(x):\n",
      "            ...     return x[0]*x[1]\n",
      "            ...\n",
      "            >>> def constr1(x):\n",
      "            ...     return 1 - (x[0]**2 + x[1]**2)\n",
      "            ...\n",
      "            >>> def constr2(x):\n",
      "            ...     return x[1]\n",
      "            ...\n",
      "            >>> from scipy.optimize import fmin_cobyla\n",
      "            >>> fmin_cobyla(objective, [0.0, 0.1], [constr1, constr2], rhoend=1e-7)\n",
      "            array([-0.70710685,  0.70710671])\n",
      "        \n",
      "        The exact solution is (-sqrt(2)/2, sqrt(2)/2).\n",
      "    \n",
      "    fmin_l_bfgs_b(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, m=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08, iprint=-1, maxfun=15000, maxiter=15000, disp=None, callback=None, maxls=20)\n",
      "        Minimize a function func using the L-BFGS-B algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Function to minimise.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable fprime(x,*args), optional\n",
      "            The gradient of `func`.  If None, then `func` returns the function\n",
      "            value and the gradient (``f, g = func(x, *args)``), unless\n",
      "            `approx_grad` is True in which case `func` returns only ``f``.\n",
      "        args : sequence, optional\n",
      "            Arguments to pass to `func` and `fprime`.\n",
      "        approx_grad : bool, optional\n",
      "            Whether to approximate the gradient numerically (in which case\n",
      "            `func` returns only the function value).\n",
      "        bounds : list, optional\n",
      "            ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "            ``max`` when there is no bound in that direction.\n",
      "        m : int, optional\n",
      "            The maximum number of variable metric corrections\n",
      "            used to define the limited memory matrix. (The limited memory BFGS\n",
      "            method does not store the full hessian but uses this many terms in an\n",
      "            approximation to it.)\n",
      "        factr : float, optional\n",
      "            The iteration stops when\n",
      "            ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "            where ``eps`` is the machine precision, which is automatically\n",
      "            generated by the code. Typical values for `factr` are: 1e12 for\n",
      "            low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "            high accuracy. See Notes for relationship to `ftol`, which is exposed\n",
      "            (instead of `factr`) by the `scipy.optimize.minimize` interface to\n",
      "            L-BFGS-B.\n",
      "        pgtol : float, optional\n",
      "            The iteration will stop when\n",
      "            ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "            where ``pg_i`` is the i-th component of the projected gradient.\n",
      "        epsilon : float, optional\n",
      "            Step size used when `approx_grad` is True, for numerically\n",
      "            calculating the gradient\n",
      "        iprint : int, optional\n",
      "            Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "            ``iprint = 0``    print only one line at the last iteration;\n",
      "            ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "            ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "            ``iprint = 100``  print also the changes of active set and final x;\n",
      "            ``iprint > 100``  print details of every iteration including x and g.\n",
      "        disp : int, optional\n",
      "            If zero, then no output.  If a positive number, then this over-rides\n",
      "            `iprint` (i.e., `iprint` gets the value of `disp`).\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        maxls : int, optional\n",
      "            Maximum number of line search steps (per iteration). Default is 20.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : array_like\n",
      "            Estimated position of the minimum.\n",
      "        f : float\n",
      "            Value of `func` at the minimum.\n",
      "        d : dict\n",
      "            Information dictionary.\n",
      "        \n",
      "            * d['warnflag'] is\n",
      "        \n",
      "              - 0 if converged,\n",
      "              - 1 if too many function evaluations or too many iterations,\n",
      "              - 2 if stopped for another reason, given in d['task']\n",
      "        \n",
      "            * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "            * d['funcalls'] is the number of function calls made.\n",
      "            * d['nit'] is the number of iterations.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'L-BFGS-B' `method` in particular. Note that the\n",
      "            `ftol` option is made available via that interface, while `factr` is\n",
      "            provided via this interface, where `factr` is the factor multiplying\n",
      "            the default machine floating-point precision to arrive at `ftol`:\n",
      "            ``ftol = factr * numpy.finfo(float).eps``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        License of L-BFGS-B (FORTRAN code):\n",
      "        \n",
      "        The version included here (in fortran code) is 3.0\n",
      "        (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd,\n",
      "        and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
      "        condition for use:\n",
      "        \n",
      "        This software is freely available, but we expect that all publications\n",
      "        describing work using this software, or all commercial products using it,\n",
      "        quote at least one of the references given below. This software is released\n",
      "        under the BSD License.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "          Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "          Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "        * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "          ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "        * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "          FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "          ACM Transactions on Mathematical Software, 38, 1.\n",
      "    \n",
      "    fmin_ncg(f, x0, fprime, fhess_p=None, fhess=None, args=(), avextol=1e-05, epsilon=1.4901161193847656e-08, maxiter=None, full_output=0, disp=1, retall=0, callback=None)\n",
      "        Unconstrained minimization of a function using the Newton-CG method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable ``f(x, *args)``\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        fprime : callable ``f'(x, *args)``\n",
      "            Gradient of f.\n",
      "        fhess_p : callable ``fhess_p(x, p, *args)``, optional\n",
      "            Function which computes the Hessian of f times an\n",
      "            arbitrary vector, p.\n",
      "        fhess : callable ``fhess(x, *args)``, optional\n",
      "            Function to compute the Hessian matrix of f.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to f, fprime, fhess_p, and fhess\n",
      "            (the same set of extra arguments is supplied to all of\n",
      "            these functions).\n",
      "        epsilon : float or ndarray, optional\n",
      "            If fhess is approximated, use this value for the step size.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function which is called after\n",
      "            each iteration.  Called as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        avextol : float, optional\n",
      "            Convergence is assumed when the average relative error in\n",
      "            the minimizer falls below this amount.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        full_output : bool, optional\n",
      "            If True, return the optional outputs.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence message.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of results at each iteration.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters which minimize f, i.e. ``f(xopt) == fopt``.\n",
      "        fopt : float\n",
      "            Value of the function at xopt, i.e. ``fopt = f(xopt)``.\n",
      "        fcalls : int\n",
      "            Number of function calls made.\n",
      "        gcalls : int\n",
      "            Number of gradient calls made.\n",
      "        hcalls : int\n",
      "            Number of hessian calls made.\n",
      "        warnflag : int\n",
      "            Warnings generated by the algorithm.\n",
      "            1 : Maximum number of iterations exceeded.\n",
      "            2 : Line search failure (precision loss).\n",
      "            3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            The result at each iteration, if retall is True (see below).\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'Newton-CG' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Only one of `fhess_p` or `fhess` need to be given.  If `fhess`\n",
      "        is provided, then `fhess_p` will be ignored.  If neither `fhess`\n",
      "        nor `fhess_p` is provided, then the hessian product will be\n",
      "        approximated using finite differences on `fprime`. `fhess_p`\n",
      "        must compute the hessian times an arbitrary vector. If it is not\n",
      "        given, finite-differences on `fprime` are used to compute\n",
      "        it.\n",
      "        \n",
      "        Newton-CG methods are also called truncated Newton methods. This\n",
      "        function differs from scipy.optimize.fmin_tnc because\n",
      "        \n",
      "        1. scipy.optimize.fmin_ncg is written purely in python using numpy\n",
      "            and scipy while scipy.optimize.fmin_tnc calls a C function.\n",
      "        2. scipy.optimize.fmin_ncg is only for unconstrained minimization\n",
      "            while scipy.optimize.fmin_tnc is for unconstrained minimization\n",
      "            or box constrained minimization. (Box constraints give\n",
      "            lower and upper bounds for each variable separately.)\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright & Nocedal, 'Numerical Optimization', 1999, pg. 140.\n",
      "    \n",
      "    fmin_powell(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None, full_output=0, disp=1, retall=0, callback=None, direc=None)\n",
      "        Minimize a function using modified Powell's method.\n",
      "        \n",
      "        This method only uses function values, not derivatives.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to func.\n",
      "        xtol : float, optional\n",
      "            Line-search error tolerance.\n",
      "        ftol : float, optional\n",
      "            Relative error in ``func(xopt)`` acceptable for convergence.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations to make.\n",
      "        full_output : bool, optional\n",
      "            If True, ``fopt``, ``xi``, ``direc``, ``iter``, ``funcalls``, and\n",
      "            ``warnflag`` are returned.\n",
      "        disp : bool, optional\n",
      "            If True, print convergence messages.\n",
      "        retall : bool, optional\n",
      "            If True, return a list of the solution at each iteration.\n",
      "        callback : callable, optional\n",
      "            An optional user-supplied function, called after each\n",
      "            iteration.  Called as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        direc : ndarray, optional\n",
      "            Initial fitting step and parameter order set as an (N, N) array, where N\n",
      "            is the number of fitting parameters in `x0`.  Defaults to step size 1.0\n",
      "            fitting all parameters simultaneously (``np.ones((N, N))``).  To\n",
      "            prevent initial consideration of values in a step or to change initial\n",
      "            step size, set to 0 or desired step size in the Jth position in the Mth\n",
      "            block, where J is the position in `x0` and M is the desired evaluation\n",
      "            step, with steps being evaluated in index order.  Step size and ordering\n",
      "            will change freely as minimization proceeds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameter which minimizes `func`.\n",
      "        fopt : number\n",
      "            Value of function at minimum: ``fopt = func(xopt)``.\n",
      "        direc : ndarray\n",
      "            Current direction set.\n",
      "        iter : int\n",
      "            Number of iterations.\n",
      "        funcalls : int\n",
      "            Number of function calls made.\n",
      "        warnflag : int\n",
      "            Integer warning flag:\n",
      "                1 : Maximum number of function evaluations.\n",
      "                2 : Maximum number of iterations.\n",
      "                3 : NaN result encountered.\n",
      "        allvecs : list\n",
      "            List of solutions at each iteration.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to unconstrained minimization algorithms for\n",
      "            multivariate functions. See the 'Powell' method in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses a modification of Powell's method to find the minimum of\n",
      "        a function of N variables. Powell's method is a conjugate\n",
      "        direction method.\n",
      "        \n",
      "        The algorithm has two loops.  The outer loop merely iterates over the inner\n",
      "        loop. The inner loop minimizes over each current direction in the direction\n",
      "        set. At the end of the inner loop, if certain conditions are met, the\n",
      "        direction that gave the largest decrease is dropped and replaced with the\n",
      "        difference between the current estimated x and the estimated x from the\n",
      "        beginning of the inner-loop.\n",
      "        \n",
      "        The technical conditions for replacing the direction of greatest\n",
      "        increase amount to checking that\n",
      "        \n",
      "        1. No further gain can be made along the direction of greatest increase\n",
      "           from that iteration.\n",
      "        2. The direction of greatest increase accounted for a large sufficient\n",
      "           fraction of the decrease in the function value from that iteration of\n",
      "           the inner loop.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Powell M.J.D. (1964) An efficient method for finding the minimum of a\n",
      "        function of several variables without calculating derivatives,\n",
      "        Computer Journal, 7 (2):155-162.\n",
      "        \n",
      "        Press W., Teukolsky S.A., Vetterling W.T., and Flannery B.P.:\n",
      "        Numerical Recipes (any edition), Cambridge University Press\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fmin_powell(f, -1)\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 2\n",
      "                 Function evaluations: 18\n",
      "        >>> minimum\n",
      "        array(0.0)\n",
      "    \n",
      "    fmin_slsqp(func, x0, eqcons=(), f_eqcons=None, ieqcons=(), f_ieqcons=None, bounds=(), fprime=None, fprime_eqcons=None, fprime_ieqcons=None, args=(), iter=100, acc=1e-06, iprint=1, disp=None, full_output=0, epsilon=1.4901161193847656e-08, callback=None)\n",
      "        Minimize a function using Sequential Least SQuares Programming\n",
      "        \n",
      "        Python interface function for the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function.  Must return a scalar.\n",
      "        x0 : 1-D ndarray of float\n",
      "            Initial guess for the independent variable(s).\n",
      "        eqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            eqcons[j](x,*args) == 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_eqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D array in which each element must equal 0.0 in a\n",
      "            successfully optimized problem.  If f_eqcons is specified,\n",
      "            eqcons is ignored.\n",
      "        ieqcons : list, optional\n",
      "            A list of functions of length n such that\n",
      "            ieqcons[j](x,*args) >= 0.0 in a successfully optimized\n",
      "            problem.\n",
      "        f_ieqcons : callable f(x,*args), optional\n",
      "            Returns a 1-D ndarray in which each element must be greater or\n",
      "            equal to 0.0 in a successfully optimized problem.  If\n",
      "            f_ieqcons is specified, ieqcons is ignored.\n",
      "        bounds : list, optional\n",
      "            A list of tuples specifying the lower and upper bound\n",
      "            for each independent variable [(xl0, xu0),(xl1, xu1),...]\n",
      "            Infinite values will be interpreted as large floating values.\n",
      "        fprime : callable `f(x,*args)`, optional\n",
      "            A function that evaluates the partial derivatives of func.\n",
      "        fprime_eqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of equality constraint normals.  If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_eqcons should be sized as ( len(eqcons), len(x0) ).\n",
      "        fprime_ieqcons : callable `f(x,*args)`, optional\n",
      "            A function of the form `f(x, *args)` that returns the m by n\n",
      "            array of inequality constraint normals.  If not provided,\n",
      "            the normals will be approximated. The array returned by\n",
      "            fprime_ieqcons should be sized as ( len(ieqcons), len(x0) ).\n",
      "        args : sequence, optional\n",
      "            Additional arguments passed to func and fprime.\n",
      "        iter : int, optional\n",
      "            The maximum number of iterations.\n",
      "        acc : float, optional\n",
      "            Requested accuracy.\n",
      "        iprint : int, optional\n",
      "            The verbosity of fmin_slsqp :\n",
      "        \n",
      "            * iprint <= 0 : Silent operation\n",
      "            * iprint == 1 : Print summary upon completion (default)\n",
      "            * iprint >= 2 : Print status of each iterate and summary\n",
      "        disp : int, optional\n",
      "            Over-rides the iprint interface (preferred).\n",
      "        full_output : bool, optional\n",
      "            If False, return only the minimizer of func (default).\n",
      "            Otherwise, output final objective function and summary\n",
      "            information.\n",
      "        epsilon : float, optional\n",
      "            The step size for finite-difference derivative estimates.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(x)``, where ``x`` is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        out : ndarray of float\n",
      "            The final minimizer of func.\n",
      "        fx : ndarray of float, if full_output is true\n",
      "            The final value of the objective function.\n",
      "        its : int, if full_output is true\n",
      "            The number of iterations.\n",
      "        imode : int, if full_output is true\n",
      "            The exit mode from the optimizer (see below).\n",
      "        smode : string, if full_output is true\n",
      "            Message describing the exit mode from the optimizer.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'SLSQP' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Exit modes are defined as follows ::\n",
      "        \n",
      "            -1 : Gradient evaluation required (g & a)\n",
      "             0 : Optimization terminated successfully.\n",
      "             1 : Function evaluation required (f & c)\n",
      "             2 : More equality constraints than independent variables\n",
      "             3 : More than 3*n iterations in LSQ subproblem\n",
      "             4 : Inequality constraints incompatible\n",
      "             5 : Singular matrix E in LSQ subproblem\n",
      "             6 : Singular matrix C in LSQ subproblem\n",
      "             7 : Rank-deficient equality constraint subproblem HFTI\n",
      "             8 : Positive directional derivative for linesearch\n",
      "             9 : Iteration limit exceeded\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Examples are given :ref:`in the tutorial <tutorial-sqlsp>`.\n",
      "    \n",
      "    fmin_tnc(func, x0, fprime=None, args=(), approx_grad=0, bounds=None, epsilon=1e-08, scale=None, offset=None, messages=15, maxCGit=-1, maxfun=None, eta=-1, stepmx=0, accuracy=0, fmin=0, ftol=-1, xtol=-1, pgtol=-1, rescale=-1, disp=None, callback=None)\n",
      "        Minimize a function with variables subject to bounds, using\n",
      "        gradient information in a truncated Newton algorithm. This\n",
      "        method wraps a C implementation of the algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``func(x, *args)``\n",
      "            Function to minimize.  Must do one of:\n",
      "        \n",
      "            1. Return f and g, where f is the value of the function and g its\n",
      "               gradient (a list of floats).\n",
      "        \n",
      "            2. Return the function value but supply gradient function\n",
      "               separately as `fprime`.\n",
      "        \n",
      "            3. Return the function value and set ``approx_grad=True``.\n",
      "        \n",
      "            If the function returns None, the minimization\n",
      "            is aborted.\n",
      "        x0 : array_like\n",
      "            Initial estimate of minimum.\n",
      "        fprime : callable ``fprime(x, *args)``, optional\n",
      "            Gradient of `func`. If None, then either `func` must return the\n",
      "            function value and the gradient (``f,g = func(x, *args)``)\n",
      "            or `approx_grad` must be True.\n",
      "        args : tuple, optional\n",
      "            Arguments to pass to function.\n",
      "        approx_grad : bool, optional\n",
      "            If true, approximate the gradient numerically.\n",
      "        bounds : list, optional\n",
      "            (min, max) pairs for each element in x0, defining the\n",
      "            bounds on that parameter. Use None or +/-inf for one of\n",
      "            min or max when there is no bound in that direction.\n",
      "        epsilon : float, optional\n",
      "            Used if approx_grad is True. The stepsize in a finite\n",
      "            difference approximation for fprime.\n",
      "        scale : array_like, optional\n",
      "            Scaling factors to apply to each variable.  If None, the\n",
      "            factors are up-low for interval bounded variables and\n",
      "            1+|x| for the others.  Defaults to None.\n",
      "        offset : array_like, optional\n",
      "            Value to subtract from each variable.  If None, the\n",
      "            offsets are (up+low)/2 for interval bounded variables\n",
      "            and x for the others.\n",
      "        messages : int, optional\n",
      "            Bit mask used to select messages display during\n",
      "            minimization values defined in the MSGS dict.  Defaults to\n",
      "            MGS_ALL.\n",
      "        disp : int, optional\n",
      "            Integer interface to messages.  0 = no message, 5 = all messages\n",
      "        maxCGit : int, optional\n",
      "            Maximum number of hessian*vector evaluations per main\n",
      "            iteration.  If maxCGit == 0, the direction chosen is\n",
      "            -gradient if maxCGit < 0, maxCGit is set to\n",
      "            max(1,min(50,n/2)).  Defaults to -1.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluation.  if None, maxfun is\n",
      "            set to max(100, 10*len(x0)).  Defaults to None.\n",
      "        eta : float, optional\n",
      "            Severity of the line search. if < 0 or > 1, set to 0.25.\n",
      "            Defaults to -1.\n",
      "        stepmx : float, optional\n",
      "            Maximum step for the line search.  May be increased during\n",
      "            call.  If too small, it will be set to 10.0.  Defaults to 0.\n",
      "        accuracy : float, optional\n",
      "            Relative precision for finite difference calculations.  If\n",
      "            <= machine_precision, set to sqrt(machine_precision).\n",
      "            Defaults to 0.\n",
      "        fmin : float, optional\n",
      "            Minimum function value estimate.  Defaults to 0.\n",
      "        ftol : float, optional\n",
      "            Precision goal for the value of f in the stopping criterion.\n",
      "            If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "        xtol : float, optional\n",
      "            Precision goal for the value of x in the stopping\n",
      "            criterion (after applying x scaling factors).  If xtol <\n",
      "            0.0, xtol is set to sqrt(machine_precision).  Defaults to\n",
      "            -1.\n",
      "        pgtol : float, optional\n",
      "            Precision goal for the value of the projected gradient in\n",
      "            the stopping criterion (after applying x scaling factors).\n",
      "            If pgtol < 0.0, pgtol is set to 1e-2 * sqrt(accuracy).\n",
      "            Setting it to 0.0 is not recommended.  Defaults to -1.\n",
      "        rescale : float, optional\n",
      "            Scaling factor (in log10) used to trigger f value\n",
      "            rescaling.  If 0, rescale at each iteration.  If a large\n",
      "            value, never rescale.  If < 0, rescale is set to 1.3.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as callback(xk), where xk is the\n",
      "            current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution.\n",
      "        nfeval : int\n",
      "            The number of function evaluations.\n",
      "        rc : int\n",
      "            Return code, see below\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize: Interface to minimization algorithms for multivariate\n",
      "            functions. See the 'TNC' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The underlying algorithm is truncated Newton, also called\n",
      "        Newton Conjugate-Gradient. This method differs from\n",
      "        scipy.optimize.fmin_ncg in that\n",
      "        \n",
      "        1. It wraps a C implementation of the algorithm\n",
      "        2. It allows each variable to be given an upper and lower bound.\n",
      "        \n",
      "        The algorithm incorporates the bound constraints by determining\n",
      "        the descent direction as in an unconstrained truncated Newton,\n",
      "        but never taking a step-size large enough to leave the space\n",
      "        of feasible x's. The algorithm keeps track of a set of\n",
      "        currently active constraints, and ignores them when computing\n",
      "        the minimum allowable step size. (The x's associated with the\n",
      "        active constraint are kept fixed.) If the maximum allowable\n",
      "        step size is zero then a new constraint is added. At the end\n",
      "        of each iteration one of the constraints may be deemed no\n",
      "        longer active and removed. A constraint is considered\n",
      "        no longer active is if it is currently active\n",
      "        but the gradient for that variable points inward from the\n",
      "        constraint. The specific constraint removed is the one\n",
      "        associated with the variable of largest index whose\n",
      "        constraint is no longer active.\n",
      "        \n",
      "        Return codes are defined as follows::\n",
      "        \n",
      "            -1 : Infeasible (lower bound > upper bound)\n",
      "             0 : Local minimum reached (|pg| ~= 0)\n",
      "             1 : Converged (|f_n-f_(n-1)| ~= 0)\n",
      "             2 : Converged (|x_n-x_(n-1)| ~= 0)\n",
      "             3 : Max. number of function evaluations reached\n",
      "             4 : Linear search failed\n",
      "             5 : All lower bounds are equal to the upper bounds\n",
      "             6 : Unable to progress\n",
      "             7 : User requested end of minimization\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Wright S., Nocedal J. (2006), 'Numerical Optimization'\n",
      "        \n",
      "        Nash S.G. (1984), \"Newton-Type Minimization Via the Lanczos Method\",\n",
      "        SIAM Journal of Numerical Analysis 21, pp. 770-778\n",
      "    \n",
      "    fminbound(func, x1, x2, args=(), xtol=1e-05, maxfun=500, full_output=0, disp=1)\n",
      "        Bounded minimization for scalar functions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable f(x,*args)\n",
      "            Objective function to be minimized (must accept and return scalars).\n",
      "        x1, x2 : float or array scalar\n",
      "            The optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to function.\n",
      "        xtol : float, optional\n",
      "            The convergence tolerance.\n",
      "        maxfun : int, optional\n",
      "            Maximum number of function evaluations allowed.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        disp : int, optional\n",
      "            If non-zero, print messages.\n",
      "                0 : no message printing.\n",
      "                1 : non-convergence notification messages only.\n",
      "                2 : print a message on convergence too.\n",
      "                3 : print iteration results.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xopt : ndarray\n",
      "            Parameters (over given interval) which minimize the\n",
      "            objective function.\n",
      "        fval : number\n",
      "            The function value at the minimum point.\n",
      "        ierr : int\n",
      "            An error flag (0 if converged, 1 if maximum number of\n",
      "            function calls reached).\n",
      "        numfunc : int\n",
      "          The number of function calls made.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Bounded' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Finds a local minimizer of the scalar function `func` in the\n",
      "        interval x1 < xopt < x2 using Brent's method.  (See `brent`\n",
      "        for auto-bracketing).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        `fminbound` finds the minimum of the function in the given range.\n",
      "        The following examples illustrate the same\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.fminbound(f, -1, 2)\n",
      "        >>> minimum\n",
      "        0.0\n",
      "        >>> minimum = optimize.fminbound(f, 1, 2)\n",
      "        >>> minimum\n",
      "        1.0000059608609866\n",
      "    \n",
      "    fsolve(func, x0, args=(), fprime=None, full_output=0, col_deriv=0, xtol=1.49012e-08, maxfev=0, band=None, epsfcn=None, factor=100, diag=None)\n",
      "        Find the roots of a function.\n",
      "        \n",
      "        Return the roots of the (non-linear) equations defined by\n",
      "        ``func(x) = 0`` given a starting estimate.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable ``f(x, *args)``\n",
      "            A function that takes at least one (possibly vector) argument,\n",
      "            and returns a value of the same length.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the roots of ``func(x) = 0``.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to `func`.\n",
      "        fprime : callable ``f(x, *args)``, optional\n",
      "            A function to compute the Jacobian of `func` with derivatives\n",
      "            across the rows. By default, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            Specify whether the Jacobian function computes derivatives down\n",
      "            the columns (faster, because there is no transpose operation).\n",
      "        xtol : float, optional\n",
      "            The calculation will terminate if the relative error between two\n",
      "            consecutive iterates is at most `xtol`.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If zero, then\n",
      "            ``100*(N+1)`` is the maximum where N is the number of elements\n",
      "            in `x0`.\n",
      "        band : tuple, optional\n",
      "            If set to a two-sequence containing the number of sub- and\n",
      "            super-diagonals within the band of the Jacobi matrix, the\n",
      "            Jacobi matrix is considered banded (only for ``fprime=None``).\n",
      "        epsfcn : float, optional\n",
      "            A suitable step length for the forward-difference\n",
      "            approximation of the Jacobian (for ``fprime=None``). If\n",
      "            `epsfcn` is less than the machine precision, it is assumed\n",
      "            that the relative errors in the functions are of the order of\n",
      "            the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``).  Should be in the interval\n",
      "            ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the\n",
      "            variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for\n",
      "            an unsuccessful call).\n",
      "        infodict : dict\n",
      "            A dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                number of function calls\n",
      "            ``njev``\n",
      "                number of Jacobian calls\n",
      "            ``fvec``\n",
      "                function evaluated at the output\n",
      "            ``fjac``\n",
      "                the orthogonal matrix, q, produced by the QR\n",
      "                factorization of the final approximate Jacobian\n",
      "                matrix, stored column wise\n",
      "            ``r``\n",
      "                upper triangular matrix produced by QR factorization\n",
      "                of the same matrix\n",
      "            ``qtf``\n",
      "                the vector ``(transpose(q) * fvec)``\n",
      "        \n",
      "        ier : int\n",
      "            An integer flag.  Set to 1 if a solution was found, otherwise refer\n",
      "            to `mesg` for more information.\n",
      "        mesg : str\n",
      "            If no solution is found, `mesg` details the cause of failure.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See the ``method=='hybr'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        ``fsolve`` is a wrapper around MINPACK's hybrd and hybrj algorithms.\n",
      "    \n",
      "    golden(func, args=(), brack=None, tol=1.4901161193847656e-08, full_output=0, maxiter=5000)\n",
      "        Return the minimum of a function of one variable using golden section\n",
      "        method.\n",
      "        \n",
      "        Given a function of one variable and a possible bracketing interval,\n",
      "        return the minimum of the function isolated to a fractional precision of\n",
      "        tol.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable func(x,*args)\n",
      "            Objective function to minimize.\n",
      "        args : tuple, optional\n",
      "            Additional arguments (if present), passed to func.\n",
      "        brack : tuple, optional\n",
      "            Triple (a,b,c), where (a<b<c) and func(b) <\n",
      "            func(a),func(c).  If bracket consists of two numbers (a,\n",
      "            c), then they are assumed to be a starting interval for a\n",
      "            downhill bracket search (see `bracket`); it doesn't always\n",
      "            mean that obtained solution will satisfy a<=x<=c.\n",
      "        tol : float, optional\n",
      "            x tolerance stop criterion\n",
      "        full_output : bool, optional\n",
      "            If True, return optional outputs.\n",
      "        maxiter : int\n",
      "            Maximum number of iterations to perform.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar: Interface to minimization algorithms for scalar\n",
      "            univariate functions. See the 'Golden' `method` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses analog of bisection method to decrease the bracketed\n",
      "        interval.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        We illustrate the behaviour of the function when `brack` is of\n",
      "        size 2 and 3 respectively. In the case where `brack` is of the\n",
      "        form (xa,xb), we can see for the given values, the output need\n",
      "        not necessarily lie in the range ``(xa, xb)``.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return x**2\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> minimum = optimize.golden(f, brack=(1, 2))\n",
      "        >>> minimum\n",
      "        1.5717277788484873e-162\n",
      "        >>> minimum = optimize.golden(f, brack=(-1, 0.5, 2))\n",
      "        >>> minimum\n",
      "        -1.5717277788484873e-162\n",
      "    \n",
      "    least_squares(fun, x0, jac='2-point', bounds=(-inf, inf), method='trf', ftol=1e-08, xtol=1e-08, gtol=1e-08, x_scale=1.0, loss='linear', f_scale=1.0, diff_step=None, tr_solver=None, tr_options={}, jac_sparsity=None, max_nfev=None, verbose=0, args=(), kwargs={})\n",
      "        Solve a nonlinear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given the residuals f(x) (an m-dimensional real function of n real\n",
      "        variables) and the loss function rho(s) (a scalar function), `least_squares`\n",
      "        finds a local minimum of the cost function F(x)::\n",
      "        \n",
      "            minimize F(x) = 0.5 * sum(rho(f_i(x)**2), i = 0, ..., m - 1)\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        The purpose of the loss function rho(s) is to reduce the influence of\n",
      "        outliers on the solution.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Function which computes the vector of residuals, with the signature\n",
      "            ``fun(x, *args, **kwargs)``, i.e., the minimization proceeds with\n",
      "            respect to its first argument. The argument ``x`` passed to this\n",
      "            function is an ndarray of shape (n,) (never a scalar, even for n=1).\n",
      "            It must return a 1-d array_like of shape (m,) or a scalar. If the\n",
      "            argument ``x`` is complex or the function ``fun`` returns complex\n",
      "            residuals, it must be wrapped in a real function of real arguments,\n",
      "            as shown at the end of the Examples section.\n",
      "        x0 : array_like with shape (n,) or float\n",
      "            Initial guess on independent variables. If float, it will be treated\n",
      "            as a 1-d array with one element.\n",
      "        jac : {'2-point', '3-point', 'cs', callable}, optional\n",
      "            Method of computing the Jacobian matrix (an m-by-n matrix, where\n",
      "            element (i, j) is the partial derivative of f[i] with respect to\n",
      "            x[j]). The keywords select a finite difference scheme for numerical\n",
      "            estimation. The scheme '3-point' is more accurate, but requires\n",
      "            twice as many operations as '2-point' (default). The scheme 'cs'\n",
      "            uses complex steps, and while potentially the most accurate, it is\n",
      "            applicable only when `fun` correctly handles complex inputs and\n",
      "            can be analytically continued to the complex plane. Method 'lm'\n",
      "            always uses the '2-point' scheme. If callable, it is used as\n",
      "            ``jac(x, *args, **kwargs)`` and should return a good approximation\n",
      "            (or the exact value) for the Jacobian as an array_like (np.atleast_2d\n",
      "            is applied), a sparse matrix or a `scipy.sparse.linalg.LinearOperator`.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must match the size of `x0` or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : {'trf', 'dogbox', 'lm'}, optional\n",
      "            Algorithm to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm, particularly suitable\n",
      "                  for large sparse problems with bounds. Generally robust method.\n",
      "                * 'dogbox' : dogleg algorithm with rectangular trust regions,\n",
      "                  typical use case is small problems with bounds. Not recommended\n",
      "                  for problems with rank-deficient Jacobian.\n",
      "                * 'lm' : Levenberg-Marquardt algorithm as implemented in MINPACK.\n",
      "                  Doesn't handle bounds and sparse Jacobians. Usually the most\n",
      "                  efficient method for small unconstrained problems.\n",
      "        \n",
      "            Default is 'trf'. See Notes for more information.\n",
      "        ftol : float or None, optional\n",
      "            Tolerance for termination by the change of the cost function. Default\n",
      "            is 1e-8. The optimization process is stopped when  ``dF < ftol * F``,\n",
      "            and there was an adequate agreement between a local quadratic model and\n",
      "            the true model in the last step. If None, the termination by this\n",
      "            condition is disabled.\n",
      "        xtol : float or None, optional\n",
      "            Tolerance for termination by the change of the independent variables.\n",
      "            Default is 1e-8. The exact condition depends on the `method` used:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : ``norm(dx) < xtol * (xtol + norm(x))``\n",
      "                * For 'lm' : ``Delta < xtol * norm(xs)``, where ``Delta`` is\n",
      "                  a trust-region radius and ``xs`` is the value of ``x``\n",
      "                  scaled according to `x_scale` parameter (see below).\n",
      "        \n",
      "            If None, the termination by this condition is disabled.\n",
      "        gtol : float or None, optional\n",
      "            Tolerance for termination by the norm of the gradient. Default is 1e-8.\n",
      "            The exact condition depends on a `method` used:\n",
      "        \n",
      "                * For 'trf' : ``norm(g_scaled, ord=np.inf) < gtol``, where\n",
      "                  ``g_scaled`` is the value of the gradient scaled to account for\n",
      "                  the presence of the bounds [STIR]_.\n",
      "                * For 'dogbox' : ``norm(g_free, ord=np.inf) < gtol``, where\n",
      "                  ``g_free`` is the gradient with respect to the variables which\n",
      "                  are not in the optimal state on the boundary.\n",
      "                * For 'lm' : the maximum absolute value of the cosine of angles\n",
      "                  between columns of the Jacobian and the residual vector is less\n",
      "                  than `gtol`, or the residual vector is zero.\n",
      "        \n",
      "            If None, the termination by this condition is disabled.\n",
      "        x_scale : array_like or 'jac', optional\n",
      "            Characteristic scale of each variable. Setting `x_scale` is equivalent\n",
      "            to reformulating the problem in scaled variables ``xs = x / x_scale``.\n",
      "            An alternative view is that the size of a trust region along j-th\n",
      "            dimension is proportional to ``x_scale[j]``. Improved convergence may\n",
      "            be achieved by setting `x_scale` such that a step of a given size\n",
      "            along any of the scaled variables has a similar effect on the cost\n",
      "            function. If set to 'jac', the scale is iteratively updated using the\n",
      "            inverse norms of the columns of the Jacobian matrix (as described in\n",
      "            [JJMore]_).\n",
      "        loss : str or callable, optional\n",
      "            Determines the loss function. The following keyword values are allowed:\n",
      "        \n",
      "                * 'linear' (default) : ``rho(z) = z``. Gives a standard\n",
      "                  least-squares problem.\n",
      "                * 'soft_l1' : ``rho(z) = 2 * ((1 + z)**0.5 - 1)``. The smooth\n",
      "                  approximation of l1 (absolute value) loss. Usually a good\n",
      "                  choice for robust least squares.\n",
      "                * 'huber' : ``rho(z) = z if z <= 1 else 2*z**0.5 - 1``. Works\n",
      "                  similarly to 'soft_l1'.\n",
      "                * 'cauchy' : ``rho(z) = ln(1 + z)``. Severely weakens outliers\n",
      "                  influence, but may cause difficulties in optimization process.\n",
      "                * 'arctan' : ``rho(z) = arctan(z)``. Limits a maximum loss on\n",
      "                  a single residual, has properties similar to 'cauchy'.\n",
      "        \n",
      "            If callable, it must take a 1-d ndarray ``z=f**2`` and return an\n",
      "            array_like with shape (3, m) where row 0 contains function values,\n",
      "            row 1 contains first derivatives and row 2 contains second\n",
      "            derivatives. Method 'lm' supports only 'linear' loss.\n",
      "        f_scale : float, optional\n",
      "            Value of soft margin between inlier and outlier residuals, default\n",
      "            is 1.0. The loss function is evaluated as follows\n",
      "            ``rho_(f**2) = C**2 * rho(f**2 / C**2)``, where ``C`` is `f_scale`,\n",
      "            and ``rho`` is determined by `loss` parameter. This parameter has\n",
      "            no effect with ``loss='linear'``, but for other `loss` values it is\n",
      "            of crucial importance.\n",
      "        max_nfev : None or int, optional\n",
      "            Maximum number of function evaluations before the termination.\n",
      "            If None (default), the value is chosen automatically:\n",
      "        \n",
      "                * For 'trf' and 'dogbox' : 100 * n.\n",
      "                * For 'lm' :  100 * n if `jac` is callable and 100 * n * (n + 1)\n",
      "                  otherwise (because 'lm' counts function calls in Jacobian\n",
      "                  estimation).\n",
      "        \n",
      "        diff_step : None or array_like, optional\n",
      "            Determines the relative step size for the finite difference\n",
      "            approximation of the Jacobian. The actual step is computed as\n",
      "            ``x * diff_step``. If None (default), then `diff_step` is taken to be\n",
      "            a conventional \"optimal\" power of machine epsilon for the finite\n",
      "            difference scheme used [NR]_.\n",
      "        tr_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method for solving trust-region subproblems, relevant only for 'trf'\n",
      "            and 'dogbox' methods.\n",
      "        \n",
      "                * 'exact' is suitable for not very large problems with dense\n",
      "                  Jacobian matrices. The computational complexity per iteration is\n",
      "                  comparable to a singular value decomposition of the Jacobian\n",
      "                  matrix.\n",
      "                * 'lsmr' is suitable for problems with sparse and large Jacobian\n",
      "                  matrices. It uses the iterative procedure\n",
      "                  `scipy.sparse.linalg.lsmr` for finding a solution of a linear\n",
      "                  least-squares problem and only requires matrix-vector product\n",
      "                  evaluations.\n",
      "        \n",
      "            If None (default) the solver is chosen based on the type of Jacobian\n",
      "            returned on the first iteration.\n",
      "        tr_options : dict, optional\n",
      "            Keyword options passed to trust-region solver.\n",
      "        \n",
      "                * ``tr_solver='exact'``: `tr_options` are ignored.\n",
      "                * ``tr_solver='lsmr'``: options for `scipy.sparse.linalg.lsmr`.\n",
      "                  Additionally  ``method='trf'`` supports  'regularize' option\n",
      "                  (bool, default is True) which adds a regularization term to the\n",
      "                  normal equation, which improves convergence if the Jacobian is\n",
      "                  rank-deficient [Byrd]_ (eq. 3.4).\n",
      "        \n",
      "        jac_sparsity : {None, array_like, sparse matrix}, optional\n",
      "            Defines the sparsity structure of the Jacobian matrix for finite\n",
      "            difference estimation, its shape must be (m, n). If the Jacobian has\n",
      "            only few non-zero elements in *each* row, providing the sparsity\n",
      "            structure will greatly speed up the computations [Curtis]_. A zero\n",
      "            entry means that a corresponding element in the Jacobian is identically\n",
      "            zero. If provided, forces the use of 'lsmr' trust-region solver.\n",
      "            If None (default) then dense differencing will be used. Has no effect\n",
      "            for 'lm' method.\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 (default) : work silently.\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations (not supported by 'lm'\n",
      "                  method).\n",
      "        \n",
      "        args, kwargs : tuple and dict, optional\n",
      "            Additional arguments passed to `fun` and `jac`. Both empty by default.\n",
      "            The calling signature is ``fun(x, *args, **kwargs)`` and the same for\n",
      "            `jac`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `OptimizeResult` with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        jac : ndarray, sparse matrix or LinearOperator, shape (m, n)\n",
      "            Modified Jacobian matrix at the solution, in the sense that J^T J\n",
      "            is a Gauss-Newton approximation of the Hessian of the cost function.\n",
      "            The type is the same as the one used by the algorithm.\n",
      "        grad : ndarray, shape (m,)\n",
      "            Gradient of the cost function at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. In unconstrained problems, it is always\n",
      "            the uniform norm of the gradient. In constrained problems, it is the\n",
      "            quantity which was compared with `gtol` during iterations.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for 'trf' method as it generates a sequence\n",
      "            of strictly feasible iterates and `active_mask` is determined within a\n",
      "            tolerance threshold.\n",
      "        nfev : int\n",
      "            Number of function evaluations done. Methods 'trf' and 'dogbox' do not\n",
      "            count function calls for numerical Jacobian approximation, as opposed\n",
      "            to 'lm' method.\n",
      "        njev : int or None\n",
      "            Number of Jacobian evaluations done. If numerical Jacobian\n",
      "            approximation is used in 'lm' method, it is set to None.\n",
      "        status : int\n",
      "            The reason for algorithm termination:\n",
      "        \n",
      "                * -1 : improper input parameters status returned from MINPACK.\n",
      "                *  0 : the maximum number of function evaluations is exceeded.\n",
      "                *  1 : `gtol` termination condition is satisfied.\n",
      "                *  2 : `ftol` termination condition is satisfied.\n",
      "                *  3 : `xtol` termination condition is satisfied.\n",
      "                *  4 : Both `ftol` and `xtol` termination conditions are satisfied.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        leastsq : A legacy wrapper for the MINPACK implementation of the\n",
      "                  Levenberg-Marquadt algorithm.\n",
      "        curve_fit : Least-squares minimization applied to a curve fitting problem.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Method 'lm' (Levenberg-Marquardt) calls a wrapper over least-squares\n",
      "        algorithms implemented in MINPACK (lmder, lmdif). It runs the\n",
      "        Levenberg-Marquardt algorithm formulated as a trust-region type algorithm.\n",
      "        The implementation is based on paper [JJMore]_, it is very robust and\n",
      "        efficient with a lot of smart tricks. It should be your first choice\n",
      "        for unconstrained problems. Note that it doesn't support bounds. Also\n",
      "        it doesn't work when m < n.\n",
      "        \n",
      "        Method 'trf' (Trust Region Reflective) is motivated by the process of\n",
      "        solving a system of equations, which constitute the first-order optimality\n",
      "        condition for a bound-constrained minimization problem as formulated in\n",
      "        [STIR]_. The algorithm iteratively solves trust-region subproblems\n",
      "        augmented by a special diagonal quadratic term and with trust-region shape\n",
      "        determined by the distance from the bounds and the direction of the\n",
      "        gradient. This enhancements help to avoid making steps directly into bounds\n",
      "        and efficiently explore the whole space of variables. To further improve\n",
      "        convergence, the algorithm considers search directions reflected from the\n",
      "        bounds. To obey theoretical requirements, the algorithm keeps iterates\n",
      "        strictly feasible. With dense Jacobians trust-region subproblems are\n",
      "        solved by an exact method very similar to the one described in [JJMore]_\n",
      "        (and implemented in MINPACK). The difference from the MINPACK\n",
      "        implementation is that a singular value decomposition of a Jacobian\n",
      "        matrix is done once per iteration, instead of a QR decomposition and series\n",
      "        of Givens rotation eliminations. For large sparse Jacobians a 2-d subspace\n",
      "        approach of solving trust-region subproblems is used [STIR]_, [Byrd]_.\n",
      "        The subspace is spanned by a scaled gradient and an approximate\n",
      "        Gauss-Newton solution delivered by `scipy.sparse.linalg.lsmr`. When no\n",
      "        constraints are imposed the algorithm is very similar to MINPACK and has\n",
      "        generally comparable performance. The algorithm works quite robust in\n",
      "        unbounded and bounded problems, thus it is chosen as a default algorithm.\n",
      "        \n",
      "        Method 'dogbox' operates in a trust-region framework, but considers\n",
      "        rectangular trust regions as opposed to conventional ellipsoids [Voglis]_.\n",
      "        The intersection of a current trust region and initial bounds is again\n",
      "        rectangular, so on each iteration a quadratic minimization problem subject\n",
      "        to bound constraints is solved approximately by Powell's dogleg method\n",
      "        [NumOpt]_. The required Gauss-Newton step can be computed exactly for\n",
      "        dense Jacobians or approximately by `scipy.sparse.linalg.lsmr` for large\n",
      "        sparse Jacobians. The algorithm is likely to exhibit slow convergence when\n",
      "        the rank of Jacobian is less than the number of variables. The algorithm\n",
      "        often outperforms 'trf' in bounded problems with a small number of\n",
      "        variables.\n",
      "        \n",
      "        Robust loss functions are implemented as described in [BA]_. The idea\n",
      "        is to modify a residual vector and a Jacobian matrix on each iteration\n",
      "        such that computed gradient and Gauss-Newton Hessian approximation match\n",
      "        the true gradient and Hessian approximation of the cost function. Then\n",
      "        the algorithm proceeds in a normal way, i.e. robust loss functions are\n",
      "        implemented as a simple wrapper over standard least-squares algorithms.\n",
      "        \n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [NR] William H. Press et. al., \"Numerical Recipes. The Art of Scientific\n",
      "                Computing. 3rd edition\", Sec. 5.7.\n",
      "        .. [Byrd] R. H. Byrd, R. B. Schnabel and G. A. Shultz, \"Approximate\n",
      "                  solution of the trust region problem by minimization over\n",
      "                  two-dimensional subspaces\", Math. Programming, 40, pp. 247-263,\n",
      "                  1988.\n",
      "        .. [Curtis] A. Curtis, M. J. D. Powell, and J. Reid, \"On the estimation of\n",
      "                    sparse Jacobian matrices\", Journal of the Institute of\n",
      "                    Mathematics and its Applications, 13, pp. 117-120, 1974.\n",
      "        .. [JJMore] J. J. More, \"The Levenberg-Marquardt Algorithm: Implementation\n",
      "                    and Theory,\" Numerical Analysis, ed. G. A. Watson, Lecture\n",
      "                    Notes in Mathematics 630, Springer Verlag, pp. 105-116, 1977.\n",
      "        .. [Voglis] C. Voglis and I. E. Lagaris, \"A Rectangular Trust Region\n",
      "                    Dogleg Approach for Unconstrained and Bound Constrained\n",
      "                    Nonlinear Optimization\", WSEAS International Conference on\n",
      "                    Applied Mathematics, Corfu, Greece, 2004.\n",
      "        .. [NumOpt] J. Nocedal and S. J. Wright, \"Numerical optimization,\n",
      "                    2nd edition\", Chapter 4.\n",
      "        .. [BA] B. Triggs et. al., \"Bundle Adjustment - A Modern Synthesis\",\n",
      "                Proceedings of the International Workshop on Vision Algorithms:\n",
      "                Theory and Practice, pp. 298-372, 1999.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example we find a minimum of the Rosenbrock function without bounds\n",
      "        on independent variables.\n",
      "        \n",
      "        >>> def fun_rosenbrock(x):\n",
      "        ...     return np.array([10 * (x[1] - x[0]**2), (1 - x[0])])\n",
      "        \n",
      "        Notice that we only provide the vector of the residuals. The algorithm\n",
      "        constructs the cost function as a sum of squares of the residuals, which\n",
      "        gives the Rosenbrock function. The exact minimum is at ``x = [1.0, 1.0]``.\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> x0_rosenbrock = np.array([2, 2])\n",
      "        >>> res_1 = least_squares(fun_rosenbrock, x0_rosenbrock)\n",
      "        >>> res_1.x\n",
      "        array([ 1.,  1.])\n",
      "        >>> res_1.cost\n",
      "        9.8669242910846867e-30\n",
      "        >>> res_1.optimality\n",
      "        8.8928864934219529e-14\n",
      "        \n",
      "        We now constrain the variables, in such a way that the previous solution\n",
      "        becomes infeasible. Specifically, we require that ``x[1] >= 1.5``, and\n",
      "        ``x[0]`` left unconstrained. To this end, we specify the `bounds` parameter\n",
      "        to `least_squares` in the form ``bounds=([-np.inf, 1.5], np.inf)``.\n",
      "        \n",
      "        We also provide the analytic Jacobian:\n",
      "        \n",
      "        >>> def jac_rosenbrock(x):\n",
      "        ...     return np.array([\n",
      "        ...         [-20 * x[0], 10],\n",
      "        ...         [-1, 0]])\n",
      "        \n",
      "        Putting this all together, we see that the new solution lies on the bound:\n",
      "        \n",
      "        >>> res_2 = least_squares(fun_rosenbrock, x0_rosenbrock, jac_rosenbrock,\n",
      "        ...                       bounds=([-np.inf, 1.5], np.inf))\n",
      "        >>> res_2.x\n",
      "        array([ 1.22437075,  1.5       ])\n",
      "        >>> res_2.cost\n",
      "        0.025213093946805685\n",
      "        >>> res_2.optimality\n",
      "        1.5885401433157753e-07\n",
      "        \n",
      "        Now we solve a system of equations (i.e., the cost function should be zero\n",
      "        at a minimum) for a Broyden tridiagonal vector-valued function of 100000\n",
      "        variables:\n",
      "        \n",
      "        >>> def fun_broyden(x):\n",
      "        ...     f = (3 - x) * x + 1\n",
      "        ...     f[1:] -= x[:-1]\n",
      "        ...     f[:-1] -= 2 * x[1:]\n",
      "        ...     return f\n",
      "        \n",
      "        The corresponding Jacobian matrix is sparse. We tell the algorithm to\n",
      "        estimate it by finite differences and provide the sparsity structure of\n",
      "        Jacobian to significantly speed up this process.\n",
      "        \n",
      "        >>> from scipy.sparse import lil_matrix\n",
      "        >>> def sparsity_broyden(n):\n",
      "        ...     sparsity = lil_matrix((n, n), dtype=int)\n",
      "        ...     i = np.arange(n)\n",
      "        ...     sparsity[i, i] = 1\n",
      "        ...     i = np.arange(1, n)\n",
      "        ...     sparsity[i, i - 1] = 1\n",
      "        ...     i = np.arange(n - 1)\n",
      "        ...     sparsity[i, i + 1] = 1\n",
      "        ...     return sparsity\n",
      "        ...\n",
      "        >>> n = 100000\n",
      "        >>> x0_broyden = -np.ones(n)\n",
      "        ...\n",
      "        >>> res_3 = least_squares(fun_broyden, x0_broyden,\n",
      "        ...                       jac_sparsity=sparsity_broyden(n))\n",
      "        >>> res_3.cost\n",
      "        4.5687069299604613e-23\n",
      "        >>> res_3.optimality\n",
      "        1.1650454296851518e-11\n",
      "        \n",
      "        Let's also solve a curve fitting problem using robust loss function to\n",
      "        take care of outliers in the data. Define the model function as\n",
      "        ``y = a + b * exp(c * t)``, where t is a predictor variable, y is an\n",
      "        observation and a, b, c are parameters to estimate.\n",
      "        \n",
      "        First, define the function which generates the data with noise and\n",
      "        outliers, define the model parameters, and generate data:\n",
      "        \n",
      "        >>> def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\n",
      "        ...     y = a + b * np.exp(t * c)\n",
      "        ...\n",
      "        ...     rnd = np.random.RandomState(random_state)\n",
      "        ...     error = noise * rnd.randn(t.size)\n",
      "        ...     outliers = rnd.randint(0, t.size, n_outliers)\n",
      "        ...     error[outliers] *= 10\n",
      "        ...\n",
      "        ...     return y + error\n",
      "        ...\n",
      "        >>> a = 0.5\n",
      "        >>> b = 2.0\n",
      "        >>> c = -1\n",
      "        >>> t_min = 0\n",
      "        >>> t_max = 10\n",
      "        >>> n_points = 15\n",
      "        ...\n",
      "        >>> t_train = np.linspace(t_min, t_max, n_points)\n",
      "        >>> y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)\n",
      "        \n",
      "        Define function for computing residuals and initial estimate of\n",
      "        parameters.\n",
      "        \n",
      "        >>> def fun(x, t, y):\n",
      "        ...     return x[0] + x[1] * np.exp(x[2] * t) - y\n",
      "        ...\n",
      "        >>> x0 = np.array([1.0, 1.0, 0.0])\n",
      "        \n",
      "        Compute a standard least-squares solution:\n",
      "        \n",
      "        >>> res_lsq = least_squares(fun, x0, args=(t_train, y_train))\n",
      "        \n",
      "        Now compute two solutions with two different robust loss functions. The\n",
      "        parameter `f_scale` is set to 0.1, meaning that inlier residuals should\n",
      "        not significantly exceed 0.1 (the noise level used).\n",
      "        \n",
      "        >>> res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
      "        ...                             args=(t_train, y_train))\n",
      "        >>> res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
      "        ...                         args=(t_train, y_train))\n",
      "        \n",
      "        And finally plot all the curves. We see that by selecting an appropriate\n",
      "        `loss`  we can get estimates close to optimal even in the presence of\n",
      "        strong outliers. But keep in mind that generally it is recommended to try\n",
      "        'soft_l1' or 'huber' losses first (if at all necessary) as the other two\n",
      "        options may cause difficulties in optimization process.\n",
      "        \n",
      "        >>> t_test = np.linspace(t_min, t_max, n_points * 10)\n",
      "        >>> y_true = gen_data(t_test, a, b, c)\n",
      "        >>> y_lsq = gen_data(t_test, *res_lsq.x)\n",
      "        >>> y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
      "        >>> y_log = gen_data(t_test, *res_log.x)\n",
      "        ...\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        >>> plt.plot(t_train, y_train, 'o')\n",
      "        >>> plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
      "        >>> plt.plot(t_test, y_lsq, label='linear loss')\n",
      "        >>> plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
      "        >>> plt.plot(t_test, y_log, label='cauchy loss')\n",
      "        >>> plt.xlabel(\"t\")\n",
      "        >>> plt.ylabel(\"y\")\n",
      "        >>> plt.legend()\n",
      "        >>> plt.show()\n",
      "        \n",
      "        In the next example, we show how complex-valued residual functions of\n",
      "        complex variables can be optimized with ``least_squares()``. Consider the\n",
      "        following function:\n",
      "        \n",
      "        >>> def f(z):\n",
      "        ...     return z - (0.5 + 0.5j)\n",
      "        \n",
      "        We wrap it into a function of real variables that returns real residuals\n",
      "        by simply handling the real and imaginary parts as independent variables:\n",
      "        \n",
      "        >>> def f_wrap(x):\n",
      "        ...     fx = f(x[0] + 1j*x[1])\n",
      "        ...     return np.array([fx.real, fx.imag])\n",
      "        \n",
      "        Thus, instead of the original m-dimensional complex function of n complex\n",
      "        variables we optimize a 2m-dimensional real function of 2n real variables:\n",
      "        \n",
      "        >>> from scipy.optimize import least_squares\n",
      "        >>> res_wrapped = least_squares(f_wrap, (0.1, 0.1), bounds=([0, 0], [1, 1]))\n",
      "        >>> z = res_wrapped.x[0] + res_wrapped.x[1]*1j\n",
      "        >>> z\n",
      "        (0.49999999999925893+0.49999999999925893j)\n",
      "    \n",
      "    leastsq(func, x0, args=(), Dfun=None, full_output=0, col_deriv=0, ftol=1.49012e-08, xtol=1.49012e-08, gtol=0.0, maxfev=0, epsfcn=None, factor=100, diag=None)\n",
      "        Minimize the sum of squares of a set of equations.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            x = arg min(sum(func(y)**2,axis=0))\n",
      "                     y\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            should take at least one (possibly length N vector) argument and\n",
      "            returns M floating point numbers. It must not return NaNs or\n",
      "            fitting might fail.\n",
      "        x0 : ndarray\n",
      "            The starting estimate for the minimization.\n",
      "        args : tuple, optional\n",
      "            Any extra arguments to func are placed in this tuple.\n",
      "        Dfun : callable, optional\n",
      "            A function or method to compute the Jacobian of func with derivatives\n",
      "            across the rows. If this is None, the Jacobian will be estimated.\n",
      "        full_output : bool, optional\n",
      "            non-zero to return all optional outputs.\n",
      "        col_deriv : bool, optional\n",
      "            non-zero to specify that the Jacobian function computes derivatives\n",
      "            down the columns (faster, because there is no transpose operation).\n",
      "        ftol : float, optional\n",
      "            Relative error desired in the sum of squares.\n",
      "        xtol : float, optional\n",
      "            Relative error desired in the approximate solution.\n",
      "        gtol : float, optional\n",
      "            Orthogonality desired between the function vector and the columns of\n",
      "            the Jacobian.\n",
      "        maxfev : int, optional\n",
      "            The maximum number of calls to the function. If `Dfun` is provided\n",
      "            then the default `maxfev` is 100*(N+1) where N is the number of elements\n",
      "            in x0, otherwise the default `maxfev` is 200*(N+1).\n",
      "        epsfcn : float, optional\n",
      "            A variable used in determining a suitable step length for the forward-\n",
      "            difference approximation of the Jacobian (for Dfun=None).\n",
      "            Normally the actual step length will be sqrt(epsfcn)*x\n",
      "            If epsfcn is less than the machine precision, it is assumed that the\n",
      "            relative errors are of the order of the machine precision.\n",
      "        factor : float, optional\n",
      "            A parameter determining the initial step bound\n",
      "            (``factor * || diag * x||``). Should be in interval ``(0.1, 100)``.\n",
      "        diag : sequence, optional\n",
      "            N positive entries that serve as a scale factors for the variables.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            The solution (or the result of the last iteration for an unsuccessful\n",
      "            call).\n",
      "        cov_x : ndarray\n",
      "            The inverse of the Hessian. `fjac` and `ipvt` are used to construct an\n",
      "            estimate of the Hessian. A value of None indicates a singular matrix,\n",
      "            which means the curvature in parameters `x` is numerically flat. To\n",
      "            obtain the covariance matrix of the parameters `x`, `cov_x` must be\n",
      "            multiplied by the variance of the residuals -- see curve_fit.\n",
      "        infodict : dict\n",
      "            a dictionary of optional outputs with the keys:\n",
      "        \n",
      "            ``nfev``\n",
      "                The number of function calls\n",
      "            ``fvec``\n",
      "                The function evaluated at the output\n",
      "            ``fjac``\n",
      "                A permutation of the R matrix of a QR\n",
      "                factorization of the final approximate\n",
      "                Jacobian matrix, stored column wise.\n",
      "                Together with ipvt, the covariance of the\n",
      "                estimate can be approximated.\n",
      "            ``ipvt``\n",
      "                An integer array of length N which defines\n",
      "                a permutation matrix, p, such that\n",
      "                fjac*p = q*r, where r is upper triangular\n",
      "                with diagonal elements of nonincreasing\n",
      "                magnitude. Column j of p is column ipvt(j)\n",
      "                of the identity matrix.\n",
      "            ``qtf``\n",
      "                The vector (transpose(q) * fvec).\n",
      "        \n",
      "        mesg : str\n",
      "            A string message giving information about the cause of failure.\n",
      "        ier : int\n",
      "            An integer flag.  If it is equal to 1, 2, 3 or 4, the solution was\n",
      "            found.  Otherwise, the solution was not found. In either case, the\n",
      "            optional output variable 'mesg' gives more information.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        least_squares : Newer interface to solve nonlinear least-squares problems\n",
      "            with bounds on the variables. See ``method=='lm'`` in particular.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \"leastsq\" is a wrapper around MINPACK's lmdif and lmder algorithms.\n",
      "        \n",
      "        cov_x is a Jacobian approximation to the Hessian of the least squares\n",
      "        objective function.\n",
      "        This approximation assumes that the objective function is based on the\n",
      "        difference between some observed target data (ydata) and a (non-linear)\n",
      "        function of the parameters `f(xdata, params)` ::\n",
      "        \n",
      "               func(params) = ydata - f(xdata, params)\n",
      "        \n",
      "        so that the objective function is ::\n",
      "        \n",
      "               min   sum((ydata - f(xdata, params))**2, axis=0)\n",
      "             params\n",
      "        \n",
      "        The solution, `x`, is always a 1D array, regardless of the shape of `x0`,\n",
      "        or whether `x0` is a scalar.\n",
      "    \n",
      "    line_search = line_search_wolfe2(f, myfprime, xk, pk, gfk=None, old_fval=None, old_old_fval=None, args=(), c1=0.0001, c2=0.9, amax=None, extra_condition=None, maxiter=10)\n",
      "        Find alpha that satisfies strong Wolfe conditions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable f(x,*args)\n",
      "            Objective function.\n",
      "        myfprime : callable f'(x,*args)\n",
      "            Objective function gradient.\n",
      "        xk : ndarray\n",
      "            Starting point.\n",
      "        pk : ndarray\n",
      "            Search direction.\n",
      "        gfk : ndarray, optional\n",
      "            Gradient value for x=xk (xk being the current parameter\n",
      "            estimate). Will be recomputed if omitted.\n",
      "        old_fval : float, optional\n",
      "            Function value for x=xk. Will be recomputed if omitted.\n",
      "        old_old_fval : float, optional\n",
      "            Function value for the point preceding x=xk\n",
      "        args : tuple, optional\n",
      "            Additional arguments passed to objective function.\n",
      "        c1 : float, optional\n",
      "            Parameter for Armijo condition rule.\n",
      "        c2 : float, optional\n",
      "            Parameter for curvature condition rule.\n",
      "        amax : float, optional\n",
      "            Maximum step size\n",
      "        extra_condition : callable, optional\n",
      "            A callable of the form ``extra_condition(alpha, x, f, g)``\n",
      "            returning a boolean. Arguments are the proposed step ``alpha``\n",
      "            and the corresponding ``x``, ``f`` and ``g`` values. The line search \n",
      "            accepts the value of ``alpha`` only if this \n",
      "            callable returns ``True``. If the callable returns ``False`` \n",
      "            for the step length, the algorithm will continue with \n",
      "            new iterates. The callable is only called for iterates \n",
      "            satisfying the strong Wolfe conditions.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to perform\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        alpha : float or None\n",
      "            Alpha for which ``x_new = x0 + alpha * pk``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        fc : int\n",
      "            Number of function evaluations made.\n",
      "        gc : int\n",
      "            Number of gradient evaluations made.\n",
      "        new_fval : float or None\n",
      "            New function value ``f(x_new)=f(x0+alpha*pk)``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        old_fval : float\n",
      "            Old function value ``f(x0)``.\n",
      "        new_slope : float or None\n",
      "            The local slope along the search direction at the\n",
      "            new value ``<myfprime(x_new), pk>``,\n",
      "            or None if the line search algorithm did not converge.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses the line search algorithm to enforce strong Wolfe\n",
      "        conditions.  See Wright and Nocedal, 'Numerical Optimization',\n",
      "        1999, pg. 59-60.\n",
      "        \n",
      "        For the zoom phase it uses an algorithm by [...].\n",
      "    \n",
      "    linear_sum_assignment(cost_matrix, maximize=False)\n",
      "        Solve the linear sum assignment problem.\n",
      "        \n",
      "        The linear sum assignment problem is also known as minimum weight matching\n",
      "        in bipartite graphs. A problem instance is described by a matrix C, where\n",
      "        each C[i,j] is the cost of matching vertex i of the first partite set\n",
      "        (a \"worker\") and vertex j of the second set (a \"job\"). The goal is to find\n",
      "        a complete assignment of workers to jobs of minimal cost.\n",
      "        \n",
      "        Formally, let X be a boolean matrix where :math:`X[i,j] = 1` iff row i is\n",
      "        assigned to column j. Then the optimal assignment has cost\n",
      "        \n",
      "        .. math::\n",
      "            \\min \\sum_i \\sum_j C_{i,j} X_{i,j}\n",
      "        \n",
      "        where, in the case where the matrix X is square, each row is assigned to\n",
      "        exactly one column, and each column to exactly one row.\n",
      "        \n",
      "        This function can also solve a generalization of the classic assignment\n",
      "        problem where the cost matrix is rectangular. If it has more rows than\n",
      "        columns, then not every row needs to be assigned to a column, and vice\n",
      "        versa.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cost_matrix : array\n",
      "            The cost matrix of the bipartite graph.\n",
      "        \n",
      "        maximize : bool (default: False)\n",
      "            Calculates a maximum weight matching if true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        row_ind, col_ind : array\n",
      "            An array of row indices and one of corresponding column indices giving\n",
      "            the optimal assignment. The cost of the assignment can be computed\n",
      "            as ``cost_matrix[row_ind, col_ind].sum()``. The row indices will be\n",
      "            sorted; in the case of a square cost matrix they will be equal to\n",
      "            ``numpy.arange(cost_matrix.shape[0])``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionadded:: 0.17.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        1. https://en.wikipedia.org/wiki/Assignment_problem\n",
      "        \n",
      "        2. DF Crouse. On implementing 2D rectangular assignment algorithms.\n",
      "           *IEEE Transactions on Aerospace and Electronic Systems*,\n",
      "           52(4):1679-1696, August 2016, https://doi.org/10.1109/TAES.2016.140952\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cost = np.array([[4, 1, 3], [2, 0, 5], [3, 2, 2]])\n",
      "        >>> from scipy.optimize import linear_sum_assignment\n",
      "        >>> row_ind, col_ind = linear_sum_assignment(cost)\n",
      "        >>> col_ind\n",
      "        array([1, 0, 2])\n",
      "        >>> cost[row_ind, col_ind].sum()\n",
      "        5\n",
      "    \n",
      "    linearmixing(F, xin, iter=None, alpha=None, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using a scalar Jacobian approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           This algorithm may be useful for specific problems, but whether\n",
      "           it will work may depend strongly on the problem.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        alpha : float, optional\n",
      "            The Jacobian approximation is (-1/alpha).\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='linearmixing'`` in particular.\n",
      "    \n",
      "    linprog(c, A_ub=None, b_ub=None, A_eq=None, b_eq=None, bounds=None, method='interior-point', callback=None, options=None, x0=None)\n",
      "        Linear programming: minimize a linear objective function subject to linear\n",
      "        equality and inequality constraints.\n",
      "        \n",
      "        Linear programming solves problems of the following form:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_x \\ & c^T x \\\\\n",
      "            \\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n",
      "            & A_{eq} x = b_{eq},\\\\\n",
      "            & l \\leq x \\leq u ,\n",
      "        \n",
      "        where :math:`x` is a vector of decision variables; :math:`c`,\n",
      "        :math:`b_{ub}`, :math:`b_{eq}`, :math:`l`, and :math:`u` are vectors; and\n",
      "        :math:`A_{ub}` and :math:`A_{eq}` are matrices.\n",
      "        \n",
      "        Informally, that's:\n",
      "        \n",
      "        minimize::\n",
      "        \n",
      "            c @ x\n",
      "        \n",
      "        such that::\n",
      "        \n",
      "            A_ub @ x <= b_ub\n",
      "            A_eq @ x == b_eq\n",
      "            lb <= x <= ub\n",
      "        \n",
      "        Note that by default ``lb = 0`` and ``ub = None`` unless specified with\n",
      "        ``bounds``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        c : 1D array\n",
      "            The coefficients of the linear objective function to be minimized.\n",
      "        A_ub : 2D array, optional\n",
      "            The inequality constraint matrix. Each row of ``A_ub`` specifies the\n",
      "            coefficients of a linear inequality constraint on ``x``.\n",
      "        b_ub : 1D array, optional\n",
      "            The inequality constraint vector. Each element represents an\n",
      "            upper bound on the corresponding value of ``A_ub @ x``.\n",
      "        A_eq : 2D array, optional\n",
      "            The equality constraint matrix. Each row of ``A_eq`` specifies the\n",
      "            coefficients of a linear equality constraint on ``x``.\n",
      "        b_eq : 1D array, optional\n",
      "            The equality constraint vector. Each element of ``A_eq @ x`` must equal\n",
      "            the corresponding element of ``b_eq``.\n",
      "        bounds : sequence, optional\n",
      "            A sequence of ``(min, max)`` pairs for each element in ``x``, defining\n",
      "            the minimum and maximum values of that decision variable. Use ``None`` to\n",
      "            indicate that there is no bound. By default, bounds are ``(0, None)``\n",
      "            (all decision variables are non-negative).\n",
      "            If a single tuple ``(min, max)`` is provided, then ``min`` and\n",
      "            ``max`` will serve as bounds for all decision variables.\n",
      "        method : {'interior-point', 'revised simplex', 'simplex'}, optional\n",
      "            The algorithm used to solve the standard form problem.\n",
      "            :ref:`'interior-point' <optimize.linprog-interior-point>` (default),\n",
      "            :ref:`'revised simplex' <optimize.linprog-revised_simplex>`, and\n",
      "            :ref:`'simplex' <optimize.linprog-simplex>` (legacy)\n",
      "            are supported.\n",
      "        callback : callable, optional\n",
      "            If a callback function is provided, it will be called at least once per\n",
      "            iteration of the algorithm. The callback function must accept a single\n",
      "            `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "                x : 1D array\n",
      "                    The current solution vector.\n",
      "                fun : float\n",
      "                    The current value of the objective function ``c @ x``.\n",
      "                success : bool\n",
      "                    ``True`` when the algorithm has completed successfully.\n",
      "                slack : 1D array\n",
      "                    The (nominally positive) values of the slack,\n",
      "                    ``b_ub - A_ub @ x``.\n",
      "                con : 1D array\n",
      "                    The (nominally zero) residuals of the equality constraints,\n",
      "                    ``b_eq - A_eq @ x``.\n",
      "                phase : int\n",
      "                    The phase of the algorithm being executed.\n",
      "                status : int\n",
      "                    An integer representing the status of the algorithm.\n",
      "        \n",
      "                    ``0`` : Optimization proceeding nominally.\n",
      "        \n",
      "                    ``1`` : Iteration limit reached.\n",
      "        \n",
      "                    ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                    ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                    ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "                nit : int\n",
      "                    The current iteration number.\n",
      "                message : str\n",
      "                    A string descriptor of the algorithm status.\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                    Default: see method-specific documentation.\n",
      "                disp : bool\n",
      "                    Set to ``True`` to print convergence messages.\n",
      "                    Default: ``False``.\n",
      "                autoscale : bool\n",
      "                    Set to ``True`` to automatically perform equilibration.\n",
      "                    Consider using this option if the numerical values in the\n",
      "                    constraints are separated by several orders of magnitude.\n",
      "                    Default: ``False``.\n",
      "                presolve : bool\n",
      "                    Set to ``False`` to disable automatic presolve.\n",
      "                    Default: ``True``.\n",
      "                rr : bool\n",
      "                    Set to ``False`` to disable automatic redundancy removal.\n",
      "                    Default: ``True``.\n",
      "        \n",
      "            For method-specific options, see\n",
      "            :func:`show_options('linprog') <show_options>`.\n",
      "        \n",
      "        x0 : 1D array, optional\n",
      "            Guess values of the decision variables, which will be refined by\n",
      "            the optimization algorithm. This argument is currently used only by the\n",
      "            'revised simplex' method, and can only be used if `x0` represents a\n",
      "            basic feasible solution.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            A :class:`scipy.optimize.OptimizeResult` consisting of the fields:\n",
      "        \n",
      "                x : 1D array\n",
      "                    The values of the decision variables that minimizes the\n",
      "                    objective function while satisfying the constraints.\n",
      "                fun : float\n",
      "                    The optimal value of the objective function ``c @ x``.\n",
      "                slack : 1D array\n",
      "                    The (nominally positive) values of the slack variables,\n",
      "                    ``b_ub - A_ub @ x``.\n",
      "                con : 1D array\n",
      "                    The (nominally zero) residuals of the equality constraints,\n",
      "                    ``b_eq - A_eq @ x``.\n",
      "                success : bool\n",
      "                    ``True`` when the algorithm succeeds in finding an optimal\n",
      "                    solution.\n",
      "                status : int\n",
      "                    An integer representing the exit status of the algorithm.\n",
      "        \n",
      "                    ``0`` : Optimization terminated successfully.\n",
      "        \n",
      "                    ``1`` : Iteration limit reached.\n",
      "        \n",
      "                    ``2`` : Problem appears to be infeasible.\n",
      "        \n",
      "                    ``3`` : Problem appears to be unbounded.\n",
      "        \n",
      "                    ``4`` : Numerical difficulties encountered.\n",
      "        \n",
      "                nit : int\n",
      "                    The total number of iterations performed in all phases.\n",
      "                message : str\n",
      "                    A string descriptor of the exit status of the algorithm.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        :ref:`'interior-point' <optimize.linprog-interior-point>` is the default\n",
      "        as it is typically the fastest and most robust method.\n",
      "        :ref:`'revised simplex' <optimize.linprog-revised_simplex>` is more\n",
      "        accurate for the problems it solves.\n",
      "        :ref:`'simplex' <optimize.linprog-simplex>` is the legacy method and is\n",
      "        included for backwards compatibility and educational purposes.\n",
      "        \n",
      "        Method *interior-point* uses the primal-dual path following algorithm\n",
      "        as outlined in [4]_. This algorithm supports sparse constraint matrices and\n",
      "        is typically faster than the simplex methods, especially for large, sparse\n",
      "        problems. Note, however, that the solution returned may be slightly less\n",
      "        accurate than those of the simplex methods and will not, in general,\n",
      "        correspond with a vertex of the polytope defined by the constraints.\n",
      "        \n",
      "        .. versionadded:: 1.0.0\n",
      "        \n",
      "        Method *revised simplex* uses the revised simplex method as described in\n",
      "        [9]_, except that a factorization [11]_ of the basis matrix, rather than\n",
      "        its inverse, is efficiently maintained and used to solve the linear systems\n",
      "        at each iteration of the algorithm.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        Method *simplex* uses a traditional, full-tableau implementation of\n",
      "        Dantzig's simplex algorithm [1]_, [2]_ (*not* the\n",
      "        Nelder-Mead simplex). This algorithm is included for backwards\n",
      "        compatibility and educational purposes.\n",
      "        \n",
      "        .. versionadded:: 0.15.0\n",
      "        \n",
      "        Before applying any method, a presolve procedure based on [8]_ attempts\n",
      "        to identify trivial infeasibilities, trivial unboundedness, and potential\n",
      "        problem simplifications. Specifically, it checks for:\n",
      "        \n",
      "        - rows of zeros in ``A_eq`` or ``A_ub``, representing trivial constraints;\n",
      "        - columns of zeros in ``A_eq`` `and` ``A_ub``, representing unconstrained\n",
      "          variables;\n",
      "        - column singletons in ``A_eq``, representing fixed variables; and\n",
      "        - column singletons in ``A_ub``, representing simple bounds.\n",
      "        \n",
      "        If presolve reveals that the problem is unbounded (e.g. an unconstrained\n",
      "        and unbounded variable has negative cost) or infeasible (e.g. a row of\n",
      "        zeros in ``A_eq`` corresponds with a nonzero in ``b_eq``), the solver\n",
      "        terminates with the appropriate status code. Note that presolve terminates\n",
      "        as soon as any sign of unboundedness is detected; consequently, a problem\n",
      "        may be reported as unbounded when in reality the problem is infeasible\n",
      "        (but infeasibility has not been detected yet). Therefore, if it is\n",
      "        important to know whether the problem is actually infeasible, solve the\n",
      "        problem again with option ``presolve=False``.\n",
      "        \n",
      "        If neither infeasibility nor unboundedness are detected in a single pass\n",
      "        of the presolve, bounds are tightened where possible and fixed\n",
      "        variables are removed from the problem. Then, linearly dependent rows\n",
      "        of the ``A_eq`` matrix are removed, (unless they represent an\n",
      "        infeasibility) to avoid numerical difficulties in the primary solve\n",
      "        routine. Note that rows that are nearly linearly dependent (within a\n",
      "        prescribed tolerance) may also be removed, which can change the optimal\n",
      "        solution in rare cases. If this is a concern, eliminate redundancy from\n",
      "        your problem formulation and run with option ``rr=False`` or\n",
      "        ``presolve=False``.\n",
      "        \n",
      "        Several potential improvements can be made here: additional presolve\n",
      "        checks outlined in [8]_ should be implemented, the presolve routine should\n",
      "        be run multiple times (until no further simplifications can be made), and\n",
      "        more of the efficiency improvements from [5]_ should be implemented in the\n",
      "        redundancy removal routines.\n",
      "        \n",
      "        After presolve, the problem is transformed to standard form by converting\n",
      "        the (tightened) simple bounds to upper bound constraints, introducing\n",
      "        non-negative slack variables for inequality constraints, and expressing\n",
      "        unbounded variables as the difference between two non-negative variables.\n",
      "        Optionally, the problem is automatically scaled via equilibration [12]_.\n",
      "        The selected algorithm solves the standard form problem, and a\n",
      "        postprocessing routine converts the result to a solution to the original\n",
      "        problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Dantzig, George B., Linear programming and extensions. Rand\n",
      "               Corporation Research Study Princeton Univ. Press, Princeton, NJ,\n",
      "               1963\n",
      "        .. [2] Hillier, S.H. and Lieberman, G.J. (1995), \"Introduction to\n",
      "               Mathematical Programming\", McGraw-Hill, Chapter 4.\n",
      "        .. [3] Bland, Robert G. New finite pivoting rules for the simplex method.\n",
      "               Mathematics of Operations Research (2), 1977: pp. 103-107.\n",
      "        .. [4] Andersen, Erling D., and Knud D. Andersen. \"The MOSEK interior point\n",
      "               optimizer for linear programming: an implementation of the\n",
      "               homogeneous algorithm.\" High performance optimization. Springer US,\n",
      "               2000. 197-232.\n",
      "        .. [5] Andersen, Erling D. \"Finding all linearly dependent rows in\n",
      "               large-scale linear programming.\" Optimization Methods and Software\n",
      "               6.3 (1995): 219-227.\n",
      "        .. [6] Freund, Robert M. \"Primal-Dual Interior-Point Methods for Linear\n",
      "               Programming based on Newton's Method.\" Unpublished Course Notes,\n",
      "               March 2004. Available 2/25/2017 at\n",
      "               https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/lecture-notes/lec14_int_pt_mthd.pdf\n",
      "        .. [7] Fourer, Robert. \"Solving Linear Programs by Interior-Point Methods.\"\n",
      "               Unpublished Course Notes, August 26, 2005. Available 2/25/2017 at\n",
      "               http://www.4er.org/CourseNotes/Book%20B/B-III.pdf\n",
      "        .. [8] Andersen, Erling D., and Knud D. Andersen. \"Presolving in linear\n",
      "               programming.\" Mathematical Programming 71.2 (1995): 221-245.\n",
      "        .. [9] Bertsimas, Dimitris, and J. Tsitsiklis. \"Introduction to linear\n",
      "               programming.\" Athena Scientific 1 (1997): 997.\n",
      "        .. [10] Andersen, Erling D., et al. Implementation of interior point\n",
      "                methods for large scale linear programming. HEC/Universite de\n",
      "                Geneve, 1996.\n",
      "        .. [11] Bartels, Richard H. \"A stabilization of the simplex method.\"\n",
      "                Journal in  Numerische Mathematik 16.5 (1971): 414-434.\n",
      "        .. [12] Tomlin, J. A. \"On scaling linear programming problems.\"\n",
      "                Mathematical Programming Study 4 (1975): 146-166.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the following problem:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            \\min_{x_0, x_1} \\ -x_0 + 4x_1 & \\\\\n",
      "            \\mbox{such that} \\ -3x_0 + x_1 & \\leq 6,\\\\\n",
      "            -x_0 - 2x_1 & \\geq -4,\\\\\n",
      "            x_1 & \\geq -3.\n",
      "        \n",
      "        The problem is not presented in the form accepted by `linprog`. This is\n",
      "        easily remedied by converting the \"greater than\" inequality\n",
      "        constraint to a \"less than\" inequality constraint by\n",
      "        multiplying both sides by a factor of :math:`-1`. Note also that the last\n",
      "        constraint is really the simple bound :math:`-3 \\leq x_1 \\leq \\infty`.\n",
      "        Finally, since there are no bounds on :math:`x_0`, we must explicitly\n",
      "        specify the bounds :math:`-\\infty \\leq x_0 \\leq \\infty`, as the\n",
      "        default is for variables to be non-negative. After collecting coeffecients\n",
      "        into arrays and tuples, the input for this problem is:\n",
      "        \n",
      "        >>> c = [-1, 4]\n",
      "        >>> A = [[-3, 1], [1, 2]]\n",
      "        >>> b = [6, 4]\n",
      "        >>> x0_bounds = (None, None)\n",
      "        >>> x1_bounds = (-3, None)\n",
      "        >>> from scipy.optimize import linprog\n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds])\n",
      "        \n",
      "        Note that the default method for `linprog` is 'interior-point', which is\n",
      "        approximate by nature.\n",
      "        \n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -21.99999984082494 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 6 # may vary\n",
      "           slack: array([3.89999997e+01, 8.46872439e-08] # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([ 9.99999989, -2.99999999]) # may vary\n",
      "        \n",
      "        If you need greater accuracy, try 'revised simplex'.\n",
      "        \n",
      "        >>> res = linprog(c, A_ub=A, b_ub=b, bounds=[x0_bounds, x1_bounds], method='revised simplex')\n",
      "        >>> print(res)\n",
      "             con: array([], dtype=float64)\n",
      "             fun: -22.0 # may vary\n",
      "         message: 'Optimization terminated successfully.'\n",
      "             nit: 1 # may vary\n",
      "           slack: array([39.,  0.]) # may vary\n",
      "          status: 0\n",
      "         success: True\n",
      "               x: array([10., -3.]) # may vary\n",
      "    \n",
      "    linprog_verbose_callback(res)\n",
      "        A sample callback function demonstrating the linprog callback interface.\n",
      "        This callback produces detailed output to sys.stdout before each iteration\n",
      "        and after the final iteration of the simplex algorithm.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        res : A `scipy.optimize.OptimizeResult` consisting of the following fields:\n",
      "        \n",
      "            x : 1D array\n",
      "                The independent variable vector which optimizes the linear\n",
      "                programming problem.\n",
      "            fun : float\n",
      "                Value of the objective function.\n",
      "            success : bool\n",
      "                True if the algorithm succeeded in finding an optimal solution.\n",
      "            slack : 1D array\n",
      "                The values of the slack variables. Each slack variable corresponds\n",
      "                to an inequality constraint. If the slack is zero, then the\n",
      "                corresponding constraint is active.\n",
      "            con : 1D array\n",
      "                The (nominally zero) residuals of the equality constraints, that is,\n",
      "                ``b - A_eq @ x``\n",
      "            phase : int\n",
      "                The phase of the optimization being executed. In phase 1 a basic\n",
      "                feasible solution is sought and the T has an additional row\n",
      "                representing an alternate objective function.\n",
      "            status : int\n",
      "                An integer representing the exit status of the optimization::\n",
      "        \n",
      "                     0 : Optimization terminated successfully\n",
      "                     1 : Iteration limit reached\n",
      "                     2 : Problem appears to be infeasible\n",
      "                     3 : Problem appears to be unbounded\n",
      "                     4 : Serious numerical difficulties encountered\n",
      "        \n",
      "            nit : int\n",
      "                The number of iterations performed.\n",
      "            message : str\n",
      "                A string descriptor of the exit status of the optimization.\n",
      "    \n",
      "    lsq_linear(A, b, bounds=(-inf, inf), method='trf', tol=1e-10, lsq_solver=None, lsmr_tol=None, max_iter=None, verbose=0)\n",
      "        Solve a linear least-squares problem with bounds on the variables.\n",
      "        \n",
      "        Given a m-by-n design matrix A and a target vector b with m elements,\n",
      "        `lsq_linear` solves the following optimization problem::\n",
      "        \n",
      "            minimize 0.5 * ||A x - b||**2\n",
      "            subject to lb <= x <= ub\n",
      "        \n",
      "        This optimization problem is convex, hence a found minimum (if iterations\n",
      "        have converged) is guaranteed to be global.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array_like, sparse matrix of LinearOperator, shape (m, n)\n",
      "            Design matrix. Can be `scipy.sparse.linalg.LinearOperator`.\n",
      "        b : array_like, shape (m,)\n",
      "            Target vector.\n",
      "        bounds : 2-tuple of array_like, optional\n",
      "            Lower and upper bounds on independent variables. Defaults to no bounds.\n",
      "            Each array must have shape (n,) or be a scalar, in the latter\n",
      "            case a bound will be the same for all variables. Use ``np.inf`` with\n",
      "            an appropriate sign to disable bounds on all or some variables.\n",
      "        method : 'trf' or 'bvls', optional\n",
      "            Method to perform minimization.\n",
      "        \n",
      "                * 'trf' : Trust Region Reflective algorithm adapted for a linear\n",
      "                  least-squares problem. This is an interior-point-like method\n",
      "                  and the required number of iterations is weakly correlated with\n",
      "                  the number of variables.\n",
      "                * 'bvls' : Bounded-Variable Least-Squares algorithm. This is\n",
      "                  an active set method, which requires the number of iterations\n",
      "                  comparable to the number of variables. Can't be used when `A` is\n",
      "                  sparse or LinearOperator.\n",
      "        \n",
      "            Default is 'trf'.\n",
      "        tol : float, optional\n",
      "            Tolerance parameter. The algorithm terminates if a relative change\n",
      "            of the cost function is less than `tol` on the last iteration.\n",
      "            Additionally the first-order optimality measure is considered:\n",
      "        \n",
      "                * ``method='trf'`` terminates if the uniform norm of the gradient,\n",
      "                  scaled to account for the presence of the bounds, is less than\n",
      "                  `tol`.\n",
      "                * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n",
      "                  are satisfied within `tol` tolerance.\n",
      "        \n",
      "        lsq_solver : {None, 'exact', 'lsmr'}, optional\n",
      "            Method of solving unbounded least-squares problems throughout\n",
      "            iterations:\n",
      "        \n",
      "                * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n",
      "                  used when `A` is sparse or LinearOperator.\n",
      "                * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n",
      "                  which requires only matrix-vector product evaluations. Can't\n",
      "                  be used with ``method='bvls'``.\n",
      "        \n",
      "            If None (default) the solver is chosen based on type of `A`.\n",
      "        lsmr_tol : None, float or 'auto', optional\n",
      "            Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n",
      "            If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n",
      "            tolerance will be adjusted based on the optimality of the current\n",
      "            iterate, which can speed up the optimization process, but is not always\n",
      "            reliable.\n",
      "        max_iter : None or int, optional\n",
      "            Maximum number of iterations before termination. If None (default), it\n",
      "            is set to 100 for ``method='trf'`` or to the number of variables for\n",
      "            ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n",
      "        verbose : {0, 1, 2}, optional\n",
      "            Level of algorithm's verbosity:\n",
      "        \n",
      "                * 0 : work silently (default).\n",
      "                * 1 : display a termination report.\n",
      "                * 2 : display progress during iterations.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        OptimizeResult with the following fields defined:\n",
      "        x : ndarray, shape (n,)\n",
      "            Solution found.\n",
      "        cost : float\n",
      "            Value of the cost function at the solution.\n",
      "        fun : ndarray, shape (m,)\n",
      "            Vector of residuals at the solution.\n",
      "        optimality : float\n",
      "            First-order optimality measure. The exact meaning depends on `method`,\n",
      "            refer to the description of `tol` parameter.\n",
      "        active_mask : ndarray of int, shape (n,)\n",
      "            Each component shows whether a corresponding constraint is active\n",
      "            (that is, whether a variable is at the bound):\n",
      "        \n",
      "                *  0 : a constraint is not active.\n",
      "                * -1 : a lower bound is active.\n",
      "                *  1 : an upper bound is active.\n",
      "        \n",
      "            Might be somewhat arbitrary for the `trf` method as it generates a\n",
      "            sequence of strictly feasible iterates and active_mask is determined\n",
      "            within a tolerance threshold.\n",
      "        nit : int\n",
      "            Number of iterations. Zero if the unconstrained solution is optimal.\n",
      "        status : int\n",
      "            Reason for algorithm termination:\n",
      "        \n",
      "                * -1 : the algorithm was not able to make progress on the last\n",
      "                  iteration.\n",
      "                *  0 : the maximum number of iterations is exceeded.\n",
      "                *  1 : the first-order optimality measure is less than `tol`.\n",
      "                *  2 : the relative change of the cost function is less than `tol`.\n",
      "                *  3 : the unconstrained solution is optimal.\n",
      "        \n",
      "        message : str\n",
      "            Verbal description of the termination reason.\n",
      "        success : bool\n",
      "            True if one of the convergence criteria is satisfied (`status` > 0).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        nnls : Linear least squares with non-negativity constraint.\n",
      "        least_squares : Nonlinear least squares with bounds on the variables.                    \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm first computes the unconstrained least-squares solution by\n",
      "        `numpy.linalg.lstsq` or `scipy.sparse.linalg.lsmr` depending on\n",
      "        `lsq_solver`. This solution is returned as optimal if it lies within the\n",
      "        bounds.\n",
      "        \n",
      "        Method 'trf' runs the adaptation of the algorithm described in [STIR]_ for\n",
      "        a linear least-squares problem. The iterations are essentially the same as\n",
      "        in the nonlinear least-squares algorithm, but as the quadratic function\n",
      "        model is always accurate, we don't need to track or modify the radius of\n",
      "        a trust region. The line search (backtracking) is used as a safety net\n",
      "        when a selected step does not decrease the cost function. Read more\n",
      "        detailed description of the algorithm in `scipy.optimize.least_squares`.\n",
      "        \n",
      "        Method 'bvls' runs a Python implementation of the algorithm described in\n",
      "        [BVLS]_. The algorithm maintains active and free sets of variables, on\n",
      "        each iteration chooses a new variable to move from the active set to the\n",
      "        free set and then solves the unconstrained least-squares problem on free\n",
      "        variables. This algorithm is guaranteed to give an accurate solution\n",
      "        eventually, but may require up to n iterations for a problem with n\n",
      "        variables. Additionally, an ad-hoc initialization procedure is\n",
      "        implemented, that determines which variables to set free or active\n",
      "        initially. It takes some number of iterations before actual BVLS starts,\n",
      "        but can significantly reduce the number of further iterations.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [STIR] M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n",
      "                  and Conjugate Gradient Method for Large-Scale Bound-Constrained\n",
      "                  Minimization Problems,\" SIAM Journal on Scientific Computing,\n",
      "                  Vol. 21, Number 1, pp 1-23, 1999.\n",
      "        .. [BVLS] P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n",
      "                  an Algorithm and Applications\", Computational Statistics, 10,\n",
      "                  129-141, 1995.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        In this example a problem with a large sparse matrix and bounds on the\n",
      "        variables is solved.\n",
      "        \n",
      "        >>> from scipy.sparse import rand\n",
      "        >>> from scipy.optimize import lsq_linear\n",
      "        ...\n",
      "        >>> np.random.seed(0)\n",
      "        ...\n",
      "        >>> m = 20000\n",
      "        >>> n = 10000\n",
      "        ...\n",
      "        >>> A = rand(m, n, density=1e-4)\n",
      "        >>> b = np.random.randn(m)\n",
      "        ...\n",
      "        >>> lb = np.random.randn(n)\n",
      "        >>> ub = lb + 1\n",
      "        ...\n",
      "        >>> res = lsq_linear(A, b, bounds=(lb, ub), lsmr_tol='auto', verbose=1)\n",
      "        # may vary\n",
      "        The relative change of the cost function is less than `tol`.\n",
      "        Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n",
      "        first-order optimality 4.66e-08.\n",
      "    \n",
      "    minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "        Minimization of scalar function of one or more variables.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            The objective function to be minimized.\n",
      "        \n",
      "                ``fun(x, *args) -> float``\n",
      "        \n",
      "            where x is an 1-D array with shape (n,) and `args`\n",
      "            is a tuple of the fixed parameters needed to completely\n",
      "            specify the function.\n",
      "        x0 : ndarray, shape (n,)\n",
      "            Initial guess. Array of real elements of size (n,),\n",
      "            where 'n' is the number of independent variables.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its\n",
      "            derivatives (`fun`, `jac` and `hess` functions).\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "                - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "                - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "                - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "                - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "                - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "                - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "                - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "                - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "                - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "                - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "                - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "                - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "                - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "                - custom - a callable object (added in version 0.14.0),\n",
      "                  see below for description.\n",
      "        \n",
      "            If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "            depending if the problem has constraints or bounds.\n",
      "        jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "            Method for computing the gradient vector. Only for CG, BFGS,\n",
      "            Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "            trust-exact and trust-constr. If it is a callable, it should be a\n",
      "            function that returns the gradient vector:\n",
      "        \n",
      "                ``jac(x, *args) -> array_like, shape (n,)``\n",
      "        \n",
      "            where x is an array with shape (n,) and `args` is a tuple with\n",
      "            the fixed parameters. Alternatively, the keywords\n",
      "            {'2-point', '3-point', 'cs'} select a finite\n",
      "            difference scheme for numerical estimation of the gradient. Options\n",
      "            '3-point' and 'cs' are available only to 'trust-constr'.\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            gradient along with the objective function. If False, the gradient\n",
      "            will be estimated using '2-point' finite difference estimation.\n",
      "        hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy},  optional\n",
      "            Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "            trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "            callable, it should return the  Hessian matrix:\n",
      "        \n",
      "                ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "        \n",
      "            where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "            parameters. LinearOperator and sparse matrix returns are\n",
      "            allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "            {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "            for numerical estimation. Or, objects implementing\n",
      "            `HessianUpdateStrategy` interface can be used to approximate\n",
      "            the Hessian. Available quasi-Newton methods implementing\n",
      "            this interface are:\n",
      "        \n",
      "                - `BFGS`;\n",
      "                - `SR1`.\n",
      "        \n",
      "            Whenever the gradient is estimated via finite-differences,\n",
      "            the Hessian cannot be estimated with options\n",
      "            {'2-point', '3-point', 'cs'} and needs to be\n",
      "            estimated using one of the quasi-Newton strategies.\n",
      "            Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "            `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "        hessp : callable, optional\n",
      "            Hessian of objective function times an arbitrary vector p. Only for\n",
      "            Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "            Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "            provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "            Hessian times an arbitrary vector:\n",
      "        \n",
      "                ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "        \n",
      "            where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "            dimension (n,) and `args` is a tuple with the fixed\n",
      "            parameters.\n",
      "        bounds : sequence or `Bounds`, optional\n",
      "            Bounds on variables for L-BFGS-B, TNC, SLSQP and\n",
      "            trust-constr methods. There are two ways to specify the bounds:\n",
      "        \n",
      "                1. Instance of `Bounds` class.\n",
      "                2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "                   is used to specify no bound.\n",
      "        \n",
      "        constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "            Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "            Constraints for 'trust-constr' are defined as a single object or a\n",
      "            list of objects specifying constraints to the optimization problem.\n",
      "            Available constraints are:\n",
      "        \n",
      "                - `LinearConstraint`\n",
      "                - `NonlinearConstraint`\n",
      "        \n",
      "            Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "            Each dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. All methods accept the following\n",
      "            generic options:\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform. Depending on the\n",
      "                    method each iteration may use several function evaluations.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            For method-specific options, see :func:`show_options()`.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration. For 'trust-constr' it is a callable with\n",
      "            the signature:\n",
      "        \n",
      "                ``callback(xk, OptimizeResult state) -> bool``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector. and ``state``\n",
      "            is an `OptimizeResult` object, with the same fields\n",
      "            as the ones from the return.  If callback returns True\n",
      "            the algorithm execution is terminated.\n",
      "            For all the other methods, the signature is:\n",
      "        \n",
      "                ``callback(xk)``\n",
      "        \n",
      "            where ``xk`` is the current parameter vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize_scalar : Interface to minimization algorithms for scalar\n",
      "            univariate functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *BFGS*.\n",
      "        \n",
      "        **Unconstrained minimization**\n",
      "        \n",
      "        Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "        Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "        applications. However, if numerical computation of derivative can be\n",
      "        trusted, other algorithms using the first and/or second derivatives\n",
      "        information might be preferred for their better performance in\n",
      "        general.\n",
      "        \n",
      "        Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "        of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "        method. It performs sequential one-dimensional minimizations along\n",
      "        each vector of the directions set (`direc` field in `options` and\n",
      "        `info`), which is updated at each iteration of the main\n",
      "        minimization loop. The function need not be differentiable, and no\n",
      "        derivatives are taken.\n",
      "        \n",
      "        Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "        gradient algorithm by Polak and Ribiere, a variant of the\n",
      "        Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "        first derivatives are used.\n",
      "        \n",
      "        Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "        method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "        pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "        performance even for non-smooth optimizations. This method also\n",
      "        returns an approximation of the Hessian inverse, stored as\n",
      "        `hess_inv` in the OptimizeResult object.\n",
      "        \n",
      "        Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "        Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "        Newton method). It uses a CG method to the compute the search\n",
      "        direction. See also *TNC* method for a box-constrained\n",
      "        minimization with a similar algorithm. Suitable for large-scale\n",
      "        problems.\n",
      "        \n",
      "        Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "        trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "        algorithm requires the gradient and Hessian; furthermore the\n",
      "        Hessian is required to be positive definite.\n",
      "        \n",
      "        Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "        Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "        unconstrained minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "        the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "        minimization. This algorithm requires the gradient\n",
      "        and either the Hessian or a function that computes the product of\n",
      "        the Hessian with a given vector. Suitable for large-scale problems.\n",
      "        On indefinite problems it requires usually less iterations than the\n",
      "        `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "        \n",
      "        Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "        is a trust-region method for unconstrained minimization in which\n",
      "        quadratic subproblems are solved almost exactly [13]_. This\n",
      "        algorithm requires the gradient and the Hessian (which is\n",
      "        *not* required to be positive definite). It is, in many\n",
      "        situations, the Newton method to converge in fewer iteraction\n",
      "        and the most recommended for small and medium-size problems.\n",
      "        \n",
      "        **Bound-Constrained minimization**\n",
      "        \n",
      "        Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "        algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "        \n",
      "        Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "        algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "        to bounds. This algorithm uses gradient information; it is also\n",
      "        called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "        method described above as it wraps a C implementation and allows\n",
      "        each variable to be given upper and lower bounds.\n",
      "        \n",
      "        **Constrained Minimization**\n",
      "        \n",
      "        Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "        Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "        [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "        approximations to the objective function and each constraint. The\n",
      "        method wraps a FORTRAN implementation of the algorithm. The\n",
      "        constraints functions 'fun' may return either a single number\n",
      "        or an array or list of numbers.\n",
      "        \n",
      "        Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "        Least SQuares Programming to minimize a function of several\n",
      "        variables with any combination of bounds, equality and inequality\n",
      "        constraints. The method wraps the SLSQP Optimization subroutine\n",
      "        originally implemented by Dieter Kraft [12]_. Note that the\n",
      "        wrapper handles infinite values in bounds by converting them into\n",
      "        large floating values.\n",
      "        \n",
      "        Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "        trust-region algorithm for constrained optimization. It swiches\n",
      "        between two implementations depending on the problem definition.\n",
      "        It is the most versatile constrained minimization algorithm\n",
      "        implemented in SciPy and the most appropriate for large-scale problems.\n",
      "        For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "        Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "        inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "        interior point  method described in [16]_. This interior point algorithm,\n",
      "        in turn, solves inequality constraints by introducing slack variables\n",
      "        and solving a sequence of equality-constrained barrier problems\n",
      "        for progressively smaller values of the barrier parameter.\n",
      "        The previously described equality constrained SQP method is\n",
      "        used to solve the subproblems with increasing levels of accuracy\n",
      "        as the iterate gets closer to a solution.\n",
      "        \n",
      "        **Finite-Difference Options**\n",
      "        \n",
      "        For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "        the gradient and the Hessian may be approximated using\n",
      "        three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "        The scheme 'cs' is, potentially, the most accurate but it\n",
      "        requires the function to correctly handles complex inputs and to\n",
      "        be differentiable in the complex plane. The scheme '3-point' is more\n",
      "        accurate than '2-point' but requires twice as much operations.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "        or a different library.  You can simply pass a callable as the ``method``\n",
      "        parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "        `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "        `fun` returns just the function values and `jac` is converted to a function\n",
      "        returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "        object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "            Minimization. The Computer Journal 7: 308-13.\n",
      "        .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "            respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "            Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "            Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "            191-208.\n",
      "        .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "           a function of several variables without calculating derivatives. The\n",
      "           Computer Journal 7: 155-162.\n",
      "        .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "           Numerical Recipes (any edition), Cambridge University Press.\n",
      "        .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "           Springer New York.\n",
      "        .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "           Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "           Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "        .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "           778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "           optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "           550-560.\n",
      "        .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "           1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "        .. [9] Powell, M J D. A direct search optimization method that models\n",
      "           the objective and constraint functions by linear interpolation.\n",
      "           1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "           and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "        .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "           calculations. 1998. Acta Numerica 7: 287-336.\n",
      "        .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "           derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "           2007/NA03\n",
      "        .. [12] Kraft, D. A software package for sequential quadratic\n",
      "           programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "           Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "        .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "           Trust region methods. 2000. Siam. pp. 169-200.\n",
      "        .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "           implementation of the GLTR method for iterative solution of\n",
      "           the trust region problem\", https://arxiv.org/abs/1611.04718\n",
      "        .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "           Trust-Region Subproblem using the Lanczos Method\",\n",
      "           SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "        .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "            An interior point algorithm for large-scale nonlinear  programming.\n",
      "            SIAM Journal on Optimization 9.4: 877-900.\n",
      "        .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "            implementation of an algorithm for large-scale equality constrained\n",
      "            optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "        function (and its respective derivatives) is implemented in `rosen`\n",
      "        (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "        \n",
      "        >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "        \n",
      "        A simple application of the *Nelder-Mead* method is:\n",
      "        \n",
      "        >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "        >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        \n",
      "        Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "        options:\n",
      "        \n",
      "        >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "        ...                options={'gtol': 1e-6, 'disp': True})\n",
      "        Optimization terminated successfully.\n",
      "                 Current function value: 0.000000\n",
      "                 Iterations: 26\n",
      "                 Function evaluations: 31\n",
      "                 Gradient evaluations: 31\n",
      "        >>> res.x\n",
      "        array([ 1.,  1.,  1.,  1.,  1.])\n",
      "        >>> print(res.message)\n",
      "        Optimization terminated successfully.\n",
      "        >>> res.hess_inv\n",
      "        array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "               [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "               [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "               [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "               [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "        \n",
      "        \n",
      "        Next, consider a minimization problem with several constraints (namely\n",
      "        Example 16.4 from [5]_). The objective function is:\n",
      "        \n",
      "        >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "        \n",
      "        There are three constraints defined as:\n",
      "        \n",
      "        >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "        ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "        \n",
      "        And variables must be positive, hence the following bounds:\n",
      "        \n",
      "        >>> bnds = ((0, None), (0, None))\n",
      "        \n",
      "        The optimization problem is solved using the SLSQP method as:\n",
      "        \n",
      "        >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "        ...                constraints=cons)\n",
      "        \n",
      "        It should converge to the theoretical solution (1.4 ,1.7).\n",
      "    \n",
      "    minimize_scalar(fun, bracket=None, bounds=None, args=(), method='brent', tol=None, options=None)\n",
      "        Minimization of scalar function of one variable.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            Objective function.\n",
      "            Scalar function, must return a scalar.\n",
      "        bracket : sequence, optional\n",
      "            For methods 'brent' and 'golden', `bracket` defines the bracketing\n",
      "            interval and can either have three items ``(a, b, c)`` so that\n",
      "            ``a < b < c`` and ``fun(b) < fun(a), fun(c)`` or two items ``a`` and\n",
      "            ``c`` which are assumed to be a starting interval for a downhill\n",
      "            bracket search (see `bracket`); it doesn't always mean that the\n",
      "            obtained solution will satisfy ``a <= x <= c``.\n",
      "        bounds : sequence, optional\n",
      "            For method 'bounded', `bounds` is mandatory and must have two items\n",
      "            corresponding to the optimization bounds.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function.\n",
      "        method : str or callable, optional\n",
      "            Type of solver.  Should be one of:\n",
      "        \n",
      "                - 'Brent'     :ref:`(see here) <optimize.minimize_scalar-brent>`\n",
      "                - 'Bounded'   :ref:`(see here) <optimize.minimize_scalar-bounded>`\n",
      "                - 'Golden'    :ref:`(see here) <optimize.minimize_scalar-golden>`\n",
      "                - custom - a callable object (added in version 0.14.0), see below\n",
      "        \n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options.\n",
      "        \n",
      "                maxiter : int\n",
      "                    Maximum number of iterations to perform.\n",
      "                disp : bool\n",
      "                    Set to True to print convergence messages.\n",
      "        \n",
      "            See :func:`show_options()` for solver-specific options.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the optimizer exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        minimize : Interface to minimization algorithms for scalar multivariate\n",
      "            functions\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *Brent*.\n",
      "        \n",
      "        Method :ref:`Brent <optimize.minimize_scalar-brent>` uses Brent's\n",
      "        algorithm to find a local minimum.  The algorithm uses inverse\n",
      "        parabolic interpolation when possible to speed up convergence of\n",
      "        the golden section method.\n",
      "        \n",
      "        Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
      "        golden section search technique. It uses analog of the bisection\n",
      "        method to decrease the bracketed interval. It is usually\n",
      "        preferable to use the *Brent* method.\n",
      "        \n",
      "        Method :ref:`Bounded <optimize.minimize_scalar-bounded>` can\n",
      "        perform bounded minimization. It uses the Brent method to find a\n",
      "        local minimum in the interval x1 < xopt < x2.\n",
      "        \n",
      "        **Custom minimizers**\n",
      "        \n",
      "        It may be useful to pass a custom minimization method, for example\n",
      "        when using some library frontend to minimize_scalar.  You can simply\n",
      "        pass a callable as the ``method`` parameter.\n",
      "        \n",
      "        The callable is called as ``method(fun, args, **kwargs, **options)``\n",
      "        where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "        (such as `bracket`, `tol`, etc.), except the `options` dict, which has\n",
      "        its contents also passed as `method` parameters pair by pair.  The method\n",
      "        shall return an `OptimizeResult` object.\n",
      "        \n",
      "        The provided `method` callable must be able to accept (and possibly ignore)\n",
      "        arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "        expand in future versions and then these parameters will be passed to\n",
      "        the method.  You can find an example in the scipy.optimize tutorial.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Consider the problem of minimizing the following function.\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x - 2) * x * (x + 2)**2\n",
      "        \n",
      "        Using the *Brent* method, we find the local minimum as:\n",
      "        \n",
      "        >>> from scipy.optimize import minimize_scalar\n",
      "        >>> res = minimize_scalar(f)\n",
      "        >>> res.x\n",
      "        1.28077640403\n",
      "        \n",
      "        Using the *Bounded* method, we find a local minimum with specified\n",
      "        bounds as:\n",
      "        \n",
      "        >>> res = minimize_scalar(f, bounds=(-3, -1), method='bounded')\n",
      "        >>> res.x\n",
      "        -2.0000002026\n",
      "    \n",
      "    newton(func, x0, fprime=None, args=(), tol=1.48e-08, maxiter=50, fprime2=None, x1=None, rtol=0.0, full_output=False, disp=True)\n",
      "        Find a zero of a real or complex function using the Newton-Raphson\n",
      "        (or secant or Halley's) method.\n",
      "        \n",
      "        Find a zero of the function `func` given a nearby starting point `x0`.\n",
      "        The Newton-Raphson method is used if the derivative `fprime` of `func`\n",
      "        is provided, otherwise the secant method is used.  If the second order\n",
      "        derivative `fprime2` of `func` is also provided, then Halley's method is\n",
      "        used.\n",
      "        \n",
      "        If `x0` is a sequence with more than one item, then `newton` returns an\n",
      "        array, and `func` must be vectorized and return a sequence or array of the\n",
      "        same shape as its first argument. If `fprime` or `fprime2` is given then\n",
      "        its return must also have the same shape.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The function whose zero is wanted. It must be a function of a\n",
      "            single variable of the form ``f(x,a,b,c...)``, where ``a,b,c...``\n",
      "            are extra arguments that can be passed in the `args` parameter.\n",
      "        x0 : float, sequence, or ndarray\n",
      "            An initial estimate of the zero that should be somewhere near the\n",
      "            actual zero. If not scalar, then `func` must be vectorized and return\n",
      "            a sequence or array of the same shape as its first argument.\n",
      "        fprime : callable, optional\n",
      "            The derivative of the function when available and convenient. If it\n",
      "            is None (default), then the secant method is used.\n",
      "        args : tuple, optional\n",
      "            Extra arguments to be used in the function call.\n",
      "        tol : float, optional\n",
      "            The allowable error of the zero value.  If `func` is complex-valued,\n",
      "            a larger `tol` is recommended as both the real and imaginary parts\n",
      "            of `x` contribute to ``|x - x0|``.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        fprime2 : callable, optional\n",
      "            The second order derivative of the function when available and\n",
      "            convenient. If it is None (default), then the normal Newton-Raphson\n",
      "            or the secant method is used. If it is not None, then Halley's method\n",
      "            is used.\n",
      "        x1 : float, optional\n",
      "            Another estimate of the zero that should be somewhere near the\n",
      "            actual zero.  Used if `fprime` is not provided.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False (default), the root is returned.\n",
      "            If True and `x0` is scalar, the return value is ``(x, r)``, where ``x``\n",
      "            is the root and ``r`` is a `RootResults` object.\n",
      "            If True and `x0` is non-scalar, the return value is ``(x, converged,\n",
      "            zero_der)`` (see Returns section for details).\n",
      "        disp : bool, optional\n",
      "            If True, raise a RuntimeError if the algorithm didn't converge, with\n",
      "            the error message containing the number of iterations and current\n",
      "            function value.  Otherwise the convergence status is recorded in a\n",
      "            `RootResults` return object.\n",
      "            Ignored if `x0` is not scalar.\n",
      "            *Note: this has little to do with displaying, however\n",
      "            the `disp` keyword cannot be renamed for backwards compatibility.*\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        root : float, sequence, or ndarray\n",
      "            Estimated location where function is zero.\n",
      "        r : `RootResults`, optional\n",
      "            Present if ``full_output=True`` and `x0` is scalar.\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        converged : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements converged successfully.\n",
      "        zero_der : ndarray of bool, optional\n",
      "            Present if ``full_output=True`` and `x0` is non-scalar.\n",
      "            For vector functions, indicates which elements had a zero derivative.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect\n",
      "        fsolve : find zeros in n dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The convergence rate of the Newton-Raphson method is quadratic,\n",
      "        the Halley method is cubic, and the secant method is\n",
      "        sub-quadratic.  This means that if the function is well behaved\n",
      "        the actual error in the estimated zero after the n-th iteration\n",
      "        is approximately the square (cube for Halley) of the error\n",
      "        after the (n-1)-th step.  However, the stopping criterion used\n",
      "        here is the step size and there is no guarantee that a zero\n",
      "        has been found. Consequently the result should be verified.\n",
      "        Safer algorithms are brentq, brenth, ridder, and bisect,\n",
      "        but they all require that the root first be bracketed in an\n",
      "        interval where the function changes sign. The brentq algorithm\n",
      "        is recommended for general use in one dimensional problems\n",
      "        when such an interval has been found.\n",
      "        \n",
      "        When `newton` is used with arrays, it is best suited for the following\n",
      "        types of problems:\n",
      "        \n",
      "        * The initial guesses, `x0`, are all relatively the same distance from\n",
      "          the roots.\n",
      "        * Some or all of the extra arguments, `args`, are also arrays so that a\n",
      "          class of similar problems can be solved together.\n",
      "        * The size of the initial guesses, `x0`, is larger than O(100) elements.\n",
      "          Otherwise, a naive loop may perform as well or better than a vector.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy import optimize\n",
      "        >>> import matplotlib.pyplot as plt\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        ``fprime`` is not provided, use the secant method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        >>> root = optimize.newton(f, 1.5, fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0000000000000016\n",
      "        \n",
      "        Only ``fprime`` is provided, use the Newton-Raphson method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        Both ``fprime2`` and ``fprime`` are provided, use Halley's method:\n",
      "        \n",
      "        >>> root = optimize.newton(f, 1.5, fprime=lambda x: 3 * x**2,\n",
      "        ...                        fprime2=lambda x: 6 * x)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        When we want to find zeros for a set of related starting values and/or\n",
      "        function parameters, we can provide both of those as an array of inputs:\n",
      "        \n",
      "        >>> f = lambda x, a: x**3 - a\n",
      "        >>> fder = lambda x, a: 3 * x**2\n",
      "        >>> np.random.seed(4321)\n",
      "        >>> x = np.random.randn(100)\n",
      "        >>> a = np.arange(-50, 50)\n",
      "        >>> vec_res = optimize.newton(f, x, fprime=fder, args=(a, ))\n",
      "        \n",
      "        The above is the equivalent of solving for each value in ``(x, a)``\n",
      "        separately in a for-loop, just faster:\n",
      "        \n",
      "        >>> loop_res = [optimize.newton(f, x0, fprime=fder, args=(a0,))\n",
      "        ...             for x0, a0 in zip(x, a)]\n",
      "        >>> np.allclose(vec_res, loop_res)\n",
      "        True\n",
      "        \n",
      "        Plot the results found for all values of ``a``:\n",
      "        \n",
      "        >>> analytical_result = np.sign(a) * np.abs(a)**(1/3)\n",
      "        >>> fig = plt.figure()\n",
      "        >>> ax = fig.add_subplot(111)\n",
      "        >>> ax.plot(a, analytical_result, 'o')\n",
      "        >>> ax.plot(a, vec_res, '.')\n",
      "        >>> ax.set_xlabel('$a$')\n",
      "        >>> ax.set_ylabel('$x$ where $f(x, a)=0$')\n",
      "        >>> plt.show()\n",
      "    \n",
      "    newton_krylov(F, xin, iter=None, rdiff=None, method='lgmres', inner_maxiter=20, inner_M=None, outer_k=10, verbose=False, maxiter=None, f_tol=None, f_rtol=None, x_tol=None, x_rtol=None, tol_norm=None, line_search='armijo', callback=None, **kw)\n",
      "        Find a root of a function, using Krylov approximation for inverse Jacobian.\n",
      "        \n",
      "        This method is suitable for solving large-scale problems.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        F : function(x) -> f\n",
      "            Function whose root to find; should take and return an array-like\n",
      "            object.\n",
      "        xin : array_like\n",
      "            Initial guess for the solution\n",
      "        rdiff : float, optional\n",
      "            Relative step size to use in numerical differentiation.\n",
      "        method : {'lgmres', 'gmres', 'bicgstab', 'cgs', 'minres'} or function\n",
      "            Krylov method to use to approximate the Jacobian.\n",
      "            Can be a string, or a function implementing the same interface as\n",
      "            the iterative solvers in `scipy.sparse.linalg`.\n",
      "        \n",
      "            The default is `scipy.sparse.linalg.lgmres`.\n",
      "        inner_M : LinearOperator or InverseJacobian\n",
      "            Preconditioner for the inner Krylov iteration.\n",
      "            Note that you can use also inverse Jacobians as (adaptive)\n",
      "            preconditioners. For example,\n",
      "        \n",
      "            >>> from scipy.optimize.nonlin import BroydenFirst, KrylovJacobian\n",
      "            >>> from scipy.optimize.nonlin import InverseJacobian\n",
      "            >>> jac = BroydenFirst()\n",
      "            >>> kjac = KrylovJacobian(inner_M=InverseJacobian(jac))\n",
      "        \n",
      "            If the preconditioner has a method named 'update', it will be called\n",
      "            as ``update(x, f)`` after each nonlinear step, with ``x`` giving\n",
      "            the current point, and ``f`` the current function value.\n",
      "        inner_tol, inner_maxiter, ...\n",
      "            Parameters to pass on to the \\\"inner\\\" Krylov solver.\n",
      "            See `scipy.sparse.linalg.gmres` for details.\n",
      "        outer_k : int, optional\n",
      "            Size of the subspace kept across LGMRES nonlinear iterations.\n",
      "            See `scipy.sparse.linalg.lgmres` for details.\n",
      "        iter : int, optional\n",
      "            Number of iterations to make. If omitted (default), make as many\n",
      "            as required to meet tolerances.\n",
      "        verbose : bool, optional\n",
      "            Print status to stdout on every iteration.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations to make. If more are needed to\n",
      "            meet convergence, `NoConvergence` is raised.\n",
      "        f_tol : float, optional\n",
      "            Absolute tolerance (in max-norm) for the residual.\n",
      "            If omitted, default is 6e-6.\n",
      "        f_rtol : float, optional\n",
      "            Relative tolerance for the residual. If omitted, not used.\n",
      "        x_tol : float, optional\n",
      "            Absolute minimum step size, as determined from the Jacobian\n",
      "            approximation. If the step size is smaller than this, optimization\n",
      "            is terminated as successful. If omitted, not used.\n",
      "        x_rtol : float, optional\n",
      "            Relative minimum step size. If omitted, not used.\n",
      "        tol_norm : function(vector) -> scalar, optional\n",
      "            Norm to use in convergence check. Default is the maximum norm.\n",
      "        line_search : {None, 'armijo' (default), 'wolfe'}, optional\n",
      "            Which type of a line search to use to determine the step size in the\n",
      "            direction given by the Jacobian approximation. Defaults to 'armijo'.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : ndarray\n",
      "            An array (of similar array type as `x0`) containing the final solution.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        NoConvergence\n",
      "            When a solution was not found.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        root : Interface to root finding algorithms for multivariate\n",
      "               functions. See ``method=='krylov'`` in particular.\n",
      "        scipy.sparse.linalg.gmres\n",
      "        scipy.sparse.linalg.lgmres\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements a Newton-Krylov solver. The basic idea is\n",
      "        to compute the inverse of the Jacobian with an iterative Krylov\n",
      "        method. These methods require only evaluating the Jacobian-vector\n",
      "        products, which are conveniently approximated by a finite difference:\n",
      "        \n",
      "        .. math:: J v \\approx (f(x + \\omega*v/|v|) - f(x)) / \\omega\n",
      "        \n",
      "        Due to the use of iterative matrix inverses, these methods can\n",
      "        deal with large nonlinear problems.\n",
      "        \n",
      "        SciPy's `scipy.sparse.linalg` module offers a selection of Krylov\n",
      "        solvers to choose from. The default here is `lgmres`, which is a\n",
      "        variant of restarted GMRES iteration that reuses some of the\n",
      "        information obtained in the previous Newton steps to invert\n",
      "        Jacobians in subsequent steps.\n",
      "        \n",
      "        For a review on Newton-Krylov methods, see for example [1]_,\n",
      "        and for the LGMRES sparse inverse method, see [2]_.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] D.A. Knoll and D.E. Keyes, J. Comp. Phys. 193, 357 (2004).\n",
      "               :doi:`10.1016/j.jcp.2003.08.010`\n",
      "        .. [2] A.H. Baker and E.R. Jessup and T. Manteuffel,\n",
      "               SIAM J. Matrix Anal. Appl. 26, 962 (2005).\n",
      "               :doi:`10.1137/S0895479803422014`\n",
      "    \n",
      "    nnls(A, b, maxiter=None)\n",
      "        Solve ``argmin_x || Ax - b ||_2`` for ``x>=0``. This is a wrapper\n",
      "        for a FORTRAN non-negative least squares solver.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : ndarray\n",
      "            Matrix ``A`` as shown above.\n",
      "        b : ndarray\n",
      "            Right-hand side vector.\n",
      "        maxiter: int, optional\n",
      "            Maximum number of iterations, optional.\n",
      "            Default is ``3 * A.shape[1]``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x : ndarray\n",
      "            Solution vector.\n",
      "        rnorm : float\n",
      "            The residual, ``|| Ax-b ||_2``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        lsq_linear : Linear least squares with bounds on the variables\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The FORTRAN code was published in the book below. The algorithm\n",
      "        is an active set method. It solves the KKT (Karush-Kuhn-Tucker)\n",
      "        conditions for the non-negative least squares problem.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Lawson C., Hanson R.J., (1987) Solving Least Squares Problems, SIAM\n",
      "        \n",
      "         Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import nnls\n",
      "        ...\n",
      "        >>> A = np.array([[1, 0], [1, 0], [0, 1]])\n",
      "        >>> b = np.array([2, 1, 1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([1.5, 1. ]), 0.7071067811865475)\n",
      "        \n",
      "        >>> b = np.array([-1, -1, -1])\n",
      "        >>> nnls(A, b)\n",
      "        (array([0., 0.]), 1.7320508075688772)\n",
      "    \n",
      "    ridder(f, a, b, args=(), xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a root of a function in an interval using Ridder's method.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a number.  f must be continuous, and f(a) and\n",
      "            f(b) must have opposite signs.\n",
      "        a : scalar\n",
      "            One end of the bracketing interval [a,b].\n",
      "        b : scalar\n",
      "            The other end of the bracketing interval [a,b].\n",
      "        xtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : number, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter cannot be smaller than its default value of\n",
      "            ``4*np.finfo(float).eps``.\n",
      "        maxiter : int, optional\n",
      "            if convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``apply(f, (x)+args)``.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise the convergence status is recorded in any `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Zero of `f` between `a` and `b`.\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.\n",
      "            In particular, ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, bisect, newton : one-dimensional root-finding\n",
      "        fixed_point : scalar fixed-point finder\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Uses [Ridders1979]_ method to find a zero of the function `f` between the\n",
      "        arguments `a` and `b`. Ridders' method is faster than bisection, but not\n",
      "        generally as fast as the Brent routines. [Ridders1979]_ provides the\n",
      "        classic description and source of the algorithm. A description can also be\n",
      "        found in any recent edition of Numerical Recipes.\n",
      "        \n",
      "        The routine used here diverges slightly from standard presentations in\n",
      "        order to be a bit more careful of tolerance.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Ridders1979]\n",
      "           Ridders, C. F. J. \"A New Algorithm for Computing a\n",
      "           Single Root of a Real Continuous Function.\"\n",
      "           IEEE Trans. Circuits Systems 26, 979-980, 1979.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> def f(x):\n",
      "        ...     return (x**2 - 1)\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        \n",
      "        >>> root = optimize.ridder(f, 0, 2)\n",
      "        >>> root\n",
      "        1.0\n",
      "        \n",
      "        >>> root = optimize.ridder(f, -2, 0)\n",
      "        >>> root\n",
      "        -1.0\n",
      "    \n",
      "    root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)\n",
      "        Find a root of a vector function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fun : callable\n",
      "            A vector function to find a root of.\n",
      "        x0 : ndarray\n",
      "            Initial guess.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its Jacobian.\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'hybr'             :ref:`(see here) <optimize.root-hybr>`\n",
      "                - 'lm'               :ref:`(see here) <optimize.root-lm>`\n",
      "                - 'broyden1'         :ref:`(see here) <optimize.root-broyden1>`\n",
      "                - 'broyden2'         :ref:`(see here) <optimize.root-broyden2>`\n",
      "                - 'anderson'         :ref:`(see here) <optimize.root-anderson>`\n",
      "                - 'linearmixing'     :ref:`(see here) <optimize.root-linearmixing>`\n",
      "                - 'diagbroyden'      :ref:`(see here) <optimize.root-diagbroyden>`\n",
      "                - 'excitingmixing'   :ref:`(see here) <optimize.root-excitingmixing>`\n",
      "                - 'krylov'           :ref:`(see here) <optimize.root-krylov>`\n",
      "                - 'df-sane'          :ref:`(see here) <optimize.root-dfsane>`\n",
      "        \n",
      "        jac : bool or callable, optional\n",
      "            If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "            value of Jacobian along with the objective function. If False, the\n",
      "            Jacobian will be estimated numerically.\n",
      "            `jac` can also be a callable returning the Jacobian of `fun`. In\n",
      "            this case, it must accept the same arguments as `fun`.\n",
      "        tol : float, optional\n",
      "            Tolerance for termination. For detailed control, use solver-specific\n",
      "            options.\n",
      "        callback : function, optional\n",
      "            Optional callback function. It is called on every iteration as\n",
      "            ``callback(x, f)`` where `x` is the current solution and `f`\n",
      "            the corresponding residual. For all methods but 'hybr' and 'lm'.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g. `xtol` or `maxiter`, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : OptimizeResult\n",
      "            The solution represented as a ``OptimizeResult`` object.\n",
      "            Important attributes are: ``x`` the solution array, ``success`` a\n",
      "            Boolean flag indicating if the algorithm exited successfully and\n",
      "            ``message`` which describes the cause of the termination. See\n",
      "            `OptimizeResult` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter. The default method is *hybr*.\n",
      "        \n",
      "        Method *hybr* uses a modification of the Powell hybrid method as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *lm* solves the system of nonlinear equations in a least squares\n",
      "        sense using a modification of the Levenberg-Marquardt algorithm as\n",
      "        implemented in MINPACK [1]_.\n",
      "        \n",
      "        Method *df-sane* is a derivative-free spectral method. [3]_\n",
      "        \n",
      "        Methods *broyden1*, *broyden2*, *anderson*, *linearmixing*,\n",
      "        *diagbroyden*, *excitingmixing*, *krylov* are inexact Newton methods,\n",
      "        with backtracking or full line searches [2]_. Each method corresponds\n",
      "        to a particular Jacobian approximations. See `nonlin` for details.\n",
      "        \n",
      "        - Method *broyden1* uses Broyden's first Jacobian approximation, it is\n",
      "          known as Broyden's good method.\n",
      "        - Method *broyden2* uses Broyden's second Jacobian approximation, it\n",
      "          is known as Broyden's bad method.\n",
      "        - Method *anderson* uses (extended) Anderson mixing.\n",
      "        - Method *Krylov* uses Krylov approximation for inverse Jacobian. It\n",
      "          is suitable for large-scale problem.\n",
      "        - Method *diagbroyden* uses diagonal Broyden Jacobian approximation.\n",
      "        - Method *linearmixing* uses a scalar Jacobian approximation.\n",
      "        - Method *excitingmixing* uses a tuned diagonal Jacobian\n",
      "          approximation.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            The algorithms implemented for methods *diagbroyden*,\n",
      "            *linearmixing* and *excitingmixing* may be useful for specific\n",
      "            problems, but whether they will work may depend strongly on the\n",
      "            problem.\n",
      "        \n",
      "        .. versionadded:: 0.11.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] More, Jorge J., Burton S. Garbow, and Kenneth E. Hillstrom.\n",
      "           1980. User Guide for MINPACK-1.\n",
      "        .. [2] C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear\n",
      "           Equations. Society for Industrial and Applied Mathematics.\n",
      "           <https://archive.siam.org/books/kelley/fr16/>\n",
      "        .. [3] W. La Cruz, J.M. Martinez, M. Raydan. Math. Comp. 75, 1429 (2006).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        The following functions define a system of nonlinear equations and its\n",
      "        jacobian.\n",
      "        \n",
      "        >>> def fun(x):\n",
      "        ...     return [x[0]  + 0.5 * (x[0] - x[1])**3 - 1.0,\n",
      "        ...             0.5 * (x[1] - x[0])**3 + x[1]]\n",
      "        \n",
      "        >>> def jac(x):\n",
      "        ...     return np.array([[1 + 1.5 * (x[0] - x[1])**2,\n",
      "        ...                       -1.5 * (x[0] - x[1])**2],\n",
      "        ...                      [-1.5 * (x[1] - x[0])**2,\n",
      "        ...                       1 + 1.5 * (x[1] - x[0])**2]])\n",
      "        \n",
      "        A solution can be obtained as follows.\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> sol = optimize.root(fun, [0, 0], jac=jac, method='hybr')\n",
      "        >>> sol.x\n",
      "        array([ 0.8411639,  0.1588361])\n",
      "    \n",
      "    root_scalar(f, args=(), method=None, bracket=None, fprime=None, fprime2=None, x0=None, x1=None, xtol=None, rtol=None, maxiter=None, options=None)\n",
      "        Find a root of a scalar function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : callable\n",
      "            A function to find a root of.\n",
      "        args : tuple, optional\n",
      "            Extra arguments passed to the objective function and its derivative(s).\n",
      "        method : str, optional\n",
      "            Type of solver.  Should be one of\n",
      "        \n",
      "                - 'bisect'    :ref:`(see here) <optimize.root_scalar-bisect>`\n",
      "                - 'brentq'    :ref:`(see here) <optimize.root_scalar-brentq>`\n",
      "                - 'brenth'    :ref:`(see here) <optimize.root_scalar-brenth>`\n",
      "                - 'ridder'    :ref:`(see here) <optimize.root_scalar-ridder>`\n",
      "                - 'toms748'    :ref:`(see here) <optimize.root_scalar-toms748>`\n",
      "                - 'newton'    :ref:`(see here) <optimize.root_scalar-newton>`\n",
      "                - 'secant'    :ref:`(see here) <optimize.root_scalar-secant>`\n",
      "                - 'halley'    :ref:`(see here) <optimize.root_scalar-halley>`\n",
      "        \n",
      "        bracket: A sequence of 2 floats, optional\n",
      "            An interval bracketing a root.  `f(x, *args)` must have different\n",
      "            signs at the two endpoints.\n",
      "        x0 : float, optional\n",
      "            Initial guess.\n",
      "        x1 : float, optional\n",
      "            A second guess.\n",
      "        fprime : bool or callable, optional\n",
      "            If `fprime` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the derivative.\n",
      "            `fprime` can also be a callable returning the derivative of `f`. In\n",
      "            this case, it must accept the same arguments as `f`.\n",
      "        fprime2 : bool or callable, optional\n",
      "            If `fprime2` is a boolean and is True, `f` is assumed to return the\n",
      "            value of the objective function and of the\n",
      "            first and second derivatives.\n",
      "            `fprime2` can also be a callable returning the second derivative of `f`.\n",
      "            In this case, it must accept the same arguments as `f`.\n",
      "        xtol : float, optional\n",
      "            Tolerance (absolute) for termination.\n",
      "        rtol : float, optional\n",
      "            Tolerance (relative) for termination.\n",
      "        maxiter : int, optional\n",
      "            Maximum number of iterations.\n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. E.g. ``k``, see\n",
      "            :obj:`show_options()` for details.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        sol : RootResults\n",
      "            The solution represented as a ``RootResults`` object.\n",
      "            Important attributes are: ``root`` the solution , ``converged`` a\n",
      "            boolean flag indicating if the algorithm exited successfully and\n",
      "            ``flag`` which describes the cause of the termination. See\n",
      "            `RootResults` for a description of other attributes.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        show_options : Additional options accepted by the solvers\n",
      "        root : Find a root of a vector function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This section describes the available solvers that can be selected by the\n",
      "        'method' parameter.\n",
      "        \n",
      "        The default is to use the best method available for the situation\n",
      "        presented.\n",
      "        If a bracket is provided, it may use one of the bracketing methods.\n",
      "        If a derivative and an initial value are specified, it may\n",
      "        select one of the derivative-based methods.\n",
      "        If no method is judged applicable, it will raise an Exception.\n",
      "        \n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Find the root of a simple cubic\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> def fprime(x):\n",
      "        ...     return 3*x**2\n",
      "        \n",
      "        The `brentq` method takes as input a bracket\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, bracket=[0, 3], method='brentq')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 10, 11)\n",
      "        \n",
      "        The `newton` method takes as input a single point and uses the derivative(s)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f, x0=0.2, fprime=fprime, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 22)\n",
      "        \n",
      "        The function can provide the value and derivative(s) in a single call.\n",
      "        \n",
      "        >>> def f_p_pp(x):\n",
      "        ...     return (x**3 - 1), 3*x**2, 6*x\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, method='newton')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 11, 11)\n",
      "        \n",
      "        >>> sol = optimize.root_scalar(f_p_pp, x0=0.2, fprime=True, fprime2=True, method='halley')\n",
      "        >>> sol.root, sol.iterations, sol.function_calls\n",
      "        (1.0, 7, 8)\n",
      "    \n",
      "    rosen(x)\n",
      "        The Rosenbrock function.\n",
      "        \n",
      "        The function computed is::\n",
      "        \n",
      "            sum(100.0*(x[1:] - x[:-1]**2.0)**2.0 + (1 - x[:-1])**2.0)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Rosenbrock function is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f : float\n",
      "            The value of the Rosenbrock function.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen_der, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen\n",
      "        >>> X = 0.1 * np.arange(10)\n",
      "        >>> rosen(X)\n",
      "        76.56\n",
      "    \n",
      "    rosen_der(x)\n",
      "        The derivative (i.e. gradient) of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the derivative is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_der : (N,) ndarray\n",
      "            The gradient of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_hess, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_der\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> rosen_der(X)\n",
      "        array([ -2. ,  10.6,  15.6,  13.4,   6.4,  -3. , -12.4, -19.4,  62. ])\n",
      "    \n",
      "    rosen_hess(x)\n",
      "        The Hessian matrix of the Rosenbrock function.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess_prod\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess\n",
      "        >>> X = 0.1 * np.arange(4)\n",
      "        >>> rosen_hess(X)\n",
      "        array([[-38.,   0.,   0.,   0.],\n",
      "               [  0., 134., -40.,   0.],\n",
      "               [  0., -40., 130., -80.],\n",
      "               [  0.,   0., -80., 200.]])\n",
      "    \n",
      "    rosen_hess_prod(x, p)\n",
      "        Product of the Hessian matrix of the Rosenbrock function with a vector.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array_like\n",
      "            1-D array of points at which the Hessian matrix is to be computed.\n",
      "        p : array_like\n",
      "            1-D array, the vector to be multiplied by the Hessian matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        rosen_hess_prod : ndarray\n",
      "            The Hessian matrix of the Rosenbrock function at `x` multiplied\n",
      "            by the vector `p`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rosen, rosen_der, rosen_hess\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from scipy.optimize import rosen_hess_prod\n",
      "        >>> X = 0.1 * np.arange(9)\n",
      "        >>> p = 0.5 * np.arange(9)\n",
      "        >>> rosen_hess_prod(X, p)\n",
      "        array([  -0.,   27.,  -10.,  -95., -192., -265., -278., -195., -180.])\n",
      "    \n",
      "    shgo(func, bounds, args=(), constraints=None, n=100, iters=1, callback=None, minimizer_kwargs=None, options=None, sampling_method='simplicial')\n",
      "        Finds the global minimum of a function using SHG optimization.\n",
      "        \n",
      "        SHGO stands for \"simplicial homology global optimization\".\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        func : callable\n",
      "            The objective function to be minimized.  Must be in the form\n",
      "            ``f(x, *args)``, where ``x`` is the argument in the form of a 1-D array\n",
      "            and ``args`` is a tuple of any additional fixed parameters needed to\n",
      "            completely specify the function.\n",
      "        bounds : sequence\n",
      "            Bounds for variables.  ``(min, max)`` pairs for each element in ``x``,\n",
      "            defining the lower and upper bounds for the optimizing argument of\n",
      "            `func`. It is required to have ``len(bounds) == len(x)``.\n",
      "            ``len(bounds)`` is used to determine the number of parameters in ``x``.\n",
      "            Use ``None`` for one of min or max when there is no bound in that\n",
      "            direction. By default bounds are ``(None, None)``.\n",
      "        args : tuple, optional\n",
      "            Any additional fixed parameters needed to completely specify the\n",
      "            objective function.\n",
      "        constraints : dict or sequence of dict, optional\n",
      "            Constraints definition.\n",
      "            Function(s) ``R**n`` in the form::\n",
      "        \n",
      "                g(x) <= 0 applied as g : R^n -> R^m\n",
      "                h(x) == 0 applied as h : R^n -> R^p\n",
      "        \n",
      "            Each constraint is defined in a dictionary with fields:\n",
      "        \n",
      "                type : str\n",
      "                    Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "                fun : callable\n",
      "                    The function defining the constraint.\n",
      "                jac : callable, optional\n",
      "                    The Jacobian of `fun` (only for SLSQP).\n",
      "                args : sequence, optional\n",
      "                    Extra arguments to be passed to the function and Jacobian.\n",
      "        \n",
      "            Equality constraint means that the constraint function result is to\n",
      "            be zero whereas inequality means that it is to be non-negative.\n",
      "            Note that COBYLA only supports inequality constraints.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "               Only the COBYLA and SLSQP local minimize methods currently\n",
      "               support constraint arguments. If the ``constraints`` sequence\n",
      "               used in the local optimization problem is not defined in\n",
      "               ``minimizer_kwargs`` and a constrained method is used then the\n",
      "               global ``constraints`` will be used.\n",
      "               (Defining a ``constraints`` sequence in ``minimizer_kwargs``\n",
      "               means that ``constraints`` will not be added so if equality\n",
      "               constraints and so forth need to be added then the inequality\n",
      "               functions in ``constraints`` need to be added to\n",
      "               ``minimizer_kwargs`` too).\n",
      "        \n",
      "        n : int, optional\n",
      "            Number of sampling points used in the construction of the simplicial\n",
      "            complex. Note that this argument is only used for ``sobol`` and other\n",
      "            arbitrary `sampling_methods`.\n",
      "        iters : int, optional\n",
      "            Number of iterations used in the construction of the simplicial complex.\n",
      "        callback : callable, optional\n",
      "            Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "            current parameter vector.\n",
      "        minimizer_kwargs : dict, optional\n",
      "            Extra keyword arguments to be passed to the minimizer\n",
      "            ``scipy.optimize.minimize`` Some important options could be:\n",
      "        \n",
      "                * method : str\n",
      "                    The minimization method (e.g. ``SLSQP``).\n",
      "                * args : tuple\n",
      "                    Extra arguments passed to the objective function (``func``) and\n",
      "                    its derivatives (Jacobian, Hessian).\n",
      "                * options : dict, optional\n",
      "                    Note that by default the tolerance is specified as\n",
      "                    ``{ftol: 1e-12}``\n",
      "        \n",
      "        options : dict, optional\n",
      "            A dictionary of solver options. Many of the options specified for the\n",
      "            global routine are also passed to the scipy.optimize.minimize routine.\n",
      "            The options that are also passed to the local routine are marked with\n",
      "            \"(L)\".\n",
      "        \n",
      "            Stopping criteria, the algorithm will terminate if any of the specified\n",
      "            criteria are met. However, the default algorithm does not require any to\n",
      "            be specified:\n",
      "        \n",
      "            * maxfev : int (L)\n",
      "                Maximum number of function evaluations in the feasible domain.\n",
      "                (Note only methods that support this option will terminate\n",
      "                the routine at precisely exact specified value. Otherwise the\n",
      "                criterion will only terminate during a global iteration)\n",
      "            * f_min\n",
      "                Specify the minimum objective function value, if it is known.\n",
      "            * f_tol : float\n",
      "                Precision goal for the value of f in the stopping\n",
      "                criterion. Note that the global routine will also\n",
      "                terminate if a sampling point in the global routine is\n",
      "                within this tolerance.\n",
      "            * maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            * maxev : int\n",
      "                Maximum number of sampling evaluations to perform (includes\n",
      "                searching in infeasible points).\n",
      "            * maxtime : float\n",
      "                Maximum processing runtime allowed\n",
      "            * minhgrd : int\n",
      "                Minimum homology group rank differential. The homology group of the\n",
      "                objective function is calculated (approximately) during every\n",
      "                iteration. The rank of this group has a one-to-one correspondence\n",
      "                with the number of locally convex subdomains in the objective\n",
      "                function (after adequate sampling points each of these subdomains\n",
      "                contain a unique global minimum). If the difference in the hgr is 0\n",
      "                between iterations for ``maxhgrd`` specified iterations the\n",
      "                algorithm will terminate.\n",
      "        \n",
      "            Objective function knowledge:\n",
      "        \n",
      "            * symmetry : bool\n",
      "                Specify True if the objective function contains symmetric variables.\n",
      "                The search space (and therefore performance) is decreased by O(n!).\n",
      "        \n",
      "            * jac : bool or callable, optional\n",
      "                Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
      "                Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg. If ``jac`` is a\n",
      "                boolean and is True, ``fun`` is assumed to return the gradient along\n",
      "                with the objective function. If False, the gradient will be\n",
      "                estimated numerically. ``jac`` can also be a callable returning the\n",
      "                gradient of the objective. In this case, it must accept the same\n",
      "                arguments as ``fun``. (Passed to `scipy.optimize.minmize` automatically)\n",
      "        \n",
      "            * hess, hessp : callable, optional\n",
      "                Hessian (matrix of second-order derivatives) of objective function\n",
      "                or Hessian of objective function times an arbitrary vector p.\n",
      "                Only for Newton-CG, dogleg, trust-ncg. Only one of ``hessp`` or\n",
      "                ``hess`` needs to be given. If ``hess`` is provided, then\n",
      "                ``hessp`` will be ignored. If neither ``hess`` nor ``hessp`` is\n",
      "                provided, then the Hessian product will be approximated using\n",
      "                finite differences on ``jac``. ``hessp`` must compute the Hessian\n",
      "                times an arbitrary vector. (Passed to `scipy.optimize.minmize`\n",
      "                automatically)\n",
      "        \n",
      "            Algorithm settings:\n",
      "        \n",
      "            * minimize_every_iter : bool\n",
      "                If True then promising global sampling points will be passed to a\n",
      "                local minimisation routine every iteration. If False then only the\n",
      "                final minimiser pool will be run. Defaults to False.\n",
      "            * local_iter : int\n",
      "                Only evaluate a few of the best minimiser pool candidates every\n",
      "                iteration. If False all potential points are passed to the local\n",
      "                minimisation routine.\n",
      "            * infty_constraints: bool\n",
      "                If True then any sampling points generated which are outside will\n",
      "                the feasible domain will be saved and given an objective function\n",
      "                value of ``inf``. If False then these points will be discarded.\n",
      "                Using this functionality could lead to higher performance with\n",
      "                respect to function evaluations before the global minimum is found,\n",
      "                specifying False will use less memory at the cost of a slight\n",
      "                decrease in performance. Defaults to True.\n",
      "        \n",
      "            Feedback:\n",
      "        \n",
      "            * disp : bool (L)\n",
      "                Set to True to print convergence messages.\n",
      "        \n",
      "        sampling_method : str or function, optional\n",
      "            Current built in sampling method options are ``sobol`` and\n",
      "            ``simplicial``. The default ``simplicial`` uses less memory and provides\n",
      "            the theoretical guarantee of convergence to the global minimum in finite\n",
      "            time. The ``sobol`` method is faster in terms of sampling point\n",
      "            generation at the cost of higher memory resources and the loss of\n",
      "            guaranteed convergence. It is more appropriate for most \"easier\"\n",
      "            problems where the convergence is relatively fast.\n",
      "            User defined sampling functions must accept two arguments of ``n``\n",
      "            sampling points of dimension ``dim`` per call and output an array of\n",
      "            sampling points with shape `n x dim`. \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        res : OptimizeResult\n",
      "            The optimization result represented as a `OptimizeResult` object.\n",
      "            Important attributes are:\n",
      "            ``x`` the solution array corresponding to the global minimum,\n",
      "            ``fun`` the function output at the global solution,\n",
      "            ``xl`` an ordered list of local minima solutions,\n",
      "            ``funl`` the function output at the corresponding local solutions,\n",
      "            ``success`` a Boolean flag indicating if the optimizer exited\n",
      "            successfully,\n",
      "            ``message`` which describes the cause of the termination,\n",
      "            ``nfev`` the total number of objective function evaluations including\n",
      "            the sampling calls,\n",
      "            ``nlfev`` the total number of objective function evaluations\n",
      "            culminating from all local search optimisations,\n",
      "            ``nit`` number of iterations performed by the global routine.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Global optimization using simplicial homology global optimisation [1]_.\n",
      "        Appropriate for solving general purpose NLP and blackbox optimisation\n",
      "        problems to global optimality (low dimensional problems).\n",
      "        \n",
      "        In general, the optimization problems are of the form::\n",
      "        \n",
      "            minimize f(x) subject to\n",
      "        \n",
      "            g_i(x) >= 0,  i = 1,...,m\n",
      "            h_j(x)  = 0,  j = 1,...,p\n",
      "        \n",
      "        where x is a vector of one or more variables. ``f(x)`` is the objective\n",
      "        function ``R^n -> R``, ``g_i(x)`` are the inequality constraints, and\n",
      "        ``h_j(x)`` are the equality constraints.\n",
      "        \n",
      "        Optionally, the lower and upper bounds for each element in x can also be\n",
      "        specified using the `bounds` argument.\n",
      "        \n",
      "        While most of the theoretical advantages of SHGO are only proven for when\n",
      "        ``f(x)`` is a Lipschitz smooth function. The algorithm is also proven to\n",
      "        converge to the global optimum for the more general case where ``f(x)`` is\n",
      "        non-continuous, non-convex and non-smooth, if the default sampling method\n",
      "        is used [1]_.\n",
      "        \n",
      "        The local search method may be specified using the ``minimizer_kwargs``\n",
      "        parameter which is passed on to ``scipy.optimize.minimize``. By default\n",
      "        the ``SLSQP`` method is used. In general it is recommended to use the\n",
      "        ``SLSQP`` or ``COBYLA`` local minimization if inequality constraints\n",
      "        are defined for the problem since the other methods do not use constraints.\n",
      "        \n",
      "        The ``sobol`` method points are generated using the Sobol (1967) [2]_\n",
      "        sequence. The primitive polynomials and various sets of initial direction\n",
      "        numbers for generating Sobol sequences is provided by [3]_ by Frances Kuo\n",
      "        and Stephen Joe. The original program sobol.cc (MIT) is available and\n",
      "        described at https://web.maths.unsw.edu.au/~fkuo/sobol/ translated to\n",
      "        Python 3 by Carl Sandrock 2016-03-31.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Endres, SC, Sandrock, C, Focke, WW (2018) \"A simplicial homology\n",
      "               algorithm for lipschitz optimisation\", Journal of Global Optimization.\n",
      "        .. [2] Sobol, IM (1967) \"The distribution of points in a cube and the\n",
      "               approximate evaluation of integrals\", USSR Comput. Math. Math. Phys.\n",
      "               7, 86-112.\n",
      "        .. [3] Joe, SW and Kuo, FY (2008) \"Constructing Sobol sequences with\n",
      "               better  two-dimensional projections\", SIAM J. Sci. Comput. 30,\n",
      "               2635-2654.\n",
      "        .. [4] Hoch, W and Schittkowski, K (1981) \"Test examples for nonlinear\n",
      "               programming codes\", Lecture Notes in Economics and mathematical\n",
      "               Systems, 187. Springer-Verlag, New York.\n",
      "               http://www.ai7.uni-bayreuth.de/test_problem_coll.pdf\n",
      "        .. [5] Wales, DJ (2015) \"Perspective: Insight into reaction coordinates and\n",
      "               dynamics from the potential energy landscape\",\n",
      "               Journal of Chemical Physics, 142(13), 2015.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        First consider the problem of minimizing the Rosenbrock function, `rosen`:\n",
      "        \n",
      "        >>> from scipy.optimize import rosen, shgo\n",
      "        >>> bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 1.,  1.,  1.,  1.,  1.]), 2.9203923741900809e-18)\n",
      "        \n",
      "        Note that bounds determine the dimensionality of the objective\n",
      "        function and is therefore a required input, however you can specify\n",
      "        empty bounds using ``None`` or objects like ``np.inf`` which will be\n",
      "        converted to large float numbers.\n",
      "        \n",
      "        >>> bounds = [(None, None), ]*4\n",
      "        >>> result = shgo(rosen, bounds)\n",
      "        >>> result.x\n",
      "        array([ 0.99999851,  0.99999704,  0.99999411,  0.9999882 ])\n",
      "        \n",
      "        Next we consider the Eggholder function, a problem with several local\n",
      "        minima and one global minimum. We will demonstrate the use of arguments and\n",
      "        the capabilities of `shgo`.\n",
      "        (https://en.wikipedia.org/wiki/Test_functions_for_optimization)\n",
      "        \n",
      "        >>> def eggholder(x):\n",
      "        ...     return (-(x[1] + 47.0)\n",
      "        ...             * np.sin(np.sqrt(abs(x[0]/2.0 + (x[1] + 47.0))))\n",
      "        ...             - x[0] * np.sin(np.sqrt(abs(x[0] - (x[1] + 47.0))))\n",
      "        ...             )\n",
      "        ...\n",
      "        >>> bounds = [(-512, 512), (-512, 512)]\n",
      "        \n",
      "        `shgo` has two built-in low discrepancy sampling sequences.  First we will\n",
      "        input 30 initial sampling points of the Sobol sequence:\n",
      "        \n",
      "        >>> result = shgo(eggholder, bounds, n=30, sampling_method='sobol')\n",
      "        >>> result.x, result.fun\n",
      "        (array([ 512.        ,  404.23180542]), -959.64066272085051)\n",
      "        \n",
      "        `shgo` also has a return for any other local minima that was found, these\n",
      "        can be called using:\n",
      "        \n",
      "        >>> result.xl\n",
      "        array([[ 512.        ,  404.23180542],\n",
      "               [ 283.07593402, -487.12566542],\n",
      "               [-294.66820039, -462.01964031],\n",
      "               [-105.87688985,  423.15324143],\n",
      "               [-242.97923629,  274.38032063],\n",
      "               [-506.25823477,    6.3131022 ],\n",
      "               [-408.71981195, -156.10117154],\n",
      "               [ 150.23210485,  301.31378508],\n",
      "               [  91.00922754, -391.28375925],\n",
      "               [ 202.8966344 , -269.38042147],\n",
      "               [ 361.66625957, -106.96490692],\n",
      "               [-219.40615102, -244.06022436],\n",
      "               [ 151.59603137, -100.61082677]])\n",
      "        \n",
      "        >>> result.funl\n",
      "        array([-959.64066272, -718.16745962, -704.80659592, -565.99778097,\n",
      "               -559.78685655, -557.36868733, -507.87385942, -493.9605115 ,\n",
      "               -426.48799655, -421.15571437, -419.31194957, -410.98477763,\n",
      "               -202.53912972])\n",
      "        \n",
      "        These results are useful in applications where there are many global minima\n",
      "        and the values of other global minima are desired or where the local minima\n",
      "        can provide insight into the system (for example morphologies\n",
      "        in physical chemistry [5]_).\n",
      "        \n",
      "        If we want to find a larger number of local minima, we can increase the\n",
      "        number of sampling points or the number of iterations. We'll increase the\n",
      "        number of sampling points to 60 and the number of iterations from the\n",
      "        default of 1 to 5. This gives us 60 x 5 = 300 initial sampling points.\n",
      "        \n",
      "        >>> result_2 = shgo(eggholder, bounds, n=60, iters=5, sampling_method='sobol')\n",
      "        >>> len(result.xl), len(result_2.xl)\n",
      "        (13, 39)\n",
      "        \n",
      "        Note the difference between, e.g., ``n=180, iters=1`` and ``n=60, iters=3``.\n",
      "        In the first case the promising points contained in the minimiser pool\n",
      "        is processed only once. In the latter case it is processed every 60 sampling\n",
      "        points for a total of 3 times.\n",
      "        \n",
      "        To demonstrate solving problems with non-linear constraints consider the\n",
      "        following example from Hock and Schittkowski problem 73 (cattle-feed) [4]_::\n",
      "        \n",
      "            minimize: f = 24.55 * x_1 + 26.75 * x_2 + 39 * x_3 + 40.50 * x_4\n",
      "        \n",
      "            subject to: 2.3 * x_1 + 5.6 * x_2 + 11.1 * x_3 + 1.3 * x_4 - 5     >= 0,\n",
      "        \n",
      "                        12 * x_1 + 11.9 * x_2 + 41.8 * x_3 + 52.1 * x_4 - 21\n",
      "                            -1.645 * sqrt(0.28 * x_1**2 + 0.19 * x_2**2 +\n",
      "                                          20.5 * x_3**2 + 0.62 * x_4**2)       >= 0,\n",
      "        \n",
      "                        x_1 + x_2 + x_3 + x_4 - 1                              == 0,\n",
      "        \n",
      "                        1 >= x_i >= 0 for all i\n",
      "        \n",
      "        The approximate answer given in [4]_ is::\n",
      "        \n",
      "            f([0.6355216, -0.12e-11, 0.3127019, 0.05177655]) = 29.894378\n",
      "        \n",
      "        >>> def f(x):  # (cattle-feed)\n",
      "        ...     return 24.55*x[0] + 26.75*x[1] + 39*x[2] + 40.50*x[3]\n",
      "        ...\n",
      "        >>> def g1(x):\n",
      "        ...     return 2.3*x[0] + 5.6*x[1] + 11.1*x[2] + 1.3*x[3] - 5  # >=0\n",
      "        ...\n",
      "        >>> def g2(x):\n",
      "        ...     return (12*x[0] + 11.9*x[1] +41.8*x[2] + 52.1*x[3] - 21\n",
      "        ...             - 1.645 * np.sqrt(0.28*x[0]**2 + 0.19*x[1]**2\n",
      "        ...                             + 20.5*x[2]**2 + 0.62*x[3]**2)\n",
      "        ...             ) # >=0\n",
      "        ...\n",
      "        >>> def h1(x):\n",
      "        ...     return x[0] + x[1] + x[2] + x[3] - 1  # == 0\n",
      "        ...\n",
      "        >>> cons = ({'type': 'ineq', 'fun': g1},\n",
      "        ...         {'type': 'ineq', 'fun': g2},\n",
      "        ...         {'type': 'eq', 'fun': h1})\n",
      "        >>> bounds = [(0, 1.0),]*4\n",
      "        >>> res = shgo(f, bounds, iters=3, constraints=cons)\n",
      "        >>> res\n",
      "             fun: 29.894378159142136\n",
      "            funl: array([29.89437816])\n",
      "         message: 'Optimization terminated successfully.'\n",
      "            nfev: 119\n",
      "             nit: 3\n",
      "           nlfev: 40\n",
      "           nlhev: 0\n",
      "           nljev: 5\n",
      "         success: True\n",
      "               x: array([6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02])\n",
      "              xl: array([[6.35521569e-01, 1.13700270e-13, 3.12701881e-01, 5.17765506e-02]])\n",
      "        \n",
      "        >>> g1(res.x), g2(res.x), h1(res.x)\n",
      "        (-5.0626169922907138e-14, -2.9594104944408173e-12, 0.0)\n",
      "    \n",
      "    show_options(solver=None, method=None, disp=True)\n",
      "        Show documentation for additional options of optimization solvers.\n",
      "        \n",
      "        These are method-specific options that can be supplied through the\n",
      "        ``options`` dict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        solver : str\n",
      "            Type of optimization solver. One of 'minimize', 'minimize_scalar',\n",
      "            'root', or 'linprog'.\n",
      "        method : str, optional\n",
      "            If not given, shows all methods of the specified solver. Otherwise,\n",
      "            show only the options for the specified method. Valid values\n",
      "            corresponds to methods' names of respective solver (e.g. 'BFGS' for\n",
      "            'minimize').\n",
      "        disp : bool, optional\n",
      "            Whether to print the result rather than returning it.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            Either None (for disp=True) or the text string (disp=False)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The solver-specific methods are:\n",
      "        \n",
      "        `scipy.optimize.minimize`\n",
      "        \n",
      "        - :ref:`Nelder-Mead <optimize.minimize-neldermead>`\n",
      "        - :ref:`Powell      <optimize.minimize-powell>`\n",
      "        - :ref:`CG          <optimize.minimize-cg>`\n",
      "        - :ref:`BFGS        <optimize.minimize-bfgs>`\n",
      "        - :ref:`Newton-CG   <optimize.minimize-newtoncg>`\n",
      "        - :ref:`L-BFGS-B    <optimize.minimize-lbfgsb>`\n",
      "        - :ref:`TNC         <optimize.minimize-tnc>`\n",
      "        - :ref:`COBYLA      <optimize.minimize-cobyla>`\n",
      "        - :ref:`SLSQP       <optimize.minimize-slsqp>`\n",
      "        - :ref:`dogleg      <optimize.minimize-dogleg>`\n",
      "        - :ref:`trust-ncg   <optimize.minimize-trustncg>`\n",
      "        \n",
      "        `scipy.optimize.root`\n",
      "        \n",
      "        - :ref:`hybr              <optimize.root-hybr>`\n",
      "        - :ref:`lm                <optimize.root-lm>`\n",
      "        - :ref:`broyden1          <optimize.root-broyden1>`\n",
      "        - :ref:`broyden2          <optimize.root-broyden2>`\n",
      "        - :ref:`anderson          <optimize.root-anderson>`\n",
      "        - :ref:`linearmixing      <optimize.root-linearmixing>`\n",
      "        - :ref:`diagbroyden       <optimize.root-diagbroyden>`\n",
      "        - :ref:`excitingmixing    <optimize.root-excitingmixing>`\n",
      "        - :ref:`krylov            <optimize.root-krylov>`\n",
      "        - :ref:`df-sane           <optimize.root-dfsane>`\n",
      "        \n",
      "        `scipy.optimize.minimize_scalar`\n",
      "        \n",
      "        - :ref:`brent       <optimize.minimize_scalar-brent>`\n",
      "        - :ref:`golden      <optimize.minimize_scalar-golden>`\n",
      "        - :ref:`bounded     <optimize.minimize_scalar-bounded>`\n",
      "        \n",
      "        `scipy.optimize.linprog`\n",
      "        \n",
      "        - :ref:`simplex         <optimize.linprog-simplex>`\n",
      "        - :ref:`interior-point  <optimize.linprog-interior-point>`\n",
      "    \n",
      "    toms748(f, a, b, args=(), k=1, xtol=2e-12, rtol=8.881784197001252e-16, maxiter=100, full_output=False, disp=True)\n",
      "        Find a zero using TOMS Algorithm 748 method.\n",
      "        \n",
      "        Implements the Algorithm 748 method of Alefeld, Potro and Shi to find a\n",
      "        zero of the function `f` on the interval `[a , b]`, where `f(a)` and\n",
      "        `f(b)` must have opposite signs.\n",
      "        \n",
      "        It uses a mixture of inverse cubic interpolation and\n",
      "        \"Newton-quadratic\" steps. [APS1995].\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            Python function returning a scalar.  The function :math:`f`\n",
      "            must be continuous, and :math:`f(a)` and :math:`f(b)`\n",
      "            have opposite signs.\n",
      "        a : scalar,\n",
      "            lower boundary of the search interval\n",
      "        b : scalar,\n",
      "            upper boundary of the search interval\n",
      "        args : tuple, optional\n",
      "            containing extra arguments for the function `f`.\n",
      "            `f` is called by ``f(x, *args)``.\n",
      "        k : int, optional\n",
      "            The number of Newton quadratic steps to perform each\n",
      "            iteration. ``k>=1``.\n",
      "        xtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root. The\n",
      "            parameter must be nonnegative.\n",
      "        rtol : scalar, optional\n",
      "            The computed root ``x0`` will satisfy ``np.allclose(x, x0,\n",
      "            atol=xtol, rtol=rtol)``, where ``x`` is the exact root.\n",
      "        maxiter : int, optional\n",
      "            if convergence is not achieved in `maxiter` iterations, an error is\n",
      "            raised.  Must be >= 0.\n",
      "        full_output : bool, optional\n",
      "            If `full_output` is False, the root is returned.  If `full_output` is\n",
      "            True, the return value is ``(x, r)``, where `x` is the root, and `r` is\n",
      "            a `RootResults` object.\n",
      "        disp : bool, optional\n",
      "            If True, raise RuntimeError if the algorithm didn't converge.\n",
      "            Otherwise the convergence status is recorded in the `RootResults`\n",
      "            return object.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        x0 : float\n",
      "            Approximate Zero of `f`\n",
      "        r : `RootResults` (present if ``full_output = True``)\n",
      "            Object containing information about the convergence.  In particular,\n",
      "            ``r.converged`` is True if the routine converged.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        brentq, brenth, ridder, bisect, newton\n",
      "        fsolve : find zeroes in n dimensions.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `f` must be continuous.\n",
      "        Algorithm 748 with ``k=2`` is asymptotically the most efficient\n",
      "        algorithm known for finding roots of a four times continuously\n",
      "        differentiable function.\n",
      "        In contrast with Brent's algorithm, which may only decrease the length of\n",
      "        the enclosing bracket on the last step, Algorithm 748 decreases it each\n",
      "        iteration with the same asymptotic efficiency as it finds the root.\n",
      "        \n",
      "        For easy statement of efficiency indices, assume that `f` has 4\n",
      "        continuouous deriviatives.\n",
      "        For ``k=1``, the convergence order is at least 2.7, and with about\n",
      "        asymptotically 2 function evaluations per iteration, the efficiency\n",
      "        index is approximately 1.65.\n",
      "        For ``k=2``, the order is about 4.6 with asymptotically 3 function\n",
      "        evaluations per iteration, and the efficiency index 1.66.\n",
      "        For higher values of `k`, the efficiency index approaches\n",
      "        the `k`-th root of ``(3k-2)``, hence ``k=1`` or ``k=2`` are\n",
      "        usually appropriate.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [APS1995]\n",
      "           Alefeld, G. E. and Potra, F. A. and Shi, Yixun,\n",
      "           *Algorithm 748: Enclosing Zeros of Continuous Functions*,\n",
      "           ACM Trans. Math. Softw. Volume 221(1995)\n",
      "           doi = {10.1145/210089.210111}\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> def f(x):\n",
      "        ...     return (x**3 - 1)  # only one real root at x = 1\n",
      "        \n",
      "        >>> from scipy import optimize\n",
      "        >>> root, results = optimize.toms748(f, 0, 2, full_output=True)\n",
      "        >>> root\n",
      "        1.0\n",
      "        >>> results\n",
      "              converged: True\n",
      "                   flag: 'converged'\n",
      "         function_calls: 11\n",
      "             iterations: 5\n",
      "                   root: 1.0\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BFGS', 'Bounds', 'HessianUpdateStrategy', 'LbfgsInvHessPro...\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 8192...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "\n",
      "FILE\n",
      "    /home/prudhvi/.local/lib/python3.6/site-packages/scipy/optimize/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(scipy.optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ml1",
   "language": "python",
   "name": ".ml1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
